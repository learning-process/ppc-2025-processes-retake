# Умножение разреженных матриц. Элементы типа double. Формат хранения матрицы — строковый (CRS)

- Student: Солонин Владислав Викторович, group 3823Б1ПР1
- Technology: SEQ | MPI
- Variant: 4

## 1. Introduction

Умножение разреженных матриц широко применяется в научных вычислениях, графовых алгоритмах и машинном обучении. При работе с матрицами, в которых большинство элементов равны нулю, эффективное хранение и обработка данных критически важны. Цель работы — реализация последовательного и параллельного алгоритмов умножения разреженных матриц в формате CRS (Compressed Row Storage) с элементами типа `double`.

## 2. Problem Statement

**Вход:** две разреженные матрицы A (размер `rows_a × cols_a`) и B (размер `cols_a × cols_b`), хранящиеся в формате CRS: массивы значений (`values`), индексов столбцов (`col_indices`) и указателей строк (`row_ptr`), а также размерности матриц.

**Выход:** матрица C = A × B в формате CRS.

**Формат CRS:** для матрицы с `n` строками и `nnz` ненулевыми элементами хранятся:
- `values[nnz]` — ненулевые значения по строкам слева направо
- `col_indices[nnz]` — индексы столбцов соответствующих элементов
- `row_ptr[n+1]` — `row_ptr[i]` указывает на начало строки `i` в массиве values

## 3. Baseline Algorithm (Sequential)

Для каждой строки `i` матрицы A проходим по её ненулевым элементам `a[i][k]`. Для каждого такого элемента берём строку `k` матрицы B и добавляем `a[i][k] * b[k][j]` к временному вектору результата по столбцу `j`. После обработки всей строки собираем ненулевые элементы из временного вектора, сортируем по индексу столбца и записываем в CRS-структуру результата.

Временная сложность: `O(rows_a * cols_b * cols_a)` в худшем случае. Реальная сложность определяется количеством ненулевых элементов.

## 4. Parallelization Scheme

Строки матрицы A распределяются между MPI-процессами по принципу чередования: процесс с номером `rank` обрабатывает строки `rank, rank + size, rank + 2*size, ...`

Матрица B целиком рассылается всем процессам через `MPI_Bcast`. Каждый процесс вычисляет свои строки результата независимо. Затем результаты собираются на процессе 0 через точечные обмены `MPI_Send` / `MPI_Recv`.

```
rank 0: строки 0, size, 2*size, ...  → local_C_0
rank 1: строки 1, size+1, 2*size+1, ... → local_C_1
...
rank 0 собирает все части → C
```

## 5. Implementation Details

- `common/include/common.hpp` — типы InType (кортеж из 9 элементов), OutType (кортеж из 3 элементов)
- `seq/` — последовательная реализация
- `mpi/` — параллельная реализация
- `tests/functional/` — 20 функциональных тестов
- `tests/performance/` — тесты производительности на диагональных матрицах 50000×50000

Ненулевые элементы результирующей строки фильтруются с порогом `1e-12` для корректной работы с `double`.

## 6. Experimental Setup

- **Hardware:** AMD Ryzen 7, 8 cores, 16 GB RAM
- **OS:** Ubuntu 24.04
- **Compiler:** GCC 14, Release
- **MPI:** OpenMPI
- **PPC_NUM_PROC:** 1, 2, 4
- **Data:** диагональные матрицы 50000×50000 с дополнительными элементами

## 7. Results and Discussion

### 7.1 Correctness

Тесты проверяют корректность умножения для матриц различных типов: квадратных, прямоугольных, единичных, нулевых, разреженных, с отрицательными и дробными элементами. Результат сравнивается с эталонным значением, вычисленным через плотное представление. Погрешность — не более `1e-10`.

### 7.2 Performance

| Mode | Processes | Time, s | Speedup | Efficiency |
|------|-----------|---------|---------|------------|
| seq  | 1         | 1.820   | 1.00    | N/A        |
| mpi  | 2         | 0.980   | 1.86    | 93%        |
| mpi  | 4         | 0.530   | 3.43    | 86%        |

## 8. Conclusions

Параллельная реализация умножения разреженных матриц в формате CRS показала хорошее ускорение. Основные затраты на коммуникацию — рассылка матрицы B и сбор результатов — незначительны по сравнению с вычислительной работой при больших разреженных матрицах. Алгоритм корректно обрабатывает все граничные случаи, включая нулевые строки и прямоугольные матрицы.

## 9. References

1. Гергель В.П. и др. Параллельные вычисления. Технологии и численные методы. — Нижний Новгород, 2013.
2. MPI Forum. MPI: A Message-Passing Interface Standard. https://www.mpi-forum.org/
3. Saad Y. Iterative Methods for Sparse Linear Systems. SIAM, 2003.
