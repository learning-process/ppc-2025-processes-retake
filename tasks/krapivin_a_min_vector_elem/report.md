# Минимальное значение элементов вектора

- Студент: Крапивин Александр Сергеевич, группа 3823Б1ПР1
- Технология: SEQ, MPI
- Вариант: 4

## 1. Введение
Задача поиска минимального элемента в векторе является фундаментальной операцией в вычислительной математике и анализе данных. Параллельная реализация позволяет значительно ускорить обработку больших массивов данных за счет распределения вычислений между несколькими процессами.

## 2. Постановка задачи
Найти минимальный элемент в целочисленном векторе произвольного размера.

Входные данные: вектор целых чисел vector<int>
Выходные данные: целое число - минимальный элемент вектора

## 3. Базовый алгоритм (Последовательный)
```cpp
if (GetInput().empty()) {
return false;
}

int result = GetInput()[0];

for (size_t i = 1; i < GetInput().size(); i++) {
    if (result > GetInput()[i]) {
        result = GetInput()[i];
    }
}

GetOutput() = result;
```
### Описание алгоритма
Проверка на пустоту: Если входной вектор пуст, функция возвращает false, сигнализируя об ошибке. 
Инициализация: Первый элемент вектора принимается в качестве начального значения минимума (result = GetInput()[0]).
Последовательный перебор: Цикл проходит по всем остальным элементам вектора (начиная с индекса 1).
Сравнение и обновление: Для каждого элемента проверяется условие result > GetInput()[i]. Если текущий элемент меньше текущего минимума, значение минимума обновляется.
Сохранение результата: Найденное минимальное значение сохраняется в выходные данные через GetOutput().

## 4. Схема распараллеливания
### Распределение данных
Исходный вектор распределяется между всеми процессами с использованием коллективной операции MPI_Scatterv. Процесс с рангом 0 (root) выполняет разделение данных на блоки примерно равного размера и рассылает их остальным процессам.

Алгоритм распределения реализован в функции SplitData:

```cpp
int step = n / mpi_size;
int remainder = n % mpi_size;

for (auto &elem : send_counts) {
    elem = step;
}

for (int i = 0; i < remainder; i++) {
    send_counts[i]++;
}

displacements[0] = 0;
int disp_sum = 0;
for (int i = 1; i < mpi_size; i++) {
    disp_sum += send_counts[i - 1];
    displacements[i] = disp_sum;
}

MPI_Scatterv(global_data.data(), send_counts.data(), displacements.data(), MPI_INT, 
             input.data(), static_cast<int>(input.size()), MPI_INT, 0, MPI_COMM_WORLD);
```

### Коммуникационная схема
1. Процесс 0 (root):
- Определяет общий размер данных (n)
- Рассылает размер всем процессам через MPI_Bcast
- Вычисляет размеры блоков (send_counts) и смещения (displacements)
- Рассылает данные всем процессам через MPI_Scatterv
- Находит локальный минимум на своей части данных

2. Процессы 1..N-1:
- Получают общий размер данных через MPI_Bcast
- Получают свою порцию данных через MPI_Scatterv
- Находят локальный минимум с помощью FindMin

3. Сбор локальных результатов:
- Все процессы отправляют свои локальные минимумы на процесс 0 через MPI_Gather
- Процесс 0 собирает все локальные минимумы в массив recv_buf

4. Определение глобального минимума:
- Процесс 0 находит минимальное значение среди всех локальных минимумов с помощью FindMin
- Результат сохраняется в GetOutput()

5. Распространение результата:
- В PostProcessingImpl процесс 0 рассылает финальный результат всем процессам через MPI_Bcast

## 5. Детали реализации
### Структура кода
- `ops_mpi.cpp` - MPI реализация
- `ops_seq.cpp` - SEQ реализация
- `common.hpp` - общие типы данных
- Тесты в папках `tests/functional/` и `tests/performance/`

### Особенности реализации
- Валидация выполняется только на процессе с рангом 0
- При пустом векторе задача считается некорректной
- Балансировка нагрузки между процессами гарантирует, что ни один процесс не будет простаивать в ожидании других

## 6. Экспериментальная установка
### Оборудование и ПО
- **Процессор:** AMD Ryzen 7 4800H (2.9 Ггц)
- **ОС:** Ubuntu devcontainer (host Windows)
- **Компилятор:** gcc
- **Тип сборки:** release

### Данные для тестирования
Тестовые данные представляют собой вектор случайных целых чисел. 
Все тестовые данные сгенерированы, ключ генерации постоянен.

## 7. Результаты и обсуждение

### 7.1 Производительность

| Процессы | Время, с | Ускорение | Эффективность |
|----------|-----------|-----------|---------------|
| 1 (SEQ)  |    0.0004   | 1     | N/A           |
| 2        |    0,002   | 0,2      | 1%           |
| 4        |    0,003   | 0,13      | 0.4%           |


## 8. Выводы
В ходе работы была решена задача поиска минимального элемента вектора с использованием последовательного алгоритма и технологии MPI для параллельных вычислений

Результаты тестов на производительность показали, что параллельная программма на большой векторе слишком долго выполняет аллоцирование и распределение данных между процессами, вследчтвие чего падает производительность

### Ограничения
- Накладные расходы MPI для небольших векторов
- Снижение эффективности при большом количестве процессов


## 9. Источники
1. Курс лекций по параллельному программированию Сысоева Александра Владимировича. 
2. Документация по курсу: https://learning-process.github.io/parallel_programming_course/ru

## Приложение

```cpp
bool KrapivinAMinVectorElemMPI::RunImpl() {
  if (GetInput().empty()) {
    return false;
  }

  int rank = 0;
  int mpi_size = 0;
  MPI_Comm_rank(MPI_COMM_WORLD, &rank);
  MPI_Comm_size(MPI_COMM_WORLD, &mpi_size);

  std::vector<int> input;
  std::vector<int> send_counts(mpi_size, 0);
  std::vector<int> displacements(mpi_size, 0);
  std::vector<int> recv_buf(mpi_size, 0);

  SplitData(input, send_counts, displacements, rank, mpi_size);
  int local_result = FindMin(input);

  MPI_Gather(&local_result, 1, MPI_INT, recv_buf.data(), 1, MPI_INT, 0, MPI_COMM_WORLD);
  if (rank == 0) {
    GetOutput() = FindMin(recv_buf);
  }
  return true;
}
```