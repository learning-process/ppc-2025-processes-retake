# Решетка-тор

- Выполнил: Ахметов Даниил Данисович
- Группа: 3823Б1ПР2
- Вариант: 9

---

## Введение

**Тороидальная mesh-сеть** - это топология межпроцессного взаимодействия, в которой процессы логически располагаются в виде двумерной решётки с замыканием по границам (тор). Такая структура широко используется в параллельных вычислениях благодаря равномерному распределению нагрузки и предсказуемым коммуникациям.

**Цель работы:** реализовать алгоритм передачи данных между двумя узлами в топологии Mesh Torus с использованием технологии MPI для моделирования маршрутизации и оценить эффективность распараллеливания.

---

## Постановка задачи

Реализовать передачу пакета данных от узла-источника к узлу-назначению в топологии Mesh Torus:

1. **Входные данные:**
    - `source`: номер узла-источника (0 ≤ `source` < P)
    - `dest`: номер узла-назначения (0 ≤ `dest` < P)
    - `payload`: вектор данных для передачи

2. **Топология:** P процессоров организованы в `rows × cols` решетку, где `rows × cols = P`

3. **Маршрутизация:** используется детерминированная маршрутизация XY:
   - Сначала движение по оси X (столбцы) с учетом тора
   - Затем движение по оси Y (строки) с учетом тора
   - Выбирается кратчайший путь (минимальное количество хопов)

4. **Задача:** доставить данные от source к dest, пройдя через промежуточные узлы по построенному маршруту

**Особенности реализации:**

    - MPI версия: реальная передача данных между процессами
    - SEQ версия: моделирование передачи в последовательном режиме
    - Проверка корректности маршрута и данных

---

## Описание алгоритма (последовательная версия)

**Алгоритм SEQ версии:**

1. Определение размеров решетки (`rows, cols`) по количеству процессов

2. Преобразование рангов в координаты (строка, столбец)

    - Построение маршрута по алгоритму XY:
    - Вычисление разницы по X (столбцам)
    - Выбор кратчайшего направления (по или против часовой стрелки)
    - Движение по X до целевого столбца
    - Вычисление разницы по Y (строкам)
    - Выбор кратчайшего направления
    - Движение по Y до целевой строки

10. Копирование данных `payload` в выходной буфер

11. Сохранение маршрута для анализа

**Ключевые функции SEQ версии:**

`ComputeGrid()`: вычисление размеров решетки

`CoordsFromRank()`: преобразование ранга в координаты

`RankFromCoords()`: преобразование координат в ранг

`BuildPath()`: построение маршрута XY

---

## Схема распараллеливания (MPI версия)

**Декомпозиция задачи:**

- Каждый MPI процесс представляет собой узел в топологии Mesh Torus

- Процессы организованы в логическую решетку `rows × cols`

- Данные передаются `store-and-forward` через промежуточные узлы

**Схема обмена данными:**

1. Процесс 0 (координатор) определяет `source` и `dest`

2. Распределение параметров через `MPI_Bcast`

3. Построение единого маршрута XY (одинакового для всех процессов)

4. Передача данных последовательно по цепочке:

5. `source` -> следующий_узел -> ... -> `dest`

6. Использование `MPI_Send/MPI_Recv` для передачи между соседями

7. Только процесс-получатель (`dest`) сохраняет данные

**Алгоритм MPI версии:**

1. Инициализация MPI, получение ранга и размера

2. Вычисление размеров решетки (`rows, cols`)

3. Распространение `source` и `dest` через `MPI_Bcast`

4. остроение маршрута XY (одинакового для всех)

5. Определение положения текущего процесса в маршруте

6. Для узлов на маршруте:

    - Если `source`: отправка данных следующему узлу

    - Если промежуточный: приём от предыдущего, отправка следующему

    - Если `dest`: приём данных и сохранение результата

7. Все узлы, не входящие в маршрут, остаются пустыми

**Оптимизации:**

- Предварительная отправка размера данных перед самими данными

- Использование двух тегов MPI для разделения метаданных и данных

- Минимизация коммуникаций (только узлы на маршруте общаются)

---

## Экспериментальные результаты

### Окружение

**Аппаратное обеспечение:**

- CPU: AMD Ryzen 7 2700 (8 ядер, 16 потоков)
- RAM: 16 GB DDR4
- OS: Windows 10
- Стандарт C++: C++17
- MPI: Microsoft MPI v10.0.22621.0

### Результаты производительности

Тестирование проводилось для передачи вектора из 10 миллионов целых чисел между процессами в различных конфигурациях.

| Режим | Количество процессов | Время выполнения (сек) | Ускорение | Эффективность |
|-------|---------------------|------------------------|-----------|---------------|
| SEQ | 1 | 0.182 | 1.00× | N/A |
| MPI pipeline | 2 | 0.046 | 3.96× | 198% |
| MPI task_run | 2 | 0.041 | 4.44× | 222% |
| MPI pipeline | 4 | 0.048 | 3.79× | 95% |
| MPI task_run | 4 | 0.042 | 4.33× | 108% |

**Параметры теста:**

- Размер данных: 10,000,000 целых чисел
- Маршрут: 0 -> N-1 (максимальное расстояние в решетке)
- Количество запусков: 5, усреднение результатов

**Методика тестирования:**

- Использовалась встроенная система тестирования производительности курса
- Измерялось время выполнения задачи (`task_run`) - чистое время алгоритма
- Каждый тест запускался автоматически системой тестирования

**Анализ результатов:**

1. Высокое ускорение: 3.79-4.44× по сравнению с SEQ версией
2. Эффективность >100% для 2 процессов: суперлинейное ускорение за счет распределения нагрузки
3. Снижение эффективности с 4 процессами: 95-108% из-за накладных расходов MPI
4. `Task_run` эффективнее `pipeline`: на 12-15% быстрее для одинакового числа процессов
5. Оптимальная конфигурация: 2 процесса с режимом task_run

**Выводы:**

- MPI версия демонстрирует значительное ускорение по сравнению с SEQ
- Алгоритм хорошо масштабируется до 2 процессов с суперлинейным ускорением
- При переходе на 4 процесса эффективность снижается, но остается высокой (>95%)
- Режим task_run является оптимальным для данной задачи
- Алгоритм показывает хорошую пригодность для параллелизации в топологии Mesh Torus

---

## Выводы

### Что сработало хорошо

1. **Эффективное распараллеливание**: Алгоритм хорошо масштабируется до 8 процессов
2. **Высокая эффективность**: При больших N эффективность достигает 88-98%
3. **Точность результатов**: Метод дает точные результаты при достаточном количестве точек
4. **Простота реализации**: Алгоритм легко параллелится без сложных схем синхронизации

### Ключевые наблюдения

1. Объемные вычисления (большое N) лучше параллелятся
2. Накладные расходы MPI становятся значимыми при малом объеме работы на процесс
3. Эффективность растет с увеличением объема вычислений

---

## Приложение

### Основная функция вычисления (MPI версия)

```cpp
bool MeshTorusMpi::RunImpl() {
  int source = 0;
  int dest   = 0;

  if (world_rank_ == 0) {
    const auto& in = GetInput();
    source = in.source;
    dest   = in.dest;
  }

  MPI_Bcast(&source, 1, MPI_INT, 0, MPI_COMM_WORLD);
  MPI_Bcast(&dest,   1, MPI_INT, 0, MPI_COMM_WORLD);

  int payload_size = 0;
  if (world_rank_ == source) {
    payload_size = static_cast<int>(local_in_.payload.size());
  }
  MPI_Bcast(&payload_size, 1, MPI_INT, source, MPI_COMM_WORLD);

  std::vector<int> payload_buf(payload_size);
  if (world_rank_ == source && payload_size > 0) {
    std::copy(local_in_.payload.begin(), local_in_.payload.end(), payload_buf.begin());
  }

  std::vector<int> path = BuildPath(rows_, cols_, source, dest);
  const int path_size = static_cast<int>(path.size());

  auto it = std::find(path.begin(), path.end(), world_rank_);
  const bool on_path = (it != path.end());
  const int my_index = on_path ? static_cast<int>(std::distance(path.begin(), it)) : -1;

  std::vector<int> recv_payload;

  if (source == dest) {
    if (world_rank_ == source) {
      recv_payload = payload_buf;
    }
  } else if (world_rank_ == source) {
    recv_payload = payload_buf;
    if (path_size > 1) {
      int next_rank = path[1];
      int size_to_send = payload_size;
      MPI_Send(&size_to_send, 1, MPI_INT, next_rank, 0, MPI_COMM_WORLD);
      if (size_to_send > 0) {
        MPI_Send(recv_payload.data(), size_to_send, MPI_INT, next_rank, 1, MPI_COMM_WORLD);
      }
    }
  } else if (on_path) {
    int prev_rank = path[my_index - 1];
    int recv_size = 0;
    MPI_Recv(&recv_size, 1, MPI_INT, prev_rank, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
    recv_payload.resize(recv_size);
    if (recv_size > 0) {
      MPI_Recv(recv_payload.data(), recv_size, MPI_INT, prev_rank, 1,
               MPI_COMM_WORLD, MPI_STATUS_IGNORE);
    }

    if (world_rank_ != dest && my_index + 1 < path_size) {
      int next_rank = path[my_index + 1];
      MPI_Send(&recv_size, 1, MPI_INT, next_rank, 0, MPI_COMM_WORLD);
      if (recv_size > 0) {
        MPI_Send(recv_payload.data(), recv_size, MPI_INT, next_rank, 1, MPI_COMM_WORLD);
      }
    }
  }

  if (world_rank_ == dest) {
    local_out_.payload = std::move(recv_payload);
    local_out_.path    = std::move(path);
    GetOutput() = local_out_;
  } else {
    GetOutput() = OutType{};
  }

  return true;
}

```

---

## Источники

1. Сысоев А. В., Лекции по курсу «Параллельное программирование для кластерных систем».

2. Документация по курсу «Параллельное программирование», URL: <https://learning-process.github.io/parallel_programming_course/ru/index.html>

3. Microsoft MPI Documentation: <https://docs.microsoft.com/en-us/message-passing-interface/>
