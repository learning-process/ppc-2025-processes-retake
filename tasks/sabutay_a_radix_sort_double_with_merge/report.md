#Поразрядная сортировка вещественных чисел(double) с простым слиянием

- Student:Иманов Сабутай Ширзад оглы, группа 3823Б1ПР5 -
    Technology:SEQ, MPI - Variant:20

                                  ##1. Introduction В данной работе рассматривается задача сортировки массива
                                      вещественных чисел типа `double` с использованием поразрядной
                                      сортировки(Radix Sort)и простого слияния отсортированных подмассивов.

                                  Целью работы является реализация последовательной(SEQ)
и параллельной(MPI)
версий алгоритма,
    а также проверка их корректности и анализ производительности.

    Поразрядная сортировка выбрана как алгоритм линейной сложности,
    хорошо подходящий для распараллеливания при обработке больших массивов данных
        .

    ##2. Problem Statement Требуется отсортировать массив вещественных чисел типа `double` по возрастанию.

    Входные данные : -вектор \(x \in \mathbb{R} ^ n \),
    содержащий значения типа `double` - допускаются отрицательные значения,
    положительные значения и значения ±0

    Выходные данные : -вектор \(y \in \mathbb{R} ^ n \),
                      содержащий те же элементы,
                      отсортированные по возрастанию

                          Ограничения : - \(n \ge 0 \) -
                                        сортировка должна быть корректной для всех допустимых значений типа `double`

                                        ##3. Baseline Algorithm(Sequential)
В последовательной версии используется поразрядная сортировка для вещественных чисел.

    Основные идеи алгоритма : -Каждое значение `double` преобразуется в 64 -
    битный целочисленный ключ,
    сохраняющий порядок вещественных чисел - Для отрицательных чисел используется инверсия битов -
        Для неотрицательных чисел инвертируется знаковый бит -
        Сортировка выполняется за 8 проходов по 8 бит(LSD Radix Sort)

            Таким образом,
    алгоритм имеет асимптотическую сложность :
\[O(8 \cdot n) = O(n)
\]

    Последовательная версия сортирует весь массив целиком.

    ##4. Parallelization Scheme## #MPI Параллельная реализация использует модель MPI со следующим подходом :

    -Исходный массив распределяется между процессами с помощью `MPI_Scatterv` -
        Каждый процесс сортирует свой локальный подмассив поразрядной сортировкой -
        Далее выполняется иерархическое попарное слияние отсортированных подмассивов
    : -процессы с меньшими рангами принимают данные от соседних процессов
      -
      используется простое слияние двух отсортированных массивов -
      Финальный результат формируется на процессе с рангом 0 -
      Итоговый массив рассылается всем процессам с помощью `MPI_Bcast`

      Данный подход минимизирует количество коммуникаций и использует локальные вычисления максимально эффективно.

      ##5. Implementation Details
      -
      Структура проекта : - `common / include / common.hpp` — общие типы данных и базовый класс задачи
      - `seq /` — последовательная реализация
      - `mpi /` — параллельная реализация
      -
      Ключевые классы : - `SabutayAradixSortDoubleWithMergeSEQ` - `SabutayAradixSortDoubleWithMergeMPI` -
                        Особенности реализации
    : -используется безопасное преобразование `double` в `uint64_t` через `std::bit_cast` -
      корректно обрабатываются отрицательные значения и + -0 - для соответствия требованиям `clang -
      tidy` используются `std::array` и `.at()`

      ##6. Experimental Setup
      -
      Hardware / OS : -CPU : 11th Gen Intel(R) Core(TM) i5 - 1135G7 @2.40GHz - RAM : 8 ГБ -
      OS : Windows 10 - Toolchain : -Compiler : MSVC / clang -
           C++ standard : C++ 20 - Build type : Release - Data : -размер входного массива : 200000 элементов -
                                                                 значения генерируются псевдослучайным образом

                                                                 Команды для запуска тестов
    : mpiexec
      -
      n 2 .\ppc_func_tests.exe-- gtest_filter = RastvorovKRadixSortDoubleMergeFuncTests/*
mpiexec -n 2 .\ppc_perf_tests.exe --gtest_filter=RunModeTests/RastvorovKRadixSortDoubleMergeRunPerfTestProcesses.*

## 7. Results and Discussion
#### 7.1 Correctness

Корректность реализации проверялась:
- функциональными тестами для SEQ и MPI версий
- сравнением результата с эталонной сортировкой
- проверкой на граничных случаях (пустой массив, один элемент, одинаковые значения)

Все функциональные тесты были успешно пройдены.

#### 7.2 Performance
Ниже приведены результаты измерения времени выполнения, ускорения и эффективности для последовательной (SEQ) и параллельной (MPI) реализаций.

 Pipeline
| Processes | Time (s) | Speedup vs MPI 1 | Efficiency |
|----------:|---------:|-----------------:|-----------:|
| MPI 1     | 0.013409 | 1.00×            | 100%       |
| MPI 2     | 0.009157 | 1.46×            | 73%        |
| MPI 4     | 0.009773 | 1.37×            | 34%        |
 Task Run
| Processes | Time (s) | Speedup vs MPI 1 | Efficiency |
|----------:|---------:|-----------------:|-----------:|
| MPI 1     | 0.010820 | 1.00×            | 100%       |
| MPI 2     | 0.017122 | 0.63×            | 32%        |
| MPI 4     | 0.026404 | 0.41×            | 10%        |


MPI-реализация демонстрирует ускорение в режиме Pipeline при использовании
двух процессов. При дальнейшем увеличении числа процессов эффективность
снижается из-за роста накладных расходов на межпроцессное взаимодействие.

В режиме Task Run ускорение не наблюдается, так как стоимость коммуникаций
и операций слияния превышает выигрыш от параллельного выполнения.

## 8. Conclusions

В рамках работы были реализованы последовательная и параллельная версии
поразрядной сортировки вещественных чисел с простым слиянием.

Проведённые эксперименты показали, что MPI-реализация может обеспечивать
умеренное ускорение по сравнению с последовательной версией
в режиме pipeline при использовании нескольких процессов,
при сохранении корректности результатов.

В то же время в режиме task_run ускорение не достигается,
а при увеличении числа процессов время выполнения возрастает.
Основным ограничением масштабируемости является стоимость межпроцессного обмена данными
и операций слияния, которые начинают доминировать над вычислениями
при большом числе процессов.

## 9. References
1. OpenMPI Documentation — https://www.open-mpi.org/doc/
2. PPC Parallel Programming Course — https://learning-process.github.io/parallel_programming_course/
3. GTest Framework — https://github.com/google/googletest
4. Статья https://www.cyberforum.ru/cpp-beginners/thread597195.html
5. Книга "Параллельное программированиена C++ в действии" автор: ANTHONY WILLIAMS https://dodo.inm.ras.ru/konshin/HPC/bib/HPC-cxx11-book.pdf
6. Физическая книга С.Макконнел "Совершенный код. Практическое руководство по разработке программного Обеспечения" 2014г.
