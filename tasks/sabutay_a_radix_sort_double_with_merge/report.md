# Поразрядная сортировка вещественных чисел (double) с простым слиянием

- Student: Иманов Сабутай Ширзад оглы, группа 3823Б1ПР5
- Technology: SEQ, MPI
- Variant: 20

## 1. Introduction

В данной работе рассматривается задача сортировки массива вещественных чисел
типа `double` с использованием поразрядной сортировки (Radix Sort)
и простого слияния отсортированных подмассивов.

Целью работы является реализация последовательной (SEQ) и параллельной (MPI)
версий алгоритма, а также проверка их корректности и анализ производительности.

Поразрядная сортировка выбрана как алгоритм линейной сложности,
хорошо подходящий для распараллеливания при обработке больших массивов данных.

## 2. Problem Statement

Требуется отсортировать массив вещественных чисел типа `double`
по возрастанию.

Входные данные:
- вектор \( x \in \mathbb{R}^n \), содержащий значения типа `double`
- допускаются отрицательные значения, положительные значения и значения ±0

Выходные данные:
- вектор \( y \in \mathbb{R}^n \), содержащий те же элементы,
  отсортированные по возрастанию

Ограничения:
- \( n \ge 0 \)
- сортировка должна быть корректной для всех допустимых значений типа `double`

## 3. Baseline Algorithm (Sequential)

В последовательной версии используется поразрядная сортировка
для вещественных чисел.

Основные идеи алгоритма:
- каждое значение `double` преобразуется в 64-битный целочисленный ключ,
  сохраняющий порядок вещественных чисел
- для отрицательных чисел используется инверсия всех битов
- для неотрицательных чисел инвертируется только знаковый бит
- сортировка выполняется за 8 проходов по 8 бит (LSD Radix Sort)

Таким образом, алгоритм имеет асимптотическую сложность:

\[
O(8 \cdot n) = O(n)
\]

Последовательная версия сортирует весь массив целиком.

## 4. Parallelization Scheme (MPI)

Параллельная реализация использует модель MPI
со следующим подходом:

- исходный массив распределяется между процессами с помощью `MPI_Scatterv`
- каждый процесс сортирует свой локальный подмассив
  поразрядной сортировкой
- далее выполняется иерархическое попарное слияние
  отсортированных подмассивов:
  - процессы с меньшими рангами принимают данные
    от соседних процессов
  - используется простое слияние двух отсортированных массивов
- финальный результат формируется на процессе с рангом 0
- итоговый массив рассылается всем процессам
  с помощью `MPI_Bcast`

Данный подход минимизирует количество коммуникаций
и использует локальные вычисления максимально эффективно.

## 5. Implementation Details

Структура проекта:
- `common/include/common.hpp` — общие типы данных и базовый класс задачи
- `seq/` — последовательная реализация
- `mpi/` — параллельная реализация

Ключевые классы:
- `SabutayARadixSortDoubleWithMergeSEQ`
- `SabutayARadixSortDoubleWithMergeMPI`

Особенности реализации:
- используется безопасное преобразование `double` в `uint64_t`
  через `std::bit_cast`
- корректно обрабатываются отрицательные значения и ±0
- для соответствия требованиям `clang-tidy`
  используются `std::array` и метод `.at()`

## 6. Experimental Setup

Hardware / OS:
- CPU: 11th Gen Intel(R) Core(TM) i5-1135G7 @ 2.40GHz
- RAM: 8 ГБ
- OS: Windows 10

Toolchain:
- Compiler: MSVC / clang
- C++ standard: C++20
- Build type: Release

Data:
- размер входного массива: 200000 элементов
- значения генерируются псевдослучайным образом

Команды для запуска тестов:

mpiexec -n 2 .\ppc_func_tests.exe --gtest_filter=SabutayARadixSortDoubleMergeFuncTests/*
mpiexec -n 2 .\ppc_perf_tests.exe --gtest_filter=RunModeTests/SabutayARadixSortDoubleMergeRunPerfTestProcesses.*

## 7. Results and Discussion

### 7.1 Correctness

Корректность реализации проверялась:
- функциональными тестами для SEQ и MPI версий
- сравнением результата с эталонной сортировкой
- проверкой на граничных случаях
  (пустой массив, один элемент, одинаковые значения)

Все функциональные тесты были успешно пройдены.

### 7.2 Performance

Ниже приведены результаты измерения времени выполнения,
ускорения и эффективности для последовательной (SEQ)
и параллельной (MPI) реализаций.

Pipeline:

| Processes | Time (s) | Speedup vs MPI 1 | Efficiency |
|-----------|----------|------------------|------------|
| MPI 1     | 0.013409 | 1.00×            | 100%       |
| MPI 2     | 0.009157 | 1.46×            | 73%        |
| MPI 4     | 0.009773 | 1.37×            | 34%        |

Task Run:

| Processes | Time (s) | Speedup vs MPI 1 | Efficiency |
|-----------|----------|------------------|------------|
| MPI 1     | 0.010820 | 1.00×            | 100%       |
| MPI 2     | 0.017122 | 0.63×            | 32%        |
| MPI 4     | 0.026404 | 0.41×            | 10%        |

MPI-реализация демонстрирует ускорение в режиме Pipeline
при использовании двух процессов.

При дальнейшем увеличении числа процессов
эффективность снижается из-за роста накладных расходов
на межпроцессное взаимодействие.

В режиме Task Run ускорение не наблюдается,
так как стоимость коммуникаций и операций слияния
превышает выигрыш от параллельного выполнения.

## 8. Conclusions

В рамках работы были реализованы последовательная
и параллельная версии поразрядной сортировки
вещественных чисел с простым слиянием.

Проведённые эксперименты показали, что MPI-реализация
может обеспечивать умеренное ускорение
по сравнению с последовательной версией
в режиме pipeline при использовании нескольких процессов
при сохранении корректности результатов.

В то же время в режиме task_run ускорение не достигается,
а при увеличении числа процессов время выполнения возрастает.

Основным ограничением масштабируемости является
стоимость межпроцессного обмена данными
и операций слияния, которые начинают доминировать
над вычислениями при большом числе процессов.

## 9. References


- OpenMPI Documentation — https://www.open-mpi.org/doc/
- PPC Parallel Programming Course — https://learning-process.github.io/parallel_programming_course/
- GTest Framework — https://github.com/google/googletest
- Статья — https://www.cyberforum.ru/cpp-beginners/thread597195.html
- Anthony Williams.
  "C++ Concurrency in Action" —
  https://dodo.inm.ras.ru/konshin/HPC/bib/HPC-cxx11-book.pdf
- С. Макконнел.
  "Совершенный код. Практическое руководство по разработке
  программного обеспечения", 2014 г.