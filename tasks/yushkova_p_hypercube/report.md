# Гиперкуб

- Student: Юшкова Полина Александровна, group 3823Б1ПР2
- Technology: SEQ + MPI
- Variant: 10

## 1. Вступление

Топология гиперкуба является одной из базовых структур в параллельных вычислениях
за счет малого диаметра сети и предсказуемой схемы обмена между процессами.
Для задач, где требуется корректно агрегировать результаты между большим числом узлов,
гиперкубическая схема позволяет организовать обмен за логарифмическое число шагов.

В данной работе реализована задача подсчета числа ребер `n`-мерного гиперкуба
в двух вариантах: последовательном (`SEQ`) и параллельном (`MPI`).

Ожидаемый результат работы:

- корректный подсчет числа ребер гиперкуба для заданной размерности `n`;
- совпадение результата `MPI` и `SEQ`;
- прохождение функциональных и производительных тестов.

## 2. Постановка задачи

Требуется вычислить количество ребер `n`-мерного гиперкуба.

### Входные данные

Одно целое число:

- `n` - размерность гиперкуба.

### Выходные данные

Одно целое число:

- `E` - количество ребер гиперкуба размерности `n`.

### Математическая формула

Для `n`-мерного гиперкуба число ребер определяется как:

`E = n * 2^(n - 1)`

### Ограничения

- Для последовательной версии (`SEQ`):
  - `n > 0`;
  - верхняя граница ограничена типом данных и проверками в `Validation`
    (в текущей реализации недопустимы некорректные/слишком большие значения).
- Для параллельной версии (`MPI`):
  - число процессов `MPI` должно быть степенью двойки;
  - `n` должен проходить входную валидацию задачи.
- Используются только базовые средства `MPI` без построения топологии
  через специализированные MPI-конструкторы.

## 3. Базовый алгоритм (последовательный)

Последовательная версия реализует прямой подсчет числа ребер по определению гиперкуба
через обход вершин и их соседей по битовым измерениям.

### Идея

Каждая вершина `n`-мерного гиперкуба кодируется битовой маской длины `n` (числа от `0` до `2^n - 1`).  
Для каждой вершины существует ровно `n` соседей: каждый сосед получается инверсией одного бита.

Чтобы не считать одно и то же ребро дважды (из вершины `u` в `v` и обратно из `v` в `u`), используется правило:

- учитывать ребро только если `vertex < neighbor`.

### Шаги алгоритма

1. Прочитать входное значение `n`.
2. Проверить корректность входа в `Validation`:
   - `n` должно быть допустимым положительным значением.
3. Вычислить количество вершин:
   - `vertices = 1ULL << n`.
4. Инициализировать счетчик:
   - `total_edges = 0`.
5. Для каждой вершины `vertex` в диапазоне `[0, vertices)`:
   1. Для каждого измерения `bit` в диапазоне `[0, n)`:
      - вычислить соседа: `neighbor = vertex ^ (1ULL << bit)`;
      - если `vertex < neighbor`, увеличить `total_edges`.
6. Записать `total_edges` в выход задачи.

### Корректность

- Перебираются все вершины и все измерения, значит рассматриваются все возможные ребра.
- Условие `vertex < neighbor` гарантирует, что каждое ребро учитывается ровно один раз.
- Поэтому итоговый `total_edges` равен точному числу ребер гиперкуба.

### Оценка сложности

- Временная сложность: `O(n * 2^n)`.
- Дополнительная память: `O(1)`.

## 4. Схема распараллеливания

### 4.1. Распределение данных между процессами

Пусть:

- `P` - число MPI-процессов,
- `N = 2^n` - число вершин гиперкуба.

Вершины диапазона `[0, N)` делятся между процессами на почти равные непрерывные блоки:

- `chunk = N / P`,
- `remainder = N % P`,
- первые `remainder` процессов получают на одну вершину больше.

Для ранга `r`:

- `start = r * chunk + min(r, remainder)`,
- `local_size = chunk + (r < remainder ? 1 : 0)`,
- `end = start + local_size`.

Каждый процесс считает локальное число ребер только на своем поддиапазоне `[start, end)`.

### 4.2. Локальная вычислительная часть

На каждом процессе выполняется тот же базовый принцип:

- для каждой вершины `vertex` из локального диапазона;
- для каждого измерения `bit` от `0` до `n-1`;
- `neighbor = vertex ^ (1ULL << bit)`;
- считать ребро, если `vertex < neighbor`.

Итог - `local_edges`.

### 4.3. Схема связи и редукции (гиперкуб)

Для агрегации `local_edges` используется поразрядная схема обмена между рангами, эквивалентная редукции по гиперкубу:

- размерность коммуникационной схемы: `d = log2(P)`;
- партнер на шаге `k`: `partner = rank ^ (1 << k)`.

Логика шага:

- если у процесса бит `k` равен `1`, он отправляет текущую сумму партнеру и завершает редукцию;
- если бит `k` равен `0`, принимает сумму от партнера и добавляет к своей.

После `d` шагов итоговая сумма находится на ранге `0`.  
Далее выполняется `MPI_Bcast`, чтобы одинаковый результат получили все процессы.

### 4.4. Роли процессов

- Все ранги выполняют одинаковую локальную часть.
- В редукции роль ранга определяется текущим битом:
  - `sender` (бит = 1),
  - `receiver/accumulator` (бит = 0).
- Ранг `0` после редукции хранит глобальный итог и рассылает его через `MPI_Bcast`.

### 4.5. Короткий псевдокод

```text
local_edges = count_edges_on_local_range(rank)

sum = local_edges
for k in [0 .. log2(P)-1]:
    partner = rank xor (1 << k)
    if rank has bit k:
        send(sum, partner)
        break
    else:
        recv(tmp, partner)
        sum += tmp

broadcast(sum, root=0)
output = sum
```

## 5. Детали реализации

### 5.1. Ключевые классы и методы

- `YushkovaPHypercubeSEQ`
  - `ValidationImpl()` - проверка корректности входных данных.
  - `RunImpl()` - подсчет числа ребер через перебор вершин и битов.
- `YushkovaPHypercubeMPI`
  - `ValidationImpl()` - проверка входа и условия `world_size` как степени двойки.
  - `RunImpl()`:
    1. разбиение диапазона вершин между процессами;
    2. локальный подсчет `local_edges`;
    3. редукция по схеме гиперкуба;
    4. `MPI_Bcast` итогового значения.

### 5.2. Важные допущения

- Число процессов MPI для параллельной версии должно быть степенью двойки.
- Подсчет выполняется в целочисленном типе с запасом (`std::uint64_t`) для промежуточных значений.
- Каждое ребро считается один раз с условием `vertex < neighbor`.
- Последовательная версия используется как эталон для сравнения с MPI.

### 5.3. Примеры

- Для `n = 1`: `E = 1`.
- Для `n = 3`: `E = 12`.
- Для `n = 6`: `E = 192`.
- Для `n = 12`: `E = 24576`.

В функциональных тестах проверяется совпадение:

- результата `SEQ` с эталонной формулой,
- результата `MPI` с `SEQ` и эталоном.

### 5.4. Рекомендации по памяти

- Использованы 64-битные беззнаковые типы для количества вершин и ребер.
- Структура графа не хранится целиком (матрица/список смежности),
  так как это резко увеличивает память.
- В MPI хранить только локальные счетчики и небольшие временные переменные для редукции.

## 6. Экспериментальная настройка

### Аппаратное обеспечение и ОС

Запуски выполнялись в локальной Linux-среде.

Локально:

- ОС: Ubuntu Linux;
- запуск MPI-тестов: `mpirun --oversubscribe -n 8`.

### Набор инструментов

- Система сборки: `CMake`;
- Тип сборки: `Release`.

### Среда выполнения

Для MPI-прогонов использовалось:

- `-n 8` процессов (`mpirun --oversubscribe -n 8`).

Тестировались бинарники:

- `build/bin/ppc_func_tests`
- `build/bin/ppc_perf_tests`

Фильтр задачи:

- `--gtest_filter='*Hypercube*'`.

### Данные и способ формирования входа

- функциональные тесты:
  - наборы значений размерности гиперкуба `n` (включая малые и средние размерности);
- performance-тесты:
  - фиксированное входное значение `n = 25`,
  - режимы запуска `pipeline` и `task_run`.

## 7. Результаты и обсуждение

### 7.1 Корректность

Корректность реализации подтверждалась следующими проверками:

- сравнение результата с аналитической формулой для числа ребер гиперкуба:
  `E = n * 2^(n - 1)`;
- сравнение результатов `MPI` и `SEQ` для одинакового входа;
- проверка валидации входных параметров и условий запуска MPI;
- запуск функциональных тестов задачи.

Итог: функциональные проверки для задачи гиперкуба проходят успешно,
расхождений между `SEQ` и `MPI` для корректных входов не обнаружено.

### 7.2 Производительность

`mpirun --oversubscribe -n 8 ./ppc_perf_tests --gtest_filter='*Hypercube*'`

- `seq pipeline`: `0.7206496239` c
- `seq task_run`: `0.6821137905` c
- `mpi pipeline`: `0.0711399534` c
- `mpi task_run`: `0.0754848000` c

#### Pipeline mode

| Режим | Количество процессов | Время, с | Ускорение | Эффективность |
| --- | ---: | ---: | ---: | ---: |
| seq | 1 | 0.7206496239 | 1.0000 | N/A |
| mpi | 8 | 0.0711399534 | 10.1308 | 126.64% |

#### Task_run mode

| Режим | Количество процессов | Время, с | Ускорение | Эффективность |
| --- | ---: | ---: | ---: | ---: |
| seq | 1 | 0.6821137905 | 1.0000 | N/A |
| mpi | 8 | 0.0754848000 | 9.0364 | 112.96% |

Формулы:

- `Ускорение = T_seq / T_parallel`;
- `Эффективность = (Ускорение / p) * 100%`, где `p = 8`.

### Обсуждение

По данным текущего запуска MPI-версия показывает существенное сокращение времени
относительно SEQ. Эффективность выше 100% указывает на эффект сверхлинейного ускорения
в данных конкретных условиях измерения (влияние кеширования, особенностей прогрева,
различий в накладных расходах тестового контура и распределении работы).

Практический вывод: при выбранном размере задачи и параметрах прогона MPI-вариант обеспечивает заметный выигрыш по времени.

## 8. Выводы

В работе реализована задача подсчета числа ребер `n`-мерного гиперкуба
в двух вариантах: последовательном (`SEQ`) и параллельном (`MPI`).
Последовательная версия построена как эталон корректности, а параллельная версия
распределяет диапазон вершин между процессами и агрегирует частичные результаты
через редукцию по гиперкубической схеме с последующей рассылкой итога.

Основные результаты:

- реализованы и интегрированы обе версии задачи в инфраструктуру проекта;
- подтверждена корректность вычислений по функциональным тестам и аналитическому инварианту;
- по полученным замерам производительности MPI-версия показывает
  заметное сокращение времени относительно SEQ при запуске на 8 процессах.
