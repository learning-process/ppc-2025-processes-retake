# Гиперкуб

- Student: Юшкова Полина Александровна, group 3823Б1ПР2
- Technology: SEQ + MPI
- Variant: 10

## 1. Introduction

Топологии сетей передачи данных играют важную роль при проектировании параллельных и распределённых алгоритмов.

Гиперкуб (`n-dimensional hypercube`) - одна из базовых топологий:
она имеет логарифмический диаметр и хорошо масштабируется с ростом числа процессов.

Цель работы - реализовать две версии вычисления числа рёбер n-мерного гиперкуба:
последовательную (SEQ) и параллельную (MPI), и сравнить их поведение.

## 2. Problem Statement

Дано:

- размерность гиперкуба `n`.

Требуется:

- вычислить число рёбер гиперкуба `Q_n`.

### Input

Целое число:

- `n`.

### Output

Целое неотрицательное число:

- `E` - количество рёбер гиперкуба.

### Constraints

- `n > 0`;
- `n < 63` (ограничение для безопасной 64-битной арифметики);
- для MPI: число процессов должно быть степенью двойки.

## 3. Baseline Algorithm (Sequential)

Используется полный перебор вершин гиперкуба.

Алгоритм:

1. Проверить корректность `n`.
2. Вычислить `V = 2^n` - число вершин.
3. Для каждой вершины `v` и каждого бита `bit`:
   - `u = v xor (1 << bit)`;
   - если `v < u`, увеличить счётчик рёбер.
4. Вернуть итоговый счётчик.

## 4. Parallelization Scheme

### 4.1 Разбиение диапазона вершин

Пусть:

- `P` - число MPI-процессов,
- `r` - ранг процесса,
- `V = 2^n` - число вершин.

Для процесса `r`:

- `base = V / P`,
- `tail = V % P`,
- `local_count = base + (r < tail ? 1 : 0)`,
- `local_start = r * base + min(r, tail)`,
- `local_end = local_start + local_count`.

### 4.2 Локальный подсчёт

Каждый процесс считает рёбра только в своём диапазоне вершин по той же схеме `v < (v xor (1 << bit))`.

### 4.3 Редукция

Локальные суммы объединяются древовидно по маскам `1, 2, 4, ...` через `MPI_Sendrecv`,
после чего итог рассылается `MPI_Bcast` от ранга `0`.

## 5. Implementation Details

### Классы

- `YushkovaPHypercubeSEQ`
- `YushkovaPHypercubeMPI`

### Описание версий

- `SEQ`: полный перебор всех вершин и измерений, подсчёт рёбер по правилу `v < u`.
- `MPI`: разбиение множества вершин между процессами, локальный перебор, затем редукция частичных сумм.

## 6. Experimental Setup

### Окружение

- ОС: Linux (WSL2)
- MPI: Open MPI
- Сборка: `Release`

### Команды запуска

- `mpirun --oversubscribe -n 8 ./ppc_perf_tests --gtest_filter='*Hypercube*'`
- `./ppc_perf_tests --gtest_filter='*Hypercube*'`

## 7. Results and Discussion

### 7.1 Performance

Данные из запуска `mpirun --oversubscribe -n 8`:

- `yushkova_p_hypercube_seq_pipeline:pipeline:0.0000002337`
- `yushkova_p_hypercube_seq_task:task_run:0.0000000497`
- `yushkova_p_hypercube_mpi_pipeline:pipeline:0.0000845243`
- `yushkova_p_hypercube_mpi_task:task_run:0.0000610313`

Формулы:

- `Speedup = T_seq / T_par`
- `Efficiency = Speedup / P * 100%`, где `P = 8`

| Mode | Count | Time, s | Speedup | Efficiency |
| --- | ---: | ---: | ---: | ---: |
| seq pipeline | 1 | 0.0000002337 | 1.00000 | N/A |
| seq task_run | 1 | 0.0000000497 | 1.00000 | N/A |
| mpi pipeline | 8 | 0.0000845243 | 0.00276 | 0.0346% |
| mpi task_run | 8 | 0.0000610313 | 0.00081 | 0.0102% |

### Analysis

Для текущих размеров ускорение не наблюдается.

Причины:

- объём полезной работы на одну итерацию мал;
- накладные расходы коммуникаций и синхронизации доминируют;
- стоимость редукции становится заметной на малых задачах.

Теоретически редукция имеет `O(log P)` по числу шагов,
но на малых данных коммуникационные издержки перекрывают выигрыш от распараллеливания.

## 8. Conclusions

Реализованы две корректные версии решения задачи подсчёта числа рёбер гиперкуба:

- последовательная (полный перебор),
- MPI-версия (разбиение + редукция).

Практические измерения показывают, что для малых сообщений эффективность ограничивается накладными расходами MPI.
