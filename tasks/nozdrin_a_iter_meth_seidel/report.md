# Итерационные Методы. Метод Зейделя

- **Студент**: Ноздрин Артём Дмитриевич, группа 3823Б1ПР4
- **Технология**: SEQ | MPI
- **Вариант**: 19

## 1. Введение
В мире есть множество задач решение которых сводится к решению систем линейных алгебраических уравнений. Умение решать таких систем один из основных навыков в математике. Существует несколько подходов к решению данных систем: классические методы и численные методы.

Классические методы (Матричный, Крамера, Гауса) дают точный ответ, но к сожалению требуют большого количества вычислений. Численные методы позволяют ускорить вычисления получить ответ с определенной, рассчитываемой погрешностью за счет дополнительных ограничений.

Иттерационные методы - это численные методы в которых рассчет конкретного значения разбивается на итерации. На каждой следующей итерации погрещность уменьшается, результат становится более точным.

Цель моей работы - реализовать итерационый метод Зейделя, распараллелить его работу при помощи MPI.

## 2. Постановка задачи
Дана система линейных алгебраических уравнений вида: `Ax = B` и параметр `Eps`

- `A` - матрица коэфициентов неизвестных размера `n * n`
- `x` - вектор неизвестных размера `n`
- `B` - вектор свободных членов размера `n`
- `Eps` - максимальное значение (абсолютной) погрешности

Требуется решить систему уравнений при помощи метода Зейделя - получить значения вектора `x` с максимальной погрешностью `Eps`, то есть если известно точное решение `x*`, то максимальный элемент из разницы векторов `x* - x`, по модулю должен быть <= `Eps`.

Тип входных данных:
```cpp
using InType = std::tuple<int /* n */, 
               std::vector<double> /* A */, 
               std::vector<double> /* B */, 
               double /* Eps */>;
```

Тип выходных данных:
```cpp
using OutType = std::vector<double>;
```
Ограничения:
- Входная система уравнений должна быть решаема
- Матрица `A` должна иметь диагональное преобладание

## 3. Базовый алгоритм (последовательная версия) 
Алгоритм представляет следующую последовательность шагов:
1. Приведение системы уравнений `Ax = B` в вид `Ex = Cx + D` (`E` - единичная матрица)
2. Выбор начальных значений `X0` (традиционно все элементы вектора равны 0)
3. Организуется итерационный процесс: (Здесь и далее x Число - номер в векторе X, [Число] - номер итерации )

    `xk [m+1] = ck1*x1 [m] + ck2*x2 [m] + ... + ckn *xn [m]`
    
    Если для данной итерации [m+1] известны более ранние x из правой части, то подставляем их.

    Пример: 
    `x1 [1] = c11*x1 [0] + c12*x2 [0] + ... + c1n *xn [0]`

    `x2 [1] = c11*x1 [1] + c12*x2 [0] + ... + c1n *xn [0]`
    Итак далее
4. Рассчитываем погрешность: `Eps = max(abs(X[m+1] - X[m]))` - находим максимальный элемент разницы векторов
5. Если `Eps` > заданного задачей повторяем итерацию

Код Алгоритма:
```cpp
  std::vector<double> x(n, 0);
  std::vector<double> iter_eps(n, -1);
  do {
    for (int i = 0; i < n; i++) {
      double iter_x = b[i];
      for (int j = 0; j < i; j++) {
        iter_x = iter_x - a[(i * n) + j] * x[j];
      }
      for (int j = i + 1; j < n; j++) {
        iter_x = iter_x - a[(i * n) + j] * x[j];
      }
      iter_x = iter_x / a[(i * n) + i];

      // обновление погрешности
      iter_eps[i] = std::fabs(iter_x - x[i]);
      // обновление полученного корня
      x[i] = iter_x;
    }
  } while (EpsOutOfBound(iter_eps, eps));
```

**Характеристики:**
Оценить временную сложноть алгоритма достаточно сложно, так как все зависит от входных данных. Если значения матрицы `A` обеспечивают высокую сходимость, то сложность алгоритма составляет `O(N)`. В противных случая алгоритм может выполняться большее число итераций.

## 4. Схема распараллеливания

Основной проблемой распараллеливания Метода Зейделя являеется то, что на каждой итерации используются обновленные значения, если они доступны. Прямая реализация превращает параллельный алгоритм в последовательный.

Наиболее распространненый подход параллелизации - **Блочное разбиение**. Система разделяется на блоки между всеми процессами. Каждый процесс обрабатывает матрицу на своем участке при помощи Метода Зейделя, в конце итерации происходит синхронизация.

### 4.1 Алгоритм Распределения

**Код**
```cpp
int rank = 0;
  int mpi_size = 0;
  MPI_Comm_rank(MPI_COMM_WORLD, &rank);
  MPI_Comm_size(MPI_COMM_WORLD, &mpi_size);

  double *a = 0;
  double *b = 0;

  int n = 0;
  if (rank == 0) {
    n = std::get<0>(GetInput());
    a = std::get<1>(GetInput()).data();
    b = std::get<2>(GetInput()).data();
  }
  MPI_Bcast(&n, 1, MPI_INT, 0, MPI_COMM_WORLD);

  int step = n / mpi_size;
  int remainder = n % mpi_size;

  std::vector<int> send_counts(mpi_size, step);
  std::vector<int> displacements(mpi_size, 0);
  //расчет смещений и количества отправленных
  //для разделения вектора B всем процессам
  std::vector<double> local_b(send_counts[rank]);
  MPI_Scatterv(b, send_counts.data(), displacements.data(), MPI_DOUBLE, local_b.data(), local_b.size(), MPI_DOUBLE, 0, MPI_COMM_WORLD);
  //расчет смещений и количества отправленных
  //для разделения матрицы B всем процессам
  std::vector<double> local_a(send_counts[rank]);
  MPI_Scatterv(a, send_counts.data(), displacements.data(), MPI_DOUBLE, local_a.data(), local_a.size(), MPI_DOUBLE, 0, MPI_COMM_WORLD);
```

**Описание**
1. Процессы узнаю свой ранг и кол-во процессов исполняющих программу
2. Процесс с `рангом 0` получает входные данные
3. Все процессы расчитывают кол-во данных, получаемых от 0 процесса, и их смещения относительно исходного вектора 
4. Происходит распредление данных при помощи операции `MPI_Scatterv()` 

## 4.2 Алгоритм Подсчета
**Код**
```cpp
  std::vector<double> x(n, 0);                           // вектор ответа
  std::vector<double> x_new(local_b.size(), 0);          // вектор для рассылки
  std::vector<double> iter_eps(n, -1);                   // вектор погрешности
  std::vector<double> iter_eps_new(local_b.size(), -1);  // вектор для рассылки
  do {
    for (int i = 0; i < static_cast<int>(local_b.size()); i++) {
      int g_row = displacement + i;  // позиция строки в общей матрице
      double iter_x = local_b[i];    // результат на итерации
      // циклы с суммой без элмента диагонали
      for (int j = 0; j < g_row; j++) {
        iter_x = iter_x - local_a[(i * n) + j] * x[j];
      }
      for (int j = g_row + 1; j < n; j++) {
        iter_x = iter_x - local_a[(i * n) + j] * x[j];
      }
      iter_x = iter_x / local_a[(i * n) + g_row];  // вычисление корня стоящего на диагонали
      // обновление погрешности
      iter_eps[g_row] = std::fabs(iter_x - x[g_row]);
      iter_eps_new[i] = iter_eps[g_row];
      // обновление полученного корня
      x[g_row] = iter_x;
      x_new[i] = iter_x;
    }
    MPI_Allgatherv(x_new.data(), x_new.size(), MPI_DOUBLE, x.data(), send_counts.data(), displacements.data(),  MPI_DOUBLE, MPI_COMM_WORLD);
    MPI_Allgatherv(iter_eps_new.data(), iter_eps_new.size(), MPI_DOUBLE, iter_eps.data(), send_counts.data(), displacements.data(), MPI_DOUBLE, MPI_COMM_WORLD);
  } while (InEpsBound(iter_eps, eps));
```
**Описание**
Каждый процесс выполнеят на своем участке итерации метода Зейделя, в конце каждой итерации происходит синхронизация - получение и отправка посчитанных результатов и погрешности на своем участке при помощи MPI_Allgather()

## 4.3 Получение конечного результата
В конце каждой итерации происходит синхронизация, следовательно, когда завершится подсчет все процессы всегда будут значть итоговый результат.

## 4.4 Схема работы программы
```
┌──────────────────────────────────────────────────────────┐
│  Входные данные ( матрицы A, вектор В, погрешность Eps)  │
└────────┬─────────────────────────────────────────────────┘
         │ (передача входных аргументов программе)
         ↓
 ┌─────────────────────┬──────────────────────┬────────────────────────┐
 │  Процесс 0          │  Процесс 1           │  Процесс 2             │  
 │ получает входные    │ Рассчитывает индексы | Рассчитывает индексы   |
 | данные              | для получения данных | для получения данных   |
 └───────┬─────────────┴──────────────────────┴────────────────────────┘
         │ передача данных MPI_Scatter
         ↓
 ┌────────────────┬────────────────┬────────────────┐
 │ local_a(0)     │ local_a(1)     │ local_a(2)     │
 | local_b(0)     │ local_b(1)     │ local_b(1)     │
 └───────┬────────┴────────────────┴────────────────┘
         │ локальное вычислени итерации  <--------------------+
         ↓                                                    |
 ┌───────────────┬─────────────────┬─────────────────┐        | 
 │    new_x(0)   │    new_x(1)     │    new_x(2)     │        |
 |   new_eps(0)  |   new_eps(1)    |   new_eps(2)    |        |
 └───────┬───────┴─────────────────┴─────────────────┘        |
         │ MPI_Allgather()  синхронизация                     |
         ↓                                                    |
┌───────────────────────────────────┐                         |
│   Проверка значения погрешности   │-------------------------+
└────────┬──────────────────────────┘
         │ Погрешность <= Заданной начальными условиями
         ↓
┌───────────────────────────────────────┐  
│  Сохранение: GetOutput() = X          │
└───────────────────────────────────────┘
```

## 5. Детали реализации
**Структура проекта**
|          Файл          |                 Назначение                  |
|------------------------|---------------------------------------------|
| `common.hpp`           | Определение входных и выходных типов задачи |
| `ops_seq.hpp/.cpp`     |         Последовательная реализация         |
| `ops_mpi.hpp/.cpp`     |                MPI-реализация               |
| `functional/main.cpp`  |             Функциональные тесты            |
| `performance/main.cpp` |       Тестирование производительности       |

## 6. Экспериментальная среда

|  Компонент |               Значение                       |
|------------|----------------------------------------------|
|     CPU    |           Apple M2 (8 cores)                 |
|     RAM    |                 16 GB                        |
|     ОС     | OS: Ubuntu 24.04 (DevContainer / macOs 15.6) |
| Компилятор | GCC 13.3.0 (g++), C++20, CMake, Release      |
|     MPI    |        mpirun (Open MPI) 4.1.6               |

Тестовые данные: 
1. Функциональные тесты: 
    - тестовые данные из фалов
    - сгенерированные системы с числом неизвестных: 4, 16
2. Перформанс тесты:
    - сгенерированныя системы с числом неизвестных

## 7. Результаты и обсуждение

### 7.1 Корректность
Функциональные тесты используют входные файлы или генерацию в зависимости от параметра теста.

Струкутра параметра теста:
- число неизвестных
- строка 
Тест считывает второй аргумент параметра и, если он не равен `"gen"`, 
считывает входной файл, в котором хранятся: матрица `A`, вектор `B`, вектор ответов `X`.
Входные файлы имеют следующие наименования:
```
test_1.txt
test_2.txt
...
```
Если тест во втором аргументе параметра считал строку `"gen"`, то система уравнений будет 
сгенерирована с количеством неизвестных из первого параметра

Обе реализации (`SEQ`, `MPI`) прошли все тесты успешно.

### 7.2 Производительность

Количество неизвечтных `2000`
| Mode        | Count | Time, s | Speedup | Efficiency |
|-------------|-------|---------|---------|------------|
| seq         | 1     | 0.06    | 1.00    | N/A        |
| omp         | 2     | 0.06    | 1.00    | 50%        |
| omp         | 4     | 0.06    | 1.00    | 25%        |

Количество неизвечтных `5000`
| Mode        | Count | Time, s | Speedup | Efficiency |
|-------------|-------|---------|---------|------------|
| seq         |  1    |  0.64   | 1.00    | N/A        |
| omp         |  2    |  0.354  | 1.81    | 90.5%      |
| omp         |  4    |  0.412  | 1.56    | 38.8%      |

Тесты Производительности выполнялись без pipeline тестов, так как для матриц 3000х3000 элементов, время выполнения на моей установке, превышало лимит.

Результаты тестов показывают, что для более корректной оценки нужна система большего размера, так как операции взаимодействия процессов забирают достаточную часть производительности
## 8. Заключение
В ходе выполнения работы:
- реализовал Итерационный метод Зейделя
- распараллелил его при помощи технологии MPI
- рассчитал характекристик ускорение и эффективности для распараллеленного алгоритма
- выяснил, что параллельная обработка позволяет ускорить алгоритм при больших размерах системы

## 9. Источники
1. Сысоев А. В.              Курс лекций по параллельному программированию
2. В.П. Гергель, Р.Г.        Стронгин Основы параллельных вычислений для многопроцессорных вычислительных систем
3. Н.В. Копченова И.А.       Марон Вычислительная математика в примерах и задачах
4. Документация Open MPI     https://www.open-mpi.org/doc/
5. Microsoft Функции MPI     https://learn.microsoft.com/ru-ru/message-passing-interface/mpi-functions
   
## Приложения

### Генерация системы уравнений
```cpp
void GenerateTestData(int n, int seed) {
    std::vector<double> x(n, 0.0);
    std::vector<double> a(n * n, 0.0);
    std::vector<double> b(n, 0.0);

    std::mt19937 gen(seed);
    std::uniform_real_distribution<double> dist_coeff(0.0, 1.0);
    std::uniform_real_distribution<double> dist_solution(-10.0, 10.0);

    for (int i = 0; i < n; i++) {
      x[i] = dist_solution(gen);
    }
    // Генерируем матрицу с диагональным преобладанием
    for (int i = 0; i < n; i++) {
      double row_sum = 0.0;
      for (int j = 0; j < n; j++) {
        if (i != j) {
          a[i * n + j] = dist_coeff(gen);
          row_sum += std::abs(a[i * n + j]);
        }
      }
      a[i * n + i] = row_sum + 1.0 + dist_coeff(gen);  // гарантируем преобладание
    }
    // Вычисляем правую часть
    for (int i = 0; i < n; i++) {
      b[i] = 0.0;
      for (int j = 0; j < n; j++) {
        b[i] += a[i * n + j] * x[j];
      }
    }
    input_data_ = std::make_tuple(n, a, b, task_eps_);
    correct_data_ = x;
  }
```
