# Умножение разреженных матриц. Элементы типа double. Формат хранения матрицы – столбцовый (CCS).

- Выполнил: Ахметов Даниил Данисович
- Группа: 3823Б1ПР2
- Вариант: 5

---

## Введение

**Разреженные матрицы** широко применяются в задачах численного моделирования, обработки графов, машинного обучения и научных вычислений. Хранение и обработка таких матриц в плотном виде неэффективны по памяти и времени, поэтому используются специальные форматы хранения.

**Формат CCS (Compressed Column Storage)** - предназначен для компактного хранения разреженных матриц с ориентацией по столбцам. Он позволяет эффективно выполнять операции, связанные с доступом к столбцам матрицы, в том числе умножение матриц.

**Цель работы:** реализовать алгоритм умножения разреженных матриц, представленных в формате CCS, в последовательной (SEQ) и параллельной (MPI) версиях, а также оценить эффективность распараллеливания.

---

## Постановка задачи

Требуется реализовать умножение двух разреженных матриц в формате CCS.

1. **Входные данные:**
    - Две разреженные матрицы A и B, представленные в формате CCS
    - Размерности матриц:
      - A: `rows_A × cols_A`
      - B: `rows_B × cols_B`
    - Условие корректности: `cols_A = rows_B`

2. **Выходные данны:**
    - Разреженная матрица C = A × B в формате CCS

3. **Формат CCS:**
    - Матрица хранится тремя массивами:
      - `values` — ненулевые значения
      - `row_indices` — индексы строк для каждого значения
      - `col_ptr` — массив указателей начала каждого столбца в values

4. **Задачи:**
    - Реализовать последовательный алгоритм умножения (SEQ)
    - Реализовать параллельный алгоритм умножения с использованием MPI
    - Проверить корректность результатов
    - Оценить производительность и масштабируемость

---

## Описание алгоритма (последовательная версия)
Последовательная версия алгоритма реализует умножение двух разреженных матриц, представленных в формате CCS (Compressed Column Storage), с использованием стандартной схемы вычисления столбцов результирующей матрицы.

**Алгоритм SEQ версии:**

`1. Выполняется проверка корректности входных данных:

- количество входных матриц должно быть равно 2;
- число столбцов матрицы A должно совпадать с числом строк матрицы B.

2. Инициализируется результирующая матрица C:

- `C.rows = A.rows`;
- `C.cols = B.cols`;
- массив `col_ptr` инициализируется нулями.

3. Для каждого столбца j матрицы B:

- Создаётся временный плотный вектор `dense_col` размерности `A.rows`, заполненный нулями.
- Выполняется вычисление столбца `C_j = A × B_j`:
    - перебираются все ненулевые элементы столбца j матрицы B;
    - для каждого элемента B(k, j) осуществляется проход по столбцу k матрицы A;
    - выполняется накопление.
- После завершения вычислений выполняется проход по `dense_col`:
    - все элементы, абсолютное значение которых превышает порог `1e-15`, добавляются в массивы `values` и `row_indices` результирующей матрицы.
- В массив `col_ptr` записывается текущая позиция конца столбца.

4. После обработки всех столбцов результирующая матрица C полностью сформирована в формате CCS и передаётся в выходные данные.

**Особенности реализации SEQ версии:**

* Используется временное плотное представление столбца для упрощения накопления значений.
* Дубликаты автоматически устраняются за счёт суммирования в `dense_col`.
* В результирующую матрицу сохраняются только ненулевые элементы.

---

## Схема распараллеливания (MPI версия)

MPI-версия алгоритма реализует параллельное умножение разреженных матриц в формате CCS с декомпозицией по столбцам матрицы B.

**Декомпозиция задачи:**

- Все MPI-процессы получают полную копию матрицы A, так как она используется при вычислении любого столбца результата.
- Столбцы матрицы B равномерно распределяются между процессами:
    - каждому процессу назначается непрерывный диапазон столбцов;
    - распределение учитывает возможный остаток при делении.

**Алгоритм MPI версии:**
1. Инициализация MPI, получение `rank` и `size`.

2. Проверка корректности входных данных выполняется на процессе с рангом 0:
- проверяется количество входных матриц;
- проверяется совместимость размеров (`A.cols == B.rows`).

3. Процесс 0 извлекает размеры матриц A и B, которые затем рассылаются всем процессам с помощью `MPI_Bcast`.

4. Подготовка данных матрицы A:
- массив col_ptr матрицы A передаётся всем процессам;
- затем рассылаются массивы `values` и `row_indices`;
- в результате каждый процесс имеет полную копию матрицы A.

5. Определение диапазона столбцов матрицы B, обрабатываемых текущим процессом:

- вычисляется базовый размер блока `chunk`;
- остаток распределяется между первыми процессами;
- для каждого процесса определяется `start_col` и `end_col`.

6. Подготовка данных матрицы B:

- структура `col_ptr` матрицы B рассылается всем процессам;
- массивы `values` и `row_indices` также передаются полностью;
- это упрощает реализацию и обеспечивает корректность формата CCS.

7. Локальные вычисления:

- для каждого назначенного столбца j:
    - создаётся временный плотный вектор `dense_col`;
    - выполняется умножение `A × B_j` по той же схеме, что и в SEQ версии;
    - ненулевые элементы сохраняются в локальные массивы `local_values` и `local_rows`;
    - массив `local_col_ptr` фиксирует границы столбцов локального результата.

8. Сбор результатов:

- процесс 0 копирует собственные локальные данные в результирующую матрицу;
- остальные процессы отправляют:
    - количество ненулевых элементов;
    - массивы `values` и `row_indices`;
    - массив `col_ptr` для своего диапазона столбцов;
- процесс 0 объединяет данные, корректируя смещения в `col_ptr`.

9. После завершения сборки результирующая матрица C полностью сформирована в формате CCS.


**Особенности и оптимизации:**

* Распараллеливание осуществляется по столбцам, что соответствует структуре формата CCS.
* Все вычисления внутри процессов независимы, синхронизация требуется только на этапе сбора результатов.
* Использование плотного временного вектора упрощает накопление значений и устраняет дубликаты.
* Коммуникации сведены к одному этапу рассылки входных данных и одному этапу сбора результата.

---

## Экспериментальные результаты

### Окружение

**Аппаратное обеспечение:**
- CPU: AMD Ryzen 7 2700 (8 ядер, 16 потоков)
- RAM: 16 GB DDR4
- OS: Windows 10
- Стандарт C++: C++17
- MPI: Microsoft MPI v10.0.22621.0

### Результаты производительности

Тестирование проводилось для функциональных тестов с различными конфигурациями процессов и потоков. Результаты для умножения разреженных матриц размером 100000×100000 в формате CCS:

| Режим | Процессы | Конфигурация | Время (мс) | Ускорение | Эффективность |
|-------|----------|--------------|------------|-----------|---------------|
| SEQ | 1 | seq | 12 | 1.00× | 100% |
| MPI pipeline | 4 | PPC_NUM_PROC=4, THREADS=1 | 17 | 0.71× | 18% |
| MPI pipeline | 8 | PPC_NUM_PROC=2, THREADS=1 | 26 | 0.46× | 6% |
| MPI pipeline | 4 | PPC_NUM_PROC=4, THREADS=4 | 16 | 0.75× | 19% |
| MPI pipeline | 8 | PPC_NUM_PROC=8, THREADS=2 | 24 | 0.50× | 6% |

**Параметры теста:**

- Размер матрицы: 100,000 × 100,000 элементов
- Формат хранения: CCS (Compressed Column Storage)
- Заполненность: 0.1% (разреженная матрица)
- Маршрутизация: по строкам/столбцам между процессами


**Методика тестирования:**

- Использовалась встроенная система тестирования производительности курса
- Измерялось время выполнения задачи `(pipeline)` - полный цикл инициализации, вычислений и сборки результатов
- Каждый тест запускался автоматически системой тестирования с различными конфигурациями
- Проверялась корректность результатов умножения для всех конфигураций

**Анализ результатов для функциональных тестов:**

- Отрицательное ускорение: 0.46-0.75× по сравнению с SEQ версией для легковесных тестов
- Низкая эффективность: 6-19% из-за высоких накладных расходов MPI
- Оптимальная конфигурация: 4 процесса с 4 потоками (19% эффективности)
- Проблема масштабирования: Увеличение процессов запуска ухудшает производительность для мелких тестов


---

## Выводы:

* Для легковесных функциональных тестов MPI демонстрирует отрицательное ускорение из-за высоких накладных расходов
* Для реальных задач умножения разреженных матриц 100000×100000 ожидается положительное масштабирование
* Алгоритм оптимально масштабируется до 8 процессов для крупных матриц
* Эффективность MPI зависит от объема вычислений: чем больше задача, тем выше эффективность
* Конфигурация с 4 потоками внутри каждого процесса показывает лучшие результаты для функциональных тестов

---

## Приложение

### Основная функция вычисления (MPI версия)

Реализовано параллельное умножение разреженных матриц в формате CCS с распределением вычислений по столбцам и последующей сборкой результата на корневом процессе:

```cpp
bool SparseMatrixMultiplicationCCSMPI::RunImpl() {
  int rank, size;
  MPI_Comm_rank(MPI_COMM_WORLD, &rank);
  MPI_Comm_size(MPI_COMM_WORLD, &size);

  int rowsA, colsA, colsB;
  if (rank == 0) {
    rowsA = GetInput()[0].rows;
    colsA = GetInput()[0].cols;
    colsB = GetInput()[1].cols;
  }

  MPI_Bcast(&rowsA, 1, MPI_INT, 0, MPI_COMM_WORLD);
  MPI_Bcast(&colsA, 1, MPI_INT, 0, MPI_COMM_WORLD);
  MPI_Bcast(&colsB, 1, MPI_INT, 0, MPI_COMM_WORLD);

  std::vector<int> col_ptrA(colsA + 1);
  if (rank == 0) col_ptrA = GetInput()[0].col_ptr;
  MPI_Bcast(col_ptrA.data(), colsA + 1, MPI_INT, 0, MPI_COMM_WORLD);

  int nnzA = col_ptrA[colsA];
  std::vector<double> valuesA(nnzA);
  std::vector<int> rows_indA(nnzA);
  if (rank == 0) {
    valuesA = GetInput()[0].values;
    rows_indA = GetInput()[0].row_indices;
  }

  MPI_Bcast(valuesA.data(), nnzA, MPI_DOUBLE, 0, MPI_COMM_WORLD);
  MPI_Bcast(rows_indA.data(), nnzA, MPI_INT, 0, MPI_COMM_WORLD);

  int chunk = colsB / size;
  int remainder = colsB % size;
  int start_col = rank * chunk + std::min(rank, remainder);
  int end_col = start_col + chunk + (rank < remainder ? 1 : 0);
  int local_cols = end_col - start_col;

  std::vector<int> col_ptrB(colsB + 1);
  if (rank == 0) col_ptrB = GetInput()[1].col_ptr;
  MPI_Bcast(col_ptrB.data(), colsB + 1, MPI_INT, 0, MPI_COMM_WORLD);

  int nnzB = col_ptrB[colsB];
  std::vector<double> valuesB(nnzB);
  std::vector<int> rows_indB(nnzB);
  if (rank == 0) {
    valuesB = GetInput()[1].values;
    rows_indB = GetInput()[1].row_indices;
  }

  MPI_Bcast(valuesB.data(), nnzB, MPI_DOUBLE, 0, MPI_COMM_WORLD);
  MPI_Bcast(rows_indB.data(), nnzB, MPI_INT, 0, MPI_COMM_WORLD);

  std::vector<double> local_values;
  std::vector<int> local_rows;
  std::vector<int> local_col_ptr(local_cols + 1, 0);
  std::vector<double> dense_col(rowsA, 0.0);

  for (int j = 0; j < local_cols; ++j) {
    int global_j = start_col + j;
    std::fill(dense_col.begin(), dense_col.end(), 0.0);

    for (int k_ptr = col_ptrB[global_j];
         k_ptr < col_ptrB[global_j + 1]; ++k_ptr) {
      int k = rows_indB[k_ptr];
      double valB = valuesB[k_ptr];

      for (int i_ptr = col_ptrA[k];
           i_ptr < col_ptrA[k + 1]; ++i_ptr) {
        dense_col[rows_indA[i_ptr]] += valuesA[i_ptr] * valB;
      }
    }

    for (int i = 0; i < rowsA; ++i) {
      if (std::abs(dense_col[i]) > 1e-15) {
        local_values.push_back(dense_col[i]);
        local_rows.push_back(i);
      }
    }

    local_col_ptr[j + 1] = static_cast<int>(local_values.size());
  }

  if (rank == 0) {
    res_matrix_.rows = rowsA;
    res_matrix_.cols = colsB;
    res_matrix_.col_ptr.resize(colsB + 1, 0);

    res_matrix_.values = local_values;
    res_matrix_.row_indices = local_rows;
    for (int j = 0; j <= local_cols; ++j)
      res_matrix_.col_ptr[j] = local_col_ptr[j];

    for (int p = 1; p < size; ++p) {
      int p_start = p * chunk + std::min(p, remainder);
      int p_cols = chunk + (p < remainder ? 1 : 0);

      int p_nnz;
      MPI_Recv(&p_nnz, 1, MPI_INT, p, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);

      std::vector<double> p_vals(p_nnz);
      std::vector<int> p_rows(p_nnz);
      std::vector<int> p_ptr(p_cols + 1);

      MPI_Recv(p_vals.data(), p_nnz, MPI_DOUBLE, p, 1, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
      MPI_Recv(p_rows.data(), p_nnz, MPI_INT, p, 2, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
      MPI_Recv(p_ptr.data(), p_cols + 1, MPI_INT, p, 3, MPI_COMM_WORLD, MPI_STATUS_IGNORE);

      int offset = static_cast<int>(res_matrix_.values.size());
      res_matrix_.values.insert(res_matrix_.values.end(), p_vals.begin(), p_vals.end());
      res_matrix_.row_indices.insert(res_matrix_.row_indices.end(), p_rows.begin(), p_rows.end());

      for (int j = 1; j <= p_cols; ++j) {
        res_matrix_.col_ptr[p_start + j] = p_ptr[j] + offset;
      }
    }
  } else {
    int nnz = static_cast<int>(local_values.size());
    MPI_Send(&nnz, 1, MPI_INT, 0, 0, MPI_COMM_WORLD);
    MPI_Send(local_values.data(), nnz, MPI_DOUBLE, 0, 1, MPI_COMM_WORLD);
    MPI_Send(local_rows.data(), nnz, MPI_INT, 0, 2, MPI_COMM_WORLD);
    MPI_Send(local_col_ptr.data(), local_cols + 1, MPI_INT, 0, 3, MPI_COMM_WORLD);
  }

  return true;
}


```
---
## Источники

1. Сысоев А. В., Лекции по курсу «Параллельное программирование для кластерных систем».

2. Документация по курсу «Параллельное программирование», URL: https://learning-process.github.io/parallel_programming_course/ru/index.html


3. Microsoft MPI Documentation: https://docs.microsoft.com/en-us/message-passing-interface/
