# Вычисление скалярного произведения векторов

* Студент: Ноздрин Артём Дмитриевич
* Группа: 3823Б1ПР4
* Вариант: 9  
* Технологии: SEQ, MPI

---

## 1. Введение

Вычисление скалярного произведения векторов является одной из фундаментальных операций линейной алгебры и широко применяется в различных областях науки и техники: от машинного обучения и компьютерной графики до физического моделирования и решения систем линейных уравнений. В современных приложениях, оперирующих большими массивами данных или результатами сложных симуляций, размерность векторов может достигать миллионов элементов, что требует значительных вычислительных ресурсов.

Прямое вычисление скалярного произведения сводится к суммированию попарных произведений соответствующих компонент векторов. Несмотря на простоту алгоритма, его реализация «в лоб» может стать узким местом производительности при обработке больших объемов данных. Ключевым преимуществом данной задачи является её естественная параллелизуемость, так как вычисление попарных произведений для разных пар элементов может выполняться независимо, а затем результаты суммируются.

Цель данной работы — реализовать вычисление скалярного произведения векторов в последовательном (SEQ) и параллельном (MPI) вариантах, провести тестирование точности, сравнение с аналитическими значениями и оценку производительности параллельной версии.

---

## 2. Постановка задачи

Пусть заданы два вещественных вектора **a** и **b** одинаковой размерности **n**:

**a** = (a₀, a₁, …, aₙ₋₁),  
**b** = (b₀, b₁, …, bₙ₋₁).

Необходимо вычислить их скалярное произведение:

\[
c = \sum_{i=0}^{n-1} a_i \cdot b_i
\]

### Входные данные

Векторы генерируются случайным образом в заданном диапазоне (по умолчанию [-100, 100]). Размер векторов **n** задаётся в тестовых программах отдельно для функционального и производительного тестирования.

### Результат

Одно число типа double — приближённое значение скалярного произведения (с точностью до погрешностей округления).

---

## 3. Последовательная реализация (SEQ)

Реализация оформлена как класс `NozdrinAScalarMultVectorsSEQ`, наследующий `ppc::task::Task`. Валидация проверяет, что оба входных вектора не пусты и имеют одинаковый размер. Предварительная обработка обнуляет выход.

Основной цикл выполняется в `RunImpl`:

```cpp
bool NozdrinAScalarMultVectorsSEQ::RunImpl() {
  const auto &in = GetInput();

  double sum = 0.0;
  for (std::size_t i = 0; i < in.a.size(); ++i) {
    sum += in.a[i] * in.b[i];
  }

  GetOutput() = sum;
  return true;
}
```

* Время: O(n)
* Память: O(1)

Последовательный вариант служит базой для проверки корректности и сравнения скорости.

---

## 4. Параллельная реализация (MPI)

Класс `NozdrinAScalarMultVectorsMPI` использует ту же валидацию, что и SEQ. Все процессы получают одинаковые входные данные через `MPI_Bcast`, поэтому результат доступен на каждом процессе.

### Распределение нагрузки

Размер задачи транслируется всем рангам, после чего входные векторы рассылаются целиком. Для равномерности используется деление с остатком:

```cpp
const std::uint64_t base = n / size;
const std::uint64_t rem = n % size;
const std::uint64_t start = rank * base + std::min<std::uint64_t>(rank, rem);
const std::uint64_t end = start + base + (rank < rem ? 1 : 0);
```

### Локальные вычисления и сбор

Каждый процесс суммирует произведения элементов на своём отрезке, затем результат агрегируется:

```cpp
double local_sum = 0.0;
for (std::uint64_t i = start; i < end; ++i) {
  local_sum += in.a[i] * in.b[i];
}

double global_sum = 0.0;
MPI_Reduce(&local_sum, &global_sum, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);
MPI_Bcast(&global_sum, 1, MPI_DOUBLE, 0, MPI_COMM_WORLD);
GetOutput() = global_sum;
```

`MPI_Bcast` результата гарантирует, что итоговая сумма доступна всем процессам, что упрощает тестирование и повторное использование.

---

## 5. Программная реализация

### Структура

* `common/include/common.hpp` — структура `Input` с векторами `a` и `b`, генератор случайных векторов `GenerateRandomVector`, проверочный расчёт `AnalyticDotProduct`.
* `seq/` — класс `NozdrinAScalarMultVectorsSEQ` (последовательный расчёт, цикл по всем элементам).
* `mpi/` — класс `NozdrinAScalarMultVectorsMPI` (распределение данных, локальные суммы, `MPI_Reduce` + `MPI_Bcast`).
* `tests/functional/` — проверка корректности для фиксированных и случайных входов, тест на невалидные размеры.
* `tests/performance/` — прогон производительности для векторов длиной 1 000 000 элементов в SEQ и MPI.

### Сборка и запуск

Сборка через CMake из корня репозитория:

```bash
cmake -S . -B build
cmake --build build
ctest --test-dir build -R nozdrin_a_scalar_mult_vectors -VV
```

Для MPI-тестов использовать `mpiexec`, указав число процессов (например, `mpiexec -n 4 ctest ...`).

---

## 6. Экспериментальная часть

### Набор тестов

* **Функциональные** (`tests/functional/main.cpp`):
  * Случайные векторы длиной 100.
  * Детерминированные проверки для SEQ и MPI.
  * Негативный кейс — разные размеры входных векторов (валидация должна провалиться).
* **Производительность** (`tests/performance/main.cpp`):
  * Векторы длиной 1 000 000 элементов для SEQ и MPI.
  * Запуск рекомендуется в релизной сборке с указанием числа процессов через `mpiexec`.

### Аппаратная конфигурация

Для актуальных измерений следует фиксировать ОС, компилятор и версию MPI. В репозитории применима стандартная связка: CMake + GoogleTest; для Windows можно использовать MSVC и Microsoft MPI, для Linux — GCC/Clang и OpenMPI/MPICH.

---

## 7. Результаты и проверка

* Функциональные тесты сравнивают результат с `AnalyticDotProduct` и проходят как для SEQ, так и для MPI (при валидных входах).
* Проверка валидации подтверждает отказ работы на векторах разной длины.
* Производительные тесты запускаются на массивах 1 000 000 элементов; фактические числа зависят от платформы и числа процессов. Для устойчивой оценки стоит запускать в Release и усреднять несколько прогонов.

---

## 8. Заключение

В работе реализованы последовательная и MPI-версии вычисления скалярного произведения, обёрнутые в интерфейс задач `ppc::task::Task`. Корректность подтверждается функциональными тестами, а производительные прогоны позволяют оценивать масштабирование на выбранной платформе. Возможные дальнейшие шаги: профиль кэш-поведения, сравнение с OpenMP/CUDA и измерения на больших кластерах или с распределённым хранением данных.

---

## 9. Литература

1. Gropp W., Lusk E., Skjellum A. Using MPI
2. MPI Standard Documentation
3. Документация GoogleTest
4. Сысоев А. В. Лекции по параллельному программированию