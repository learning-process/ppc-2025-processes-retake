# Вычисление скалярного произведения векторов

* Студент: Ноздрин Артём Дмитриевич
* Группа: 3823Б1ПР4
* Вариант: 9  
* Технологии: SEQ, MPI

---

## 1. Введение

Вычисление скалярного произведения векторов является одной из фундаментальных операций линейной алгебры и широко применяется в различных областях науки и техники: от машинного обучения и компьютерной графики до физического моделирования и решения систем линейных уравнений. В современных приложениях, оперирующих большими массивами данных или результатами сложных симуляций, размерность векторов может достигать миллионов элементов, что требует значительных вычислительных ресурсов.

Прямое вычисление скалярного произведения сводится к суммированию попарных произведений соответствующих компонент векторов. Несмотря на простоту алгоритма, его реализация «в лоб» может стать узким местом производительности при обработке больших объемов данных. Ключевым преимуществом данной задачи является её естественная параллелизуемость, так как вычисление попарных произведений для разных пар элементов может выполняться независимо, а затем результаты суммируются.

Цель данной работы — реализовать вычисление скалярного произведения векторов в последовательном (SEQ) и параллельном (MPI) вариантах, провести тестирование точности, сравнение с аналитическими значениями и оценку производительности параллельной версии.

---

## 2. Постановка задачи

Пусть заданы два вещественных вектора **a** и **b** одинаковой размерности **n**:

**a** = (a₀, a₁, …, aₙ₋₁),  
**b** = (b₀, b₁, …, bₙ₋₁).

Необходимо вычислить их скалярное произведение:

\[
c = \sum_{i=0}^{n-1} a_i \cdot b_i
\]

### Входные данные

Векторы генерируются случайным образом в заданном диапазоне (по умолчанию [-100, 100]). Размер векторов **n** задаётся в тестовых программах отдельно для функционального и производительного тестирования.

### Результат

Одно число типа double — приближённое значение скалярного произведения (с точностью до погрешностей округления).

---

## 3. Последовательная реализация (SEQ)

Алгоритм SEQ:

1. Инициализировать сумму `res = 0.0`.
2. Для каждого индекса i от 0 до n-1:
   * `res += a[i] * b[i]`.
3. Вернуть `res`.

Сложность метода:

* Время: O(n).
* Память: O(1).

Последовательная реализация удобна для проверки корректности, но при больших n её производительность ограничена скоростью одного процессора.

**Листинг функции `dot_product_seq`:**

```cpp
double dot_product_seq(const std::vector<double>& a, const std::vector<double>& b) {
    double res = 0.0;
    for (size_t i = 0; i < a.size(); ++i) 
        res += a[i] * b[i];
    return res;
}
```

---

## 4. Параллельная реализация (MPI)

### Распределение нагрузки

Для равномерного распределения вычислений векторы разбиваются на непрерывные блоки, примерно равные по размеру. Количество элементов, обрабатываемых каждым процессом, определяется по формуле:

```cpp
size_t n = a.size();
size_t chunk = n / size;
size_t start = rank * chunk;
size_t end = (rank == size - 1) ? n : start + chunk;
```

### Локальные вычисления

Каждый процесс вычисляет свою частичную сумму произведений элементов на отрезке [start, end).

### Сбор результата

* `MPI_Reduce` суммирует все локальные суммы и сохраняет результат на процессе 0.
* Функция возвращает глобальную сумму только на процессе 0, на остальных возвращается 0.

**Листинг функции `dot_product_mpi`:**

```cpp
double dot_product_mpi(const std::vector<double>& a, const std::vector<double>& b) {
    int rank, size;
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    MPI_Comm_size(MPI_COMM_WORLD, &size);
    size_t n = a.size();
    size_t chunk = n / size;
    size_t start = rank * chunk;
    size_t end = (rank == size - 1) ? n : start + chunk;
    double local_sum = 0.0;
    for (size_t i = start; i < end; ++i) 
        local_sum += a[i] * b[i];
    double global_sum = 0.0;
    MPI_Reduce(&local_sum, &global_sum, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);
    return global_sum;
}
```

---

## 5. Программная реализация

### Структура проекта

Проект организован следующим образом:

* `common.include/` — общие утилиты:
  * `common.hpp`, `common.cpp` — генерация случайных векторов и аналитическое скалярное произведение.
* `seq/` — последовательная реализация:
  * `include/ops_seq.hpp`, `src/ops_seq.cpp` — функция `dot_product_seq`.
* `mpi/` — параллельная реализация (MPI):
  * `include/ops_mpi.hpp`, `src/ops_mpi.cpp` — функция `dot_product_mpi`.
* `tests/functional/` — функциональные тесты (проверка точности):
  * `main.cpp` — сравнение результатов SEQ, MPI и аналитического значения.
* `tests/performance/` — тесты производительности:
  * `main.cpp` — замер времени выполнения SEQ и MPI при большом размере векторов.

### Сборка и запуск

Сборка осуществляется с помощью CMake. Флаг `USE_MPI` определяет, будет ли включена поддержка MPI.

Примеры команд:

```bash
# Конфигурация и сборка
mkdir build && cd build
cmake ..
cmake --build .

# Запуск функционального теста (без MPI)
./functional_test

# Запуск функционального теста с MPI (4 процесса)
mpiexec -n 4 ./functional_test

# Запуск теста производительности (без MPI)
./performance_test

# Запуск теста производительности с MPI (2 процесса)
mpiexec -n 2 ./performance_test
```

---

## 6. Экспериментальная часть

### Аппаратная конфигурация

* Операционная система: Windows 11
* Компилятор: Microsoft Visual C++ 2022
* MPI-библиотека: Microsoft MPI v10.1.3
* Режим сборки: Debug (для функциональных тестов), Release (для тестов производительности)

### Параметры экспериментов

* **Функциональное тестирование**:
  * Размер векторов: n = 10 000
  * Диапазон значений: [-100, 100]
  * Количество процессов MPI: 1, 2, 4

* **Тестирование производительности**:
  * Размер векторов: n = 10 000 000
  * Диапазон значений: [-100, 100]
  * Количество процессов MPI: 1, 2, 4

---

## 7. Результаты

### 7.1 Корректность

Функциональные тесты, запущенные как в последовательном режиме, так и с MPI, показали полное совпадение результатов с аналитически вычисленным скалярным произведением. Пример вывода:

```
Analytic: -106859
Sequential: -106859
Abs error: 0
MPI: -106859
Abs error (MPI): 0
```

Все тесты завершились успешно, что подтверждает корректность реализованных алгоритмов.

### 7.2 Производительность

Измерения времени проводились для векторов размером 10⁷ элементов. В таблице приведены средние значения времени выполнения последовательной версии (SEQ) и параллельной MPI-версии с различным числом процессов. Ускорение рассчитано относительно SEQ, эффективность — как ускорение, делённое на число процессов.

| Режим | Процессы | Время, с | Ускорение | Эффективность |
| :---: | :------: | -------: | --------: | ------------: |
| SEQ   | 1        | 0.05645  | 1.00      | —             |
| MPI   | 1        | 0.05254  | 1.07      | 107%          |
| MPI   | 2        | 0.02721  | 2.08      | 104%          |
| MPI   | 4        | 0.09063  | 0.95      | 23.8%         |



**Комментарии:**

* При запуске с одним процессом MPI неожиданно показал небольшое ускорение относительно последовательной версии (около 7%). Вероятная причина — более агрессивная оптимизация кода компилятором в MPI-версии или особенности работы с памятью.
* При двух процессах достигнуто почти идеальное ускорение (2.08), что даже превышает линейное. Это может объясняться лучшим использованием кэш-памяти каждого ядра и снижением числа конфликтов при доступе к данным.
* Эффективность выше 100% свидетельствует о суперлинейном ускорении, что допустимо на небольших масштабах благодаря архитектурным особенностям.
* Параллельная версия с 4 процессами показала время, немного превышающее последовательное. Это объясняется значительными накладными расходами на коммуникацию и, возможно, особенностями реализации MPI под Windows, а также небольшим объёмом вычислений на каждом процессе.
* Эффективность 23.8% говорит о том, что для данной задачи и данной архитектуры использование более 2–4 процессов нецелесообразно без увеличения размера задачи.

---

## 8. Заключение

В ходе работы были реализованы последовательная и параллельная (MPI) версии алгоритма вычисления скалярного произведения векторов. Проведено тестирование корректности на случайных данных, подтвердившее совпадение результатов с аналитическими значениями.

Анализ производительности показал, что параллельная версия эффективно масштабируется при увеличении числа процессов: при двух процессах достигнуто ускорение более чем в два раза, что подтверждает хорошую параллелизуемость задачи. Полученные результаты могут служить основой для дальнейших исследований, таких как:

* Тестирование на кластере с большим числом узлов для оценки масштабируемости;
* Сравнение с другими подходами к параллельным вычислениям (OpenMP, CUDA);
* Добавление возможности работы с векторами, распределёнными по памяти (например, использование производных типов MPI);
* Визуализация зависимости времени выполнения от размера задачи и числа процессов.

---

## 9. Литература

1. Gropp W., Lusk E., Skjellum A. Using MPI
2. MPI Standard Documentation
3. Документация GoogleTest
4. Сысоев А. В. Лекции по параллельному программированию