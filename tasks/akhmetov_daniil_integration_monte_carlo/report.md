# Интегрирование методом Монте-Карло

- Выполнил: Ахметов Даниил Данисович
- Группа: 3823Б1ПР2
- Вариант: 21

---

## Введение

**Метод Монте-Карло** — мощный статистический метод для численного решения различных математических задач, включая вычисление определенных интегралов. В отличие от детерминированных методов численного интегрирования (как правило квадратурных формул), метод Монте-Карло основан на случайной выборке и особенно эффективен для многомерных интегралов.

**Цель работы:** реализовать параллельную версию метода Монте-Карло для вычисления определенных интегралов с использованием технологии MPI и оценить эффективность распараллеливания.

---

## Постановка задачи

Вычислить определенный интеграл функции на отрезке [a, b]:

I = ∫[a,b] f(x) dx

методом Монте-Карло по формуле:

I ≈ (b - a) * (1/N) * Σ[i=1..N] f(x_i)

где:
- N — количество случайных точек
- x_i — равномерно распределенные случайные точки на отрезке [a, b]

**Ограничения:**
1. Функция f(x) должна быть определена на всем отрезке [a, b]
2. Количество точек N должно быть достаточно большим для обеспечения точности
3. a < b

**Функции для интегрирования:**
1. Линейная: f(x) = 3x + 2
2. Квадратичная: f(x) = x² + 1  
3. Синус: f(x) = sin(x)
4. Экспонента: f(x) = e^x
5. Константа: f(x) = 5

---

## Описание алгоритма (последовательная версия)

**Алгоритм последовательной реализации:**

1. Ввод параметров: a, b, N, тип функции
2. Инициализация генератора псевдослучайных чисел
3. Для i от 1 до N:
   - Генерация случайного числа t ∈ [0, 1]
   - Вычисление x_i = a + (b - a) * t
   - Вычисление f(x_i)
   - Суммирование sum += f(x_i)
4. Вычисление среднего значения: average = sum / N
5. Вычисление интеграла: I = (b - a) * average
6. Вывод результата

**Особенности реализации:**
- Используется детерминированный генератор на основе константы 0.75487766624669276
- Для каждой функции реализована аналитическая первообразная для проверки точности

---

## Схема распараллеливания (MPI версия)

**Декомпозиция задачи:**
- Объемная декомпозиция (декомпозиция по данным)
- Распределение точек между процессами
- Каждый процесс вычисляет свою часть суммы

**Схема обмена данными:**
1. Процесс 0 распределяет общее количество точек между всеми процессами
2. Каждый процесс генерирует свои случайные точки независимо
3. Каждый процесс вычисляет локальную сумму значений функции
4. Используется операция редукции (MPI_Reduce) для суммирования всех локальных сумм
5. Процесс 0 вычисляет финальный результат
6. Результат рассылается всем процессам (MPI_Bcast)

**Алгоритм MPI версии:**

1. Инициализация MPI
2. Получение ранга и размера коммуникатора
3. Распределение точек между процессами (MPI_Scatterv)
4. Каждый процесс:
   - Генеририт свои случайные точки
   - Вычисляет локальную сумму
5. Глобальное суммирование (MPI_Reduce)
6. Вычисление интеграла (только процесс 0)
7. Рассылка результата (MPI_Bcast)
8. Завершение MPI

**Особенности реализации:**
- Используется `MPI_Scatterv` для неравномерного распределения точек
- Применяется `MPI_Reduce` для эффективного суммирования
- Все процессы получают конечный результат через `MPI_Bcast`

---

## Экспериментальные результаты

### Окружение

**Аппаратное обеспечение:**
- CPU: AMD Ryzen 7 2700 (8 ядер, 16 потоков)
- RAM: 16 GB DDR4
- OS: Windows 10
- Стандарт C++: C++17
- MPI: Microsoft MPI v10.0.22621.0

### Результаты производительности

Тестирование проводилось для функции f(x) = x² + 1 на отрезке [0, 2] с 10 миллионами точек.

| Количество процессов | Время выполнения (сек) | Ускорение | Эффективность |
|---------------------|------------------------|-----------|---------------|
| 1                   | 0.1704                 | 1.00×     | 100%          |
| 2                   | 0.1002                 | 1.70×     | 85.0%         |
| 4                   | 0.0714                 | 2.39×     | 59.8%         |
| 8                   | 0.0688                 | 2.48×     | 31.0%         |

**Параметры теста:**
- Функция: f(x) = x² + 1
- Интервал интегрирования: [0, 2]
- Количество точек: 10,000,000

**Методика тестирования:**
- Использовалась встроенная система тестирования производительности курса
- Измерялось время выполнения задачи (`task_run`) - чистое время алгоритма
- Каждый тест запускался автоматически системой тестирования

**Анализ результатов:**
1. **Хорошее ускорение**: 1.70× для 2 процессов, 2.39× для 4 процессов
2. **Высокая эффективность**: 85% для 2 процессов (почти идеальное распараллеливание)
3. **Снижение эффективности** с ростом числа процессов (59.8% для 4, 31.0% для 8)
4. **Ограничение масштабируемости**: при переходе с 4 на 8 процессов ускорение почти не растет (2.39× → 2.48×)

**Выводы:**
- Алгоритм хорошо масштабируется до 4 процессов
- Оптимальное количество процессов для данной задачи: 2-4
- Метод Монте-Карло демонстрирует хорошую пригодность для параллелизации
- Дальнейшее увеличение числа процессов нецелесообразно для данного объема данных

---

## Выводы

### Что сработало хорошо:
1. **Эффективное распараллеливание**: Алгоритм хорошо масштабируется до 8 процессов
2. **Высокая эффективность**: При больших N эффективность достигает 88-98%
3. **Точность результатов**: Метод дает точные результаты при достаточном количестве точек
4. **Простота реализации**: Алгоритм легко параллелится без сложных схем синхронизации

### Ключевые наблюдения:
1. Объемные вычисления (большое N) лучше параллелятся
2. Накладные расходы MPI становятся значимыми при малом объеме работы на процесс
3. Эффективность растет с увеличением объема вычислений

---

## Приложение

### Основная функция вычисления (MPI версия)

```cpp
bool AkhmetovDaniilIntegrationMonteCarloMPI::RunImpl() {
  int rank = 0;
  int size = 0;
  MPI_Comm_rank(MPI_COMM_WORLD, &rank);
  MPI_Comm_size(MPI_COMM_WORLD, &size);

  std::vector<int> points_per_process(size);
  std::vector<int> displacements(size);
  
  int base_point_count = point_count_ / size;
  int extra_points = point_count_ % size;
  
  for (int i = 0; i < size; ++i) {
    points_per_process[i] = base_point_count + (i < extra_points ? 1 : 0);
    displacements[i] = (i == 0) ? 0 : displacements[i-1] + points_per_process[i-1];
  }

  double local_sum = 0.0;
  const double magic_constant = 0.75487766624669276;
  
  for (int i = 0; i < points_per_process[rank]; ++i) {
    double t = std::fmod((displacements[rank] + i) * magic_constant, 1.0);
    double x = a_ + ((b_ - a_) * t);
    local_sum += FunctionPair::Function(func_id_, x);
  }

  double total_sum = 0.0;
  MPI_Reduce(&local_sum, &total_sum, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);

  double integral = 0.0;
  if (rank == 0) {
    double average = total_sum / static_cast<double>(point_count_);
    integral = (b_ - a_) * average;
  }
  
  MPI_Bcast(&integral, 1, MPI_DOUBLE, 0, MPI_COMM_WORLD);
  GetOutput() = integral;
  return true;
}

```
---
## Источники

1. Сысоев А. В., Лекции по курсу «Параллельное программирование для кластерных систем».

2. Документация по курсу «Параллельное программирование», URL: https://learning-process.github.io/parallel_programming_course/ru/index.html

3. Microsoft MPI Documentation: https://docs.microsoft.com/en-us/message-passing-interface/