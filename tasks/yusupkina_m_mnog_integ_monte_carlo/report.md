# <Вычисление многомерных интегралов методом Монте-Карло>

- Студент: <Юсупкина Маргарита Альбертовна>, группа <3823Б1ПМоп3>
- Технология: <SEQ | MPI >
- Вариант: <10>

## 1. Введение 
- Мотивация: Исследовать эффективность распараллеливания метода Монте-Карло для вычисления двойных интегралов с использованием MPI.

- Метод Монте-Карло основан на генерации большого количества случайных точек и их независимой обработке, что делает его потенциально хорошо параллелизуемым.

- Ожидаемый результат: MPI-версия должна демонстрировать хорошее ускорение благодаря независимости вычислений и минимальным коммуникационным затратам.


## 2. Постановка задачи
**Задача:** реализация последовательной (SEQ) и параллельной (MPI) версий вычисления двойного интеграла методом Монте-Карло.

- Вход: Границы области интегрирования (x_min, x_max, y_min, y_max), подынтегральная функция f(x,y), количество случайных точек N.
- Выход: Приближённое значение интеграла

**Ограничения** 
- N > 0 
- Область интегрирования должна иметь ненулевую площадь (x_min < x_max, y_min < y_max)
- Поддерживаются любые непрерывные функции


## 3. Описание алгоритма (Последовательного)
**Последовательный алгоритм реализует классический метод Монте-Карло:**
- Вычисляется площадь прямоугольной области: `area = (x_max - x_min) * (y_max - y_min)`
- Генерируются N случайных точек (x, y) равномерно в заданной области
- Для каждой точки вычисляется значение функции и накапливается сумма
- Результат вычисляется по формуле: `(сумма значений / N) * area`

## 4. Схема распараллеливания
**Проверка особого случая** 
- Если N <= 0 или площадь <= 0, все процессы сразу возвращают 0


**Распределение данных**
- Root-процесс (ранг 0) рассылает параметры (количество точек, границы области) всем процессам через MPI_Bcast
- Каждый процесс вычисляет свою долю точек: `local_points = points / size` плюс возможно 1 дополнительная точка для первых `points % size` процессов
- Каждый процесс генерирует свои локальные случайные точки, используя уникальный seed `(rd() + rank)`, и вычисляет локальную сумму значений функции
- Локальные суммы агрегируются на root-процессе через MPI_Reduce с операцией MPI_SUM
- Root-процесс вычисляет итоговый интеграл по формуле `(global_sum / points) * area` и рассылает результат всем процессам через MPI_Bcast

**Коммуникационная схема**
- MPI_Bcast — для рассылки параметров от root всем процессам 
- MPI_Reduce — для сбора локальных сумм на root
- MPI_Bcast — для рассылки результата всем процессам 


## 5. Детали реализации
**Струкрута кода**
yusupkina_m_mnog_integ_monte_carlo/
├── common/
│   └── include/
│       └── common.hpp - Общие типы данных
├── seq/
│   ├── include/
│   │   └── ops_seq.hpp - Заголовочный файл SEQ версии
│   └── src/
│       └── ops_seq.cpp - Реализация SEQ версии
├── mpi/
│   ├── include/
│   │   └── ops_mpi.hpp - Заголовочный файл MPI версии
│   └── src/
│       └── ops_mpi.cpp - Реализация MPI версии
└── tests/
    ├── functional/
    │   └── main.cpp - Функциональные тесты
    └── performance/
        └── main.cpp - Тесты производительности

**Ключевые классы**
- `YusupkinaMMnogIntegMonteCarloMPI` — описание параллельного алгоритма
- `YusupkinaMMnogIntegMonteCarloSEQ` — описание последовательного алгоритма
- Определение пространства имен `yusupkina_m_mnog_integ_monte_carlo` с обозначением используемых типов данных

**Ключевые функции**
- `YusupkinaMMnogIntegMonteCarloMPI(const InType &in)` - конструктор
- `ValidationImpl()` - проверка входных данных
- `PreProcessingImpl()` - предварительная обработка данных
- `RunImpl()` - основная логика вычислений
- `PostProcessingImpl()` - постобработка результатов

**Генерация случайных чисел:**
- Используется генератор std::mt19937 с инициализацией от std::random_device
- В MPI версии каждый процесс имеет уникальный seed: `std::mt19937 gen(rd() + rank)` для обеспечения независимости последовательностей случайных чисел
- Распределение точек в области задаётся через `std::uniform_real_distribution<double> dist_x(x_min, x_max)` и аналогично для y

## 6. Окружение
- Hardware/OS: AMD Ryzen 7 8845H, 8 ядер, 16 потоков, 32GB RAM, Win11 Pro
- Toolchain: gcc, version 13.3.0, Release
- Environment: PPC_NUM_PROC: 1, 4, 8, Docker container with AMD-V virtualization
- Data: 
    - Functional tests- : 13 тестов с различными функциями и размерами задач (от 1000 до 1 млн точек)
    - Performance tests- функция f(x,y)=x*y на [0,1]×[0,1], 10 млн точек    

## 7. Экспериментальные результаты

### 7.1 Корректность
**Корректность проверена с помощью функциональных тестов, охватывающих:**
- различные функции: константа, линейная, произведение, тригонометрические
- разные области интегрирования: единичный квадрат, сдвинутые, отрицательные
- различные размеры задач: от 1000 до 1 млн точек
- граничные случаи: нулевая площадь
- сравнение с аналитическими значениями 

### 7.2 Производительность 

**Для 10 миллионов точек на функции f(x,y) = x * y:**
| Режим | Процессы | Время, с | Ускорение | Эффективность |
|:-----:|:--------:|:--------:|:---------:|:-------------:|
| seq   | 1        | 0.1003   | 1.00     | 100%          |
| mpi   | 2        | 0.0520   | 1.93     | 96.5%         |
| mpi   | 4        | 0.0271   | 3.70     | 92.5%         |
| mpi   | 8        | 0.0258   | 3.89     | 48.6%         |

- На 2 процессах достигается ускорение(1.93×) с эффективностью 96.5%
- На 4 процессах ускорение 3.70× подтверждает отличную масштабируемость (эффективность 92.5%)
- На 8 процессах ускорение ограничивается аппаратными возможностями системы (3.89×, эффективность 48.6%)
- Коммуникационные затраты минимальны благодаря операции `MPI_Reduce`


## 8. Выводы
- Обе версии (SEQ и MPI) работают корректно на всех тестовых случаях
- Метод Монте-Карло демонстрирует **хорошую масштабируемость** благодаря независимой генерации и обработке точек
- MPI-версия показывает почти линейное ускорение на 2 и 4 процессах
- На 8 процессах ускорение ограничивается аппаратными возможностями системы
- Полученные результаты подтверждают, что метод Монте-Карло эффективно распараллеливается с помощью MPI

## 9. References
1. Документация по курсу «Параллельное программирование» - URL: https://learning-process.github.io/parallel_programming_course/ru/
2. Курс лекций по параллельному программированию -URL: https://source.unn.ru

