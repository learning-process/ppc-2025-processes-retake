# Нахождение максимальной разности между соседними элементами вектора

**Студент:** Красавин Артем Павлович  
**Группа:** 3823Б1ПР5  
**Технология:** SEQ | MPI  
**Вариант:** 8

---

## 1. Введение

Задача определения наибольшей абсолютной разницы между соседними элементами одномерного массива часто встречается при анализе временных рядов, обработке сигналов и в вычислительной математике. Тривиальное последовательное решение требует одного прохода по массиву, что даёт линейную сложность \(O(N)\). Однако при работе с очень большими объёмами данных (порядка сотен миллионов элементов) целесообразно применять параллельные вычисления. В данной работе используется технология MPI, позволяющая распределить исходный вектор между несколькими процессами, выполнить локальные вычисления, а затем собрать глобальный результат.

Разработаны две реализации алгоритма:
- **SEQ** – последовательная версия (базовый алгоритм);
- **MPI** – параллельная версия с использованием интерфейса передачи сообщений.

Основная цель – получить корректное решение, сравнить его производительность с последовательным аналогом и оценить эффективность параллелизации.

---

## 2. Постановка задачи

Пусть задан целочисленный вектор `vec` длины \(N\) (индексы от 0 до \(N-1\)). Требуется вычислить величину

\[
\max_{0 \le i < N-1} |\,\text{vec}[i+1] - \text{vec}[i]\,|.
\]

Если вектор содержит менее двух элементов, результат считается равным нулю. Входные данные могут быть произвольными (в том числе отрицательными числами).

- **Вход:** `std::vector<int>`
- **Выход:** `int` – максимальная разность соседних элементов.

---

## 3. Последовательный алгоритм

Описание последовательной версии:

1. Проверить длину вектора. Если \(N < 2\), сразу вернуть \(0\).
2. Инициализировать переменную `max_diff = 0`.
3. Для каждого \(i\) от \(0\) до \(N-2\):
   - Вычислить `diff = abs(vec[i+1] - vec[i])`.
   - Если `diff > max_diff`, присвоить `max_diff = diff`.
4. Вернуть `max_diff`.

**Оценка сложности:**
- Временная – \(O(N)\).
- По дополнительной памяти – \(O(1)\).

---

## 4. Параллельная реализация (MPI)

### 4.1. Распределение данных

Исходный вектор равномерно (насколько это возможно) разделяется между всеми процессами. Для процесса с номером `rank` размер блока вычисляется по формуле:

- `base_size = N / world_size`
- `remainder = N % world_size`
- Если `rank < remainder`, то размер блока увеличивается на 1.

Начальные смещения для каждого процесса хранятся в массиве `displs`. Процесс с рангом 0 рассылает остальным процессам их части данных с помощью точечных операций (`MPI_Send` / `MPI_Recv`).

### 4.2. Локальные вычисления

Каждый процесс, получив свою часть вектора, вычисляет максимальную разность внутри этой части. Используется та же логика, что и в последовательном алгоритме (функция `LocalCompute`).

### 4.3. Обмен граничными элементами

Поскольку пары элементов, лежащие на стыке блоков, принадлежат разным процессам, необходимо учесть разность между последним элементом предыдущего блока и первым элементом текущего блока. Для этого реализован обмен граничными значениями:

- Процесс с рангом \(r > 0\) принимает последний элемент от процесса \(r-1\).
- Процесс с рангом \(r < \text{world\_size} - 1\) отправляет свой последний элемент процессу \(r+1\).

После получения граничного значения процесс вычисляет дополнительную разность и при необходимости обновляет локальный максимум.

### 4.4. Сбор результатов

Локальные максимумы собираются на процессе с рангом 0 при помощи операции `MPI_Reduce` с оператором `MPI_MAX`. Затем глобальный результат рассылается всем процессам через `MPI_Bcast`.

### 4.5. Обработка особых случаев

Если размер вектора мал (\(N < 2\) или \(N < \text{world\_size}\)), применяется упрощённая логика: либо результат сразу равен нулю, либо вычисления выполняются только на процессе 0 с последующей рассылкой.

---

## 5. Структура программного кода

Реализация выполнена в рамках учебного проекта с соблюдением единого интерфейса. Основные файлы:

- `common.hpp` – общие типы и вспомогательные функции.
- `ops_seq.hpp`, `ops_seq.cpp` – класс последовательной версии.
- `ops_mpi.hpp`, `ops_mpi.cpp` – класс параллельной версии.
- `main.cpp` (functional) – функциональные тесты.
- `main.cpp` (performance) – тесты производительности.

Имена классов:
- `KrasavinIDiffBetwNeighbElemVecSEQ` – последовательная реализация.
- `KrasavinIDiffBetwNeighbElemVecMPI` – параллельная реализация.

Классы содержат стандартные методы:
- `ValidationImpl()` – проверка корректности входных данных.
- `PreProcessingImpl()` – предварительная подготовка.
- `RunImpl()` – основной вычислительный этап.
- `PostProcessingImpl()` – извлечение результата.

Для параллельной версии дополнительно реализованы служебные функции:
- `ComputeCountsAndDispls` – расчёт размеров блоков и смещений.
- `ScatterData` – распределение данных.
- `BoundaryExchange` – обмен граничными элементами.

---

## 6. Условия экспериментов

**Аппаратная платформа:**
- Процессор: Intel Core 5 220H (2.70 ГГц)
- Оперативная память: 32 ГБ
- ОС: Windows 11 Домашняя

**Программное обеспечение:**
- Компилятор: MSVC 14.44
- Библиотека MPI: MS-MPI 10.0
- Режим сборки: Release
- Система сборки: CMake 4.2.0-rc1
- Тестирование: Google Test

**Входные данные для тестов производительности:**
- Размер вектора: 200 000 000 элементов.
- Генерация значений: чётные позиции заполняются малыми числами (0–100), нечётные – большими (1000–10000). Такое чередование создаёт значительные перепады и позволяет проверить корректность нахождения максимальной разности.

**Проведённые измерения:**
- Последовательная версия – однократный запуск.
- Параллельная версия – с числом процессов 1, 2, 4, 8.
- Режимы запуска MPI: `task_run` и `pipeline`.

---

## 7. Результаты и их анализ

### 7.1. Проверка корректности

Функциональные тесты включали 10 различных наборов данных (в том числе граничные случаи – пустой вектор, вектор из одного элемента, отрицательные числа). Для каждого набора результаты последовательной и параллельной версий совпали. Дополнительно на больших случайных данных проверялось, что вычисленный максимум действительно является максимальной разностью соседних элементов.

### 7.2. Временные характеристики

Ниже приведены результаты замеров времени работы (в секундах) для вектора из 200 млн элементов. Для MPI‑версии указано время выполнения параллельной части (без учёта инициализации данных).

**Режим task\_pipeline:**

| Процессов | SEQ время | MPI время | Ускорение | Эффективность |
|:---------:|:---------:|:---------:|:---------:|:-------------:|
| 1         | 0.1415    | 0.4777    | 0.30      | 30%           |
| 2         | –         | 0.5979    | 0.24      | 12%           |
| 4         | –         | 0.1963    | 0.72      | 18%           |
| 8         | –         | 0.1683    | 0.84      | 10%           |

**Режим task\_run:**

| Процессов | SEQ время | MPI время | Ускорение | Эффективность |
|:---------:|:---------:|:---------:|:---------:|:-------------:|
| 1         | 0.1477    | 0.4705    | 0.31      | 31%           |
| 2         | –         | 0.6106    | 0.24      | 12%           |
| 4         | –         | 0.2038    | 0.72      | 18%           |
| 8         | –         | 0.1816    | 0.81      | 10%           |

*Ускорение* = время<sub>SEQ</sub> / время<sub>MPI</sub>,  
*Эффективность* = (ускорение / число процессов) × 100%.

### 7.3. Обсуждение результатов

- **Один процесс MPI** работает в 3–3.3 раза медленнее последовательной версии. Это объясняется накладными расходами на инициализацию MPI‑среды и внутренние коммуникации, которые не компенсируются полезной работой.
- **При двух процессах** время даже возрастает по сравнению с одним процессом MPI. Коммуникационные издержки (пересылка данных, обмен границами) на данном объёме данных перевешивают выигрыш от распараллеливания.
- **При четырёх процессах** наблюдается заметное снижение времени – оно становится близким к последовательному (ускорение около 0.72). Однако эффективность остаётся низкой (18%), что говорит о том, что ресурсы используются неоптимально.
- **При восьми процессах** время продолжает уменьшаться, но эффективность падает до 10%. Это типичное поведение для задач с простыми вычислениями и интенсивным обменом данными: с ростом числа процессов доля коммуникаций возрастает.

Разница между режимами `task_pipeline` и `task_run` незначительна; оба режима демонстрируют схожие тенденции.

### 7.4. Причины низкой эффективности

Основной фактор – малая вычислительная сложность обработки одного элемента (всего одна операция вычитания и сравнения). В таких условиях время передачи данных (особенно при большом количестве процессов) доминирует. Для достижения хорошего ускорения необходимо либо значительно увеличить размер задачи, либо усложнить вычисления на каждом элементе.

---

## 8. Заключение

В ходе работы были разработаны две версии алгоритма поиска максимальной разности соседних элементов вектора – последовательная и параллельная (с использованием MPI).

- **Корректность** подтверждена функциональными тестами и сравнением результатов обеих реализаций на наборах данных различного характера.
- **Производительность** проанализирована на векторе из 200 млн элементов. Последовательная версия на данной архитектуре оказалась быстрее MPI‑версии при малом числе процессов из-за высоких накладных расходов. С увеличением количества процессов время параллельной версии снижается, но эффективность остаётся невысокой (не превышает 30%).
- **Масштабируемость** ограничена простотой вычислений: накладные расходы на коммуникации не позволяют получить линейное ускорение. Для более тяжёлых вычислений (например, с плавающей точкой или дополнительной обработкой) эффективность MPI могла бы быть выше.

Тем не менее, разработанная MPI‑реализация корректно выполняет поставленную задачу и может служить основой для дальнейших оптимизаций (например, использование неблокирующих обменов или асинхронных операций).

---

**Студент:** Красавин Артем Павлович