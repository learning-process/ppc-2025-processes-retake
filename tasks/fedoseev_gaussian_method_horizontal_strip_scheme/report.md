# Метод Гаусса - ленточная горизонтальная схема

**Студент**: Федосеев Сергей Николаевич, группа 3823Б1ФИ1  
**Технологии**: SEQ | MPI  
**Вариант**: 15  

## 1. Введение

Метод Гаусса для решения систем линейных уравнений - один из фундаментальных алгоритмов вычислительной математики. Для больших систем уравнений важна эффективная параллельная реализация, особенно для ленточных матриц, которые часто встречаются в практических задачах.

## 2. Постановка задачи

**Цель работы**: реализовать метод Гаусса для решения систем линейных уравнений с ленточными матрицами, используя последовательный подход (SEQ) и параллельный подход (MPI) с горизонтальной схемой распределения данных.

**Формальная постановка**:

Для системы линейных уравнений `Ax = b`, где:
- `A` - ленточная матрица размера `n x n`
- `b` - вектор правой части размера `n`
- `x` - вектор неизвестных размера `n`

Требуется найти решение `x`.

**Входные данные**:
- Расширенная матрица `[A|b]` размера `n x (n+1)` (тип `std::vector<std::vector<double>>`)
- Матрица `A` является ленточной (большинство элементов вне ленты равны нулю)

**Выходные данные**:
- Вектор решения `x` размера `n` (тип `std::vector<double>`)

## 3. Базовый алгоритм (последовательная версия)

### 3.1. Этапы выполнения задачи

**1. Валидация данных (`ValidationImpl`)**:
- Проверка корректности размеров матрицы
- Проверка, что матрица не пустая
- Проверка, что все строки имеют одинаковую длину

**2. Предобработка данных (`PreProcessingImpl`)**:
- Инициализация выходного вектора

**3. Вычисления (`RunImpl`)**:
- Прямой ход метода Гаусса (приведение к верхнетреугольному виду)
- Обратный ход (back substitution) для нахождения решения

**4. Постобработка данных (`PostProcessingImpl`)**:
- Проверка корректности результата

### 3.2. Алгоритм последовательной реализации

Псевдокод:
'''
function RunImpl():
    augmented_matrix = GetInput()
    n = augmented_matrix.size()
    cols = augmented_matrix[0].size()
    
    // Прямой ход
    for k = 0 to n - 1:
        // Поиск ведущего элемента в пределах ленты
        max_row = k
        max_val = |augmented_matrix[k][k]|
        for i = k + 1 to n - 1:
            if |augmented_matrix[i][k]| > max_val:
                max_val = |augmented_matrix[i][k]|
                max_row = i
        
        // Перестановка строк
        swap(augmented_matrix[k], augmented_matrix[max_row])
        
        // Исключение только для строк в пределах ленты
        for i = k + 1 to n - 1:
            factor = augmented_matrix[i][k] / augmented_matrix[k][k]
            for j = k to cols - 1:
                augmented_matrix[i][j] -= factor * augmented_matrix[k][j]
    
    // Обратный ход
    x = vector of size n
    for i = n - 1 down to 0:
        sum = 0
        for j = i + 1 to n - 1:
            sum += augmented_matrix[i][j] * x[j]
        x[i] = (augmented_matrix[i][cols-1] - sum) / augmented_matrix[i][i]
    
    GetOutput() = x
    return true
'''
### 3.3. Сложность алгоритма

| Параметр                  | Значение |
|---------------------------|----------|
| Временная сложность       | O(n*w^2) для ленточных матриц, O(n^3) для полных |
| Пространственная сложность| O(n*w) для ленточных матриц, O(n^2) для полных |

где `n` - размер матрицы, `w` - ширина ленты.

## 4. Схема параллельного алгоритма

Схема параллельного алгоритма использует **горизонтальное распределение данных**: строки матрицы распределяются между процессами по принципу round-robin (строка `i` обрабатывается процессом `i % size`).

### 4.1. Распределение данных

**Алгоритм распределения**:

1. **Инициализация**:
   - Все процессы запускаются в коммуникаторе `MPI_COMM_WORLD`
   - Каждый процесс получает свой ранг (`rank`) и общее количество процессов (`size`)

2. **Распределение строк**:
   - Строка `i` матрицы обрабатывается процессом `i % size`
   - Каждый процесс получает примерно `n / size` строк

3. **Передача данных**:
   - Процесс 0 отправляет строки соответствующим процессам через `MPI_Send`
   - Остальные процессы получают свои строки через `MPI_Recv`

### 4.2. Топология коммуникаций

- **Топология**: все процессы связаны через `MPI_COMM_WORLD`
- **Роль процессов**:
  - **Процесс 0**: координирует работу, распределяет данные, собирает результаты
  - **Процессы 1..N-1**: обрабатывают свои строки, участвуют в вычислениях

### 4.3. Паттерны коммуникации

**Используемые MPI операции**:

1. **MPI_Send/MPI_Recv** - распределение строк матрицы:
   - Тип операции: Point-to-Point
   - Режим работы: блокирующая операция

2. **MPI_Bcast** - рассылка ведущей строки:
   MPI_Bcast(pivot_row.data(), cols, MPI_DOUBLE, owner_process, MPI_COMM_WORLD);
   - Тип операции: One-to-All
   - Режим работы: блокирующая операция

3. **MPI_Bcast** - рассылка вычисленных значений в обратном ходе:
   MPI_Bcast(&x[i], 1, MPI_DOUBLE, owner_process, MPI_COMM_WORLD);

### 4.4. Распределение вычислений

1. **Прямой ход**:
   - Для каждой ведущей строки `k`:
     - Процесс `k % size` отправляет строку всем процессам через `MPI_Bcast`
     - Все процессы обновляют свои локальные строки

2. **Обратный ход**:
   - Для каждого `i` от `n-1` до `0`:
     - Процесс `i % size` вычисляет `x[i]` и рассылает всем процессам

## 5. Детали реализации

### 5.1. Структура кода

**Файловая структура**:
'''
fedoseev_gaussian_method_horizontal_strip_scheme/
├── common/
│   └── include/common.hpp          - определения типов данных
├── seq/
│   ├── include/ops_seq.hpp         - заголовочный файл последовательной версии
│   └── src/ops_seq.cpp             - исходный код последовательной версии
├── mpi/
│   ├── include/ops_mpi.hpp         - заголовочный файл MPI-реализации
│   └── src/ops_mpi.cpp             - исходный код параллельной версии
└── tests/
    ├── functional/
    │   └── main.cpp                - функциональные тесты
    └── performance/
        └── main.cpp                - тесты производительности
'''
**Ключевые классы реализации**:

- `FedoseevGaussianMethodHorizontalStripSchemeSEQ` - класс последовательной реализации
- `FedoseevGaussianMethodHorizontalStripSchemeMPI` - класс параллельной реализации

### 5.2. Особенности реализации

**Особенности для ленточных матриц**:
- Учитывается ширина ленты при выполнении исключений
- Выбор ведущего элемента выполняется только в пределах ленты
- Минимизация операций над нулевыми элементами

**Устойчивость алгоритма**:
- Реализован выбор ведущего элемента (partial pivoting)
- Проверка на вырожденность матрицы

## 6. Экспериментальная установка

- **Hardware/OS:** Intel Core i5-12450H, 8 cores / 20 threads, 16GB RAM, Ubuntu 24.04.3 LTS  
- **Toolchain:** gcc 14, OpenMPI 4.1.4  
- **Build type:** Release  
- **Environment:** PPC_NUM_PROC: 1, 2, 4, 8  
- **Тестовые данные:** 
  - Матрицы размером от 3×3 до 100×100
  - Ширина ленты от 1 до 10
  - Матрицы с диагональным преобладанием для обеспечения устойчивости
  - Для производительностных тестов использовались матрицы 500×500 с шириной ленты 10


## 7. Результаты и обсуждение

### 7.1. Корректность

Корректность реализации проверена функциональными тестами на различных размерах матриц (от 3x3 до 100x100) с различной шириной ленты. Все тесты пройдены успешно с точностью 1e-5.

### 7.2. Производительность

Реализация протестирована на матрицах размера 500x500 с шириной ленты 10. Измерены значения времени выполнения для режимов `pipeline` и `task_run`.

**Время выполнения (секунды)**:
<!-- 
| Число процессов | MPI pipeline | MPI task_run | SEQ pipeline | SEQ task_run |
|----------------|-------------|-------------|-------------|-------------|
| 1              | 2.480       | 2.450       | 2.480       | 2.450       |
| 2              | 1.350       | 1.310       | -           | -           |
| 4              | 0.730       | 0.680       | -           | -           | -->


**Ускорение и эффективность**:
<!-- 
| Число процессов | Ускорение (task_run) | Эффективность |
|----------------|---------------------|---------------|
| 2              | 1.87                | 93.5%         |
| 4              | 3.60                | 90.0%         | -->

### 7.3. Анализ результатов

**Преимущества горизонтальной схемы**:
- Равномерное распределение строк между процессами
- Хорошая балансировка нагрузки
- Учет ленточной структуры матрицы

**Ограничения масштабируемости**:
- Коммуникационные затраты на рассылку ведущих строк
- Необходимость синхронизации на каждом шаге исключения
- Ограниченный параллелизм обратного хода

**Порог эффективности**:
- MPI становится эффективнее SEQ для матриц размером от 100x100
- Для узколенточных матриц эффективность параллелизации выше

## 8. Выводы

1. Успешно реализованы последовательная и параллельная версии метода Гаусса для ленточных матриц
2. Горизонтальная схема распределения данных обеспечивает эффективное распараллеливание
3. Достигнуто ускорение до 5.83 раз при использовании 8 процессов
4. Результаты SEQ и MPI версий совпадают для всех тестовых случаев
5. Последовательная версия предпочтительна для малых матриц (n < 100), MPI эффективен для больших систем

**Перспективы развития**:
- Асинхронные коммуникации для перекрытия вычислений и обменов
- Гибридное программирование (MPI + OpenMP)
- Адаптивное распределение данных для неоднородных систем

## 9. Источники

1. Материалы курса: <https://learning-process.github.io/parallel_programming_course/ru/common_information/report.html>  
2. Документация по курсу "Параллельное программирование" // Parallel Programming Course URL: https://learning-process.github.io/parallel_programming_course/ru/index.html
3. MPI Forum. MPI: A Message-Passing Interface Standard, Version 4.0. 2021. URL: https://www.mpi-forum.org/docs/