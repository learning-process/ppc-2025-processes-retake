# Умножение матриц (горизонтальное разделение)

- Студент: Зюзин Никита Михайлович, группа 3823Б1ПР2
- Технологии: SEQ + MPI
- Вариант: 13

## 1. Введение

Задача умножения матриц представляет фундаментальную операцию линейной алгебры.
С ростом размеров матриц последовательные алгоритмы демонстрируют увеличение времени выполнения.
Параллельная реализация позволит существенно ускорить обработку и обеспечить возможность работы с большими наборами данных.

Цель работы - разработать последовательный и параллельный алгоритмы умножения матриц с использованием горизонтального
разделения и сравнить их производительность на многопроцессорной системе.

## 2. Постановка задачи

Задача: найти произведение двух матриц А и В, то есть вычислить матрицу С = А × В.

Входные данные:

- матрица A размером m × n типа `std::vector<std::vector<double>>`
- матрица B размером n × p типа `std::vector<std::vector<double>>`

Выходные данные: матрица C размером m × p типа `std::vector<std::vector<double>>`

Ограничения матриц:

- матрицы A и B не должны быть пустыми;
- количество столбцов матрицы A должно равняться количеству строк матрицы B (условие умножаемости);
- все строки матрицы A должны иметь одинаковое количество столбцов;
- все строки матрицы B должны иметь одинаковое количество столбцов;
- элементы матриц представлены в виде чисел типа double.

Ограничения программы:

- должна корректно обрабатывать матрицы различных размеров;
- для параллельной реализации должна обеспечивать корректное распределение строк матрицы A между процессами;
- должна корректно передавать матрицу B всем процессам.

## 3. Базовый (последовательный) алгоритм (Sequential)

Работа последовательного алгоритма проходит через четыре этапа:

1. `ValidationImpl()` - проверяется, что матрица A не пуста, матрица B не пуста, количество столбцов A
    равняется количеству строк B, а все строки в каждой матрице имеют одинаковое количество столбцов.
2. `PreProcessingImpl()` - подготовительный этап, выходная матрица очищается.
3. `RunImpl()` - основной этап обработки матриц.
   - создается матрица результата размером m × p (количество строк A × количество столбцов B);
   - для каждого элемента результирующей матрицы C вычисляется скалярное произведение строки i матрицы A
     и столбца j матрицы B;
   - используется тройной цикл: по строкам результата, по столбцам результата, по элементам для скалярного произведения.
4. `PostProcessingImpl()` - завершающий этап, дополнительных операций не выполняется.

Полноценная реализация последовательного алгоритма представлена в Приложении (п.1).

## 4. Схема параллелизации

Идея параллелизации заключается в том, что матрицу A можно разбить на несколько независимых блоков строк и
предоставить обработку каждому блоку отдельному процессу. Матрица B остается полной и доступной всем процессам.
Каждый процесс вычисляет часть строк результирующей матрицы, затем полученные результаты собираются в итоговый ответ.

Распределение данных:

- Распределение матрицы A (горизонтальное разделение): исходная матрица A делится на n-ое количество
  примерно равных блоков строк, где n равняется числу процессов.
- Распределение матрицы B: матрица B полностью передается всем процессам через операцию broadcast.
- Распределение нагрузки: для равномерного распределения используются следующие формулы:
  - вычисляется базовое количество строк: количество строк A делится (оператор `/`) на количество процессов;
  - вычисляется остаток: количество строк A делится (оператор `%`) на количество процессов;
  - первые k процессов (где k - остаток от деления) получают блоки строк увеличенного размера (базовый размер + 1);
  - остальные процессы получают блоки базового размера.
- Определение границ блоков:
  - для каждого процесса вычисляется количество строк, которое он будет обрабатывать;
  - смещения (displacements) для MPI_Scatterv вычисляются кумулятивно на основе количеств строк для каждого процесса.

Схема связи/топологии:

- Топология: коммуникатор MPI_COMM_WORLD.
- Коммуникационные операции:
  - MPI_Bcast - операция трансляции матрицы B и информации о размерах всем процессам;
  - MPI_Scatterv - операция распределения блоков матрицы A между процессами;
  - MPI_Allgatherv - операция сбора результатов всех процессов в полную результирующую матрицу;
  - MPI_Barrier - барьер синхронизации перед завершением.

Ранжирование ролей:
Процесс 0 имеет специальную роль (владеет исходными данными), остальные процессы равноправны:

1. Процесс 0:
   - преобразует входные матрицы в плоское представление;
   - вычисляет распределение строк матрицы A между процессами;
   - передает размеры и матрицу B всем процессам используя MPI_Bcast;
   - распределяет блоки матрицы A остальным процессам используя MPI_Scatterv;
   - обрабатывает свою часть матрицы A.
2. Остальные процессы:
   - получают информацию о размерах и матрицу B от процесса 0;
   - получают блоки матрицы A от процесса 0;
   - вычисляют свою часть результирующей матрицы C;
   - участвуют в коллективной операции MPI_Allgatherv для сбора результата.

Полноценная реализация распараллеленного алгоритма представлена в Приложении (п.2).

## 5. Детали реализации

Файловая структура:

zyuzin_n_multiplication_matrix_horiz/
├── common/include
│   └── common.hpp                  # Базовые определения типов
├── mpi/
│   ├── include/ops_mpi.hpp         # MPI версия
│   └── src/ops_mpi.cpp
├── seq/
│   ├── include/ops_seq.hpp         # Последовательная версия
│   └── src/ops_seq.cpp
└── tests/
    ├── functional/main.cpp         # Функциональные тесты
    └── performance/main.cpp        # Производительные тесты

Ключевые классы:

- `ZyuzinNMultiplicationMatrixSEQ` - последовательная реализация.
- `ZyuzinNMultiplicationMatrixMPI` - параллельная реализация.

Основные методы:

- Общие для обеих реализаций:
  - `ValidationImpl()` - проверка входных данных;
  - `PreProcessingImpl()` - подготовительный этап;
  - `PostProcessingImpl()` - завершающий этап;
- `RunImpl()` - основной алгоритм обработки.

Вспомогательные методы для MPI версии:

- `BroadcastMatricesInfo()` - трансляция размеров матриц и полной матрицы B всем процессам;
- `ScatterMatrixA()` - распределение строк матрицы A между процессами;
- `ComputeLocalMultiplication()` - локальное вычисление произведения на каждом процессе;
- `GatherAndConvertResults()` - сбор части результатов всех процессов в полную матрицу.

Алгоритмические особенности:

- использование плоского представления матриц для удобства передачи через MPI;
- использование MPI_Bcast для трансляции матрицы B всем процессам;
- использование MPI_Scatterv для неравномерного распределения строк матрицы A;
- использование MPI_Allgatherv для сбора результатов от всех процессов;
- корректное вычисление размеров блоков и смещений с учетом остатка от деления.

## 6. Экспериментальная среда

Hardware/OS:

- процессор: AMD Ryzen 5 5600x
- ядра/потоки: 6 ядер / 12 потоков
- оперативная память: 16 GB
- операционная система: Ubuntu 25.10
- архитектура: x64

Toolchain:

- компилятор: GCC 15.2.0
- IDE: Visual Studio Code 2026
- тип сборки: Release
- система сборки: CMake
- версия MPI: Open MPI 5.0.8

Environment:

- количество процессов: задается через mpirun -n N
- коммуникатор: MPI_COMM_WORLD

Тестовые данные:

1. Функциональные тесты:
   - 22 теста с матрицами различных размеров (от 1×1 до 3×3, включая прямоугольные матрицы)
   - матрицы с положительными, отрицательными и нулевыми значениями
   - дробные числа (для проверки работы с типом double)
   - граничные случаи и специальные матрицы (единичные матрицы, нулевые матрицы)

2. Перформанс тесты:
   - матрица A размером 500 × 500 (единичная матрица с единицами на диагонали)
   - матрица B размером 500 × 500 (матрица с элементами 0.1)
   - размер результирующей матрицы: 500 × 500
   - общее количество операций: 125,000,000 (1.25 × 10^8) умножений

## 7. Результаты и обсуждение

### 7.1 Корректность

Корректность реализации была проверена следующими методами:

- разработано 22 функциональных теста с известными ожидаемыми результатами;
- тесты охватывают различные сценарии:
  - матрицы различных размеров (квадратные и прямоугольные);
  - матрицы с положительными, отрицательными и нулевыми значениями;
  - матрицы с дробными числами типа double;
  - граничные случаи (матрицы 1×1, 1×n, n×1).

Сравнение последовательной и параллельной версий:

- последовательная версия служит эталоном для проверки параллельной реализации;
- обе версии проходят идентичный набор тестов с одинаковыми входными данными;
- результаты последовательного и MPI алгоритмов полностью совпадают для всех тестовых случаев;
- различия в точности вычислений менее чем на машинный эпсилон (обусловлены порядком суммирования).

### 7.2 Производительность

Входные данные: матрица A (500×500) × матрица B (500×500) = матрица C (500×500)

Методы измерений:

- Каждый тест запускается 5 раз
- Берется среднее время выполнения (ΣTime / 5)
- Speedup = Time_seq / Time_mpi
- Efficiency = Speedup / Count * 100%

#### Время выполнения `task_run` (секунды)

| Mode | Count | Time  | Speedup | Efficiency |
|------|-------|-------|---------|------------|
| SEQ  | 1     | 2.231 | 1.00    | —          |
| MPI  | 2     | 0.643 | 3.47    | 173.50 %   |
| MPI  | 4     | 0.330 | 6.77    | 169.15 %   |
| MPI  | 6     | 0.235 | 9.49    | 158.12 %   |

#### Время выполнения `pipeline` (секунды)

| Mode | Count | Time  | Speedup | Efficiency |
|------|-------|-------|---------|------------|
| SEQ  | 1     | 2.233 | 1.00    | —          |
| MPI  | 2     | 0.640 | 3.49    | 174.50 %   |
| MPI  | 4     | 0.341 | 6.55    | 163.85 %   |
| MPI  | 6     | 0.241 | 9.25    | 154.15 %   |

Анализ результатов:

- Ускорение увеличивается быстро с ростом количества процессов: 3.47× на 2 процессах, 6.77× на 4 процессах,
  9.49× на 6 процессах для режима task_run
- Эффективность превышает 100% на всех конфигурациях, что указывает на положительное влияние
  параллельной организации вычислений на использование кэша и памяти
- Эффективность постепенно снижается с ростом количества процессов: 173.5% на 2 процессах, 169.15% на 4 процессах,
  158.12% на 6 процессах, что объясняется накладными расходами MPI коммуникаций
- Результаты task_run и pipeline практически идентичны (разница менее 2%), что говорит об отсутствии преимущества
  конвейерной обработки для данной задачи
- Задача демонстрирует исключительную масштабируемость благодаря эффективной схеме горизонтального разделения
  матрицы A, при которой каждый процесс получает собственную часть данных для обработки

## 8. Заключение

В ходе работы была успешно решена задача умножения матриц размером 500×500 с использованием
последовательного алгоритма и технологии MPI для параллельных вычислений в двух вариантах реализации: task_run
и pipeline.

Основные результаты:

- разработаны корректные алгоритмы – созданы последовательная и параллельные версии, прошедшие полное тестирование
  на корректность результатов;
- реализована схема горизонтального разделения – матрица A разделяется по строкам между процессами, а матрица B
  полностью передается всем процессам, что позволило эффективно выполнять вычисления;
- достигнуто исключительное ускорение – параллельные реализации демонстрируют ускорение до 9.49× на 6 процессах
  по сравнению с последовательной версией (для режима task_run). Эффективность при этом составляет 173.5% на
  2 процессах и остается выше 150% даже на 6 процессах, что указывает на суперлинейное ускорение за счет
  лучшего использования кэша и параллельной архитектуры;
- оба варианта (task_run и pipeline) показали практически идентичные результаты (разница менее 2%), что говорит об
  отсутствии преимущества конвейерной обработки для данной задачи.

Алгоритм демонстрирует выдающуюся масштабируемость благодаря эффективной схеме горизонтального разделения строк матрицы
A, при которой один процесс получает все столбцы матрицы B один раз, а затем выполняет вычисления независимо,
минимизируя объемы повторяющихся коммуникаций.

## 9. Источники

1. Документация по курсу «Параллельное программирование» // URL:
   <https://learning-process.github.io/parallel_programming_course/ru/index.html>
2. Репозиторий курса «Параллельное программирование» // URL:
   <https://github.com/learning-process/ppc-2025-processes-engineers>
3. Сысоев А. В., Лекции по курсу «Параллельное программирование для кластерных систем».

## Приложение

П.1

```cpp
bool ZyuzinNMultiplicationMatrixSEQ::RunImpl() {
  const auto &matrix_a = GetInput().first;
  const auto &matrix_b = GetInput().second;
  size_t rows_a = matrix_a.size();
  size_t cols_b = matrix_b[0].size();
  size_t cols_a = matrix_a[0].size();
  GetOutput().resize(rows_a, std::vector<double>(cols_b, 0));

  for (size_t i = 0; i < rows_a; i++) {
    for (size_t j = 0; j < cols_b; j++) {
      for (size_t k = 0; k < cols_a; k++) {
        GetOutput()[i][j] += matrix_a[i][k] * matrix_b[k][j];
      }
    }
  }

  return true;
}
```

П.2

```cpp
bool ZyuzinNMultiplicationMatrixMPI::RunImpl() {
  int rank = 0;
  int size = 0;
  MPI_Comm_rank(MPI_COMM_WORLD, &rank);
  MPI_Comm_size(MPI_COMM_WORLD, &size);

  int rows_a = 0;
  int cols_a = 0;
  int rows_b = 0;
  int cols_b = 0;
  std::vector<double> matrix_b_flat;

  BroadcastMatricesInfo(rank, rows_a, cols_a, rows_b, cols_b, matrix_b_flat);

  std::vector<double> matrix_a_flat;
  std::vector<double> local_a_flat;
  int actual_local_rows = 0;
  ScatterMatrixA(rank, size, rows_a, cols_a, matrix_a_flat, local_a_flat, actual_local_rows);

  std::vector<double> local_result_flat;
  ComputeLocalMultiplication(local_a_flat, matrix_b_flat, local_result_flat, actual_local_rows, cols_a, cols_b);

  GatherAndConvertResults(size, rows_a, cols_b, actual_local_rows, local_result_flat);

  MPI_Barrier(MPI_COMM_WORLD);
  return true;
}
```
