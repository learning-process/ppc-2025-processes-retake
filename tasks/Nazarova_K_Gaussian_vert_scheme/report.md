# Метод Гаусса — ленточная вертикальная схема (MPI). Решение СЛАУ

- Студент: Назарова Ксения Олеговна, группа 3823Б1ПР3
- Технология: SEQ | MPI
- Задача: **Processes / Second task / Variant 16**

## Введение

Цель работы — реализовать последовательную (SEQ) и параллельную (MPI) версии решения системы линейных алгебраических уравнений \(Ax=b\) методом Гаусса.
Параллельная версия выполняется по **ленточной вертикальной схеме** (разбиение столбцов матрицы по процессам).

## Постановка задачи

Дана система \(Ax=b\), где \(A\) — квадратная матрица размера \(n \times n\), \(b\) — вектор размера \(n\).
Требуется найти вектор решения \(x\).

Входные данные задаются расширенной матрицей \([A|b]\) в виде одномерного массива `augmented` (строчно-ориентированное хранение):

```cpp
struct Input {
  int n;
  std::vector<double> augmented;  // size = n * (n + 1), row-major: [A | b]
};
```

Выходные данные:

```cpp
using OutType = std::vector<double>;  // solution x, size = n
```

Ограничения:
- \(n > 0\);
- матрица должна быть невырожденной (в реализации используется частичный выбор главного элемента).

## Базовый алгоритм (Sequential)

Последовательный метод Гаусса состоит из двух этапов:

1) **Прямой ход** (forward elimination): приведение \([A|b]\) к верхнетреугольному виду.
2) **Обратный ход** (back substitution): вычисление компонент \(x_i\) начиная с \(x_{n-1}\).

Используется частичный выбор главного элемента по текущему столбцу \(k\) (поиск максимума \(|a_{ik}|\) для \(i \ge k\)).

Сложность: \(O(n^3)\) по времени, \(O(n^2)\) по памяти (расширенная матрица).

## Схема распараллеливания (MPI): ленточная вертикальная

В параллельной версии все \(n+1\) столбцов расширенной матрицы \([A|b]\) делятся на \(p\) процессов по блокам:

- процесс `rank` хранит столбцы \([ \lfloor rank \cdot (n+1) / p \rfloor, \lfloor (rank+1) \cdot (n+1) / p \rfloor )\).

### Прямой ход

Для шага \(k\):

- владелец столбца \(k\) (процесс `owner(k)`) ищет `pivot_row` по своему локальному столбцу \(k\) и рассылает его через `MPI_Bcast`;
- все процессы синхронно выполняют обмен строк `k` и `pivot_row` **только в своих локальных столбцах**;
- `owner(k)` рассылает значения столбца \(k\) для строк \(k..n-1\) (вектор) через `MPI_Bcast`;
- каждый процесс обновляет элементы ниже диагонали в своих локальных столбцах.

### Обратный ход

Для строки \(i\) (снизу вверх):

- диагональный элемент \(a_{ii}\) рассылается владельцем столбца \(i\) (`MPI_Bcast`);
- значение правой части \(b_i\) рассылается владельцем столбца \(n\) (`MPI_Bcast`);
- сумма \(\sum_{j=i+1}^{n-1} a_{ij} x_j\) вычисляется по частям на каждом процессе (только по его локальным столбцам) и объединяется через `MPI_Allreduce`.

## Детали реализации

Файлы реализации:
- SEQ: `tasks/Nazarova_K_Gaussian_vert_scheme/seq/src/ops_seq.cpp`
- MPI: `tasks/Nazarova_K_Gaussian_vert_scheme/mpi/src/ops_mpi.cpp`

MPI-реализация хранит только локальную «вертикальную ленту» расширенной матрицы, что соответствует схеме разбиения.

## Проверка корректности

Функциональные тесты: `tasks/Nazarova_K_Gaussian_vert_scheme/tests/functional/main.cpp`.

Для проверок генерируется диагонально-доминантная матрица \(A\) и заранее заданный вектор решения \(x\).
Далее вычисляется \(b = A x\), и результат сравнивается с ожидаемым \(x\) с точностью `1e-7`.

## Производительность

Performance-тесты: `tasks/Nazarova_K_Gaussian_vert_scheme/tests/performance/main.cpp`.

В тесте решается система размера \(n=620\) (диагонально-доминантная).
Для сравнения используются режимы perf `pipeline` и `task_run`, измеряемые фреймворком курса.

Замеры (для MPI запуск под `mpirun --oversubscribe -n N`):

| Mode     | Procs | Time, s     | Speedup | Efficiency |
|----------|------:|------------:|--------:|-----------:|
| pipeline |     1 | 0.7819871426 | 1.00    | N/A        |
| pipeline |     2 | 0.6156919176 | 1.27    | 63.5%      |
| pipeline |     3 | 0.4062674908 | 1.93    | 64.2%      |
| pipeline |     4 | 0.4022273552 | 1.94    | 48.6%      |
| task_run |     1 | 0.7610472679 | 1.00    | N/A        |
| task_run |     2 | 0.5377476688 | 1.42    | 70.8%      |
| task_run |     3 | 0.6784045884 | 1.12    | 37.4%      |
| task_run |     4 | 0.6879001166 | 1.11    | 27.7%      |

Где \(Speedup = T_{seq} / T_{mpi}\), \(Efficiency = Speedup / p\).

## Как воспроизвести

Сборка:

```bash
git submodule update --init --recursive
cmake -S . -B build -D USE_FUNC_TESTS=ON -D USE_PERF_TESTS=ON -D CMAKE_BUILD_TYPE=Release
cmake --build build --config Release --parallel
```

Функциональные тесты (MPI часть запускается под `mpirun`):

```bash
export PPC_NUM_THREADS=4
export PPC_NUM_PROC=2
python3 scripts/run_tests.py --running-type processes --counts 2
```

## Заключение

Реализованы SEQ и MPI версии метода Гаусса для решения СЛАУ.
MPI-версия использует ленточную вертикальную схему разбиения столбцов и набор коллективных операций `MPI_Bcast` / `MPI_Allreduce` для синхронизации.
