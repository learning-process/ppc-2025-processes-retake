# Поиск минимального значения в векторе
- Студентка: Шкрылёва С.А., группа 3823Б1ПР1
- Технология: MPI  
- Вариант: 4

## 1. Введение
Параллельные вычисления являются важным направлением в современном программировании, позволяющим значительно ускорить обработку больших объемов данных. В данной работе рассматривается задача поиска минимального элемента в векторе с использованием технологии MPI (Message Passing Interface) для распределения вычислений между несколькими процессами.

## 2. Постановка задачи
Разработать параллельный алгоритм для нахождения минимального значения в векторе целых чисел. Алгоритм должен эффективно распределять вычисления между несколькими процессами и обеспечивать корректный результат при различных размерах входных данных.

## 3. Последовательный алгоритм
```cpp
  int min_val = GetInput()[0];
  for (size_t i = 1; i < GetInput().size(); i++) {
    if (GetInput()[i] < min_val) {
      min_val = GetInput()[i];
    }
  }
```
Алгоритм использует простой итеративный подход с циклом для поиска минимального элемента, что обеспечивает временную сложность O(n).

## 4. Схема распараллеливания
### Распределение данных
- Блочное распределение: Исходный вектор делится на приблизительно равные блоки между процессами
- Обработка остатка: При неравномерном делении первые процессы получают на один элемент больше
- Динамическое вычисление размеров:
  base_size = total_size / world_size - базовый размер блока
  extra_items = total_size % world_size - остаток для распределения
### Коммуникационная схема
1. Broadcast размера данных (MPI_Bcast):
  Процесс 0 рассылает общий размер вектора всем процессам
  Обеспечивает согласованность данных между всеми узлами
2. Распределение блоков (MPI_Scatterv):
  Процесс 0 распределяет данные с учетом неравных блоков
  Используются массивы sendcounts и displacements для точного указания размеров и смещений
3. Локальные вычисления:
  Каждый процесс находит минимум в своем блоке последовательным поиском
  Временная сложность: O(local_size)
4. Глобальная редукция (MPI_Allreduce):
  Сбор минимальных значений со всех процессов
  Использование операции MPI_MIN для нахождения глобального минимума
  Все процессы получают финальный результат

## 5. Детали реализации
Проект имеет модульную структуру:
seq - последовательная версия (ops_seq.hpp/cpp)
mpi - параллельная реализация на MPI (ops_mpi.hpp/cpp)
tests - тесты (functional и performance)

Ключевые функции и их назначение:
ValidationImpl()
Проверяет, что входной вектор не пустой
Проверяет, что выходное значение инициализировано нулем 

PreProcessingImpl()
Инициализация MPI среды (с проверкой уже инициализированного состояния)
Установка начального значения выходного результата (INT_MAX)

RunImpl() - основная логика параллельного алгоритма:
Получение ранга и размера коммуникатора
Broadcast общего размера данных от процесса 0 ко всем процессам
Расчет распределения данных с балансировкой остатка
Распределение блоков данных через MPI_Scatterv
Локальный поиск минимума в каждом процессе
Глобальная редукция через MPI_Allreduce с операцией MPI_MIN

PostProcessingImpl()
Корректное завершение работы MPI (с проверкой состояния)
Финальная валидация результата

Использование MPI_Scatterv для распределения данных с балансировкой остатка (первые процессы получают +1 элемент)
```cpp
int base_size = total_size / world_size;
int extra_items = total_size % world_size;

for (int i = 0; i < world_size; ++i) {
    sendcounts[i] = base_size + (i < extra_items ? 1 : 0);
}
```

Обработка краевых случаев
Пустой вектор: возврат false на этапе ValidationImpl
Один процесс: работает как последовательная версия
Процессы без данных: local_min остается INT_MAX, участвует в редукции

Потребление памяти
Каждый процесс: локальный вектор размером ~total_size/world_size
Накладные расходы: массивы sendcounts и displacements размером world_size
Общие данные: только на процессе 0 хранится полный исходный вектор

## 6. Экспериментальная установка
Оборудование/ОС:
  CPU: AMD Ryzen 5 4600H with Radeon Graphics (6 ядер, 12 потоков)
  RAM: 16 GB
  ОС: Windows 10

Инструменты:
  Система сборки: CMake
  Компилятор: Microsoft Visual C++ (MSVC) версии 14.37.32822
  MPI: Microsoft MPI (MS-MPI) версии 10.1.12498.52
  Тип сборки: Release
  Среда разработки: Visual Studio Code

Окружение:
Количество процессов: 1, 2, 4, 8

Данные:
Вектор размером 100,000,000 элементов, генерируемый в тестах
Диапазон значений: от -1000 до 1000

## 7. Результаты экспериментов

### 7.1 Корректность
- Все функциональные тесты пройдены
- Результаты MPI и sequential версий идентичны

### 7.2 Производительность
Параметры тестирования:
- Размер вектора: 100,000,000 элементов
- Диапазон значений: от -1000 до 1000

### Результаты времени выполнения (секунды):
| Процессы | Время (с) | Ускорение | Эффективность |
|----------|-----------|-----------|---------------|
| 1 (seq)  | 0.082347  | 1.00      | 100%          |
| 2        | 0.203896  | 0.40      | 20%           |
| 4        | 0.184119  | 0.45      | 11%           |
| 8        | 0.154157  | 0.53      | 7%            |

### Анализ производительности:
- Замедление MPI: параллельная версия в 2-2.5 раз медленнее sequential эталона
- Стабильность MPI: Наблюдается постепенное улучшение производительности от 2 к 8 процессам (ускорение от 0.40 до 0.53)
- Накладные расходы: время на коммуникации составляет значительную часть общего времени выполнения

## 8. Выводы
- Функциональная корректность: все 10 тестов успешно пройдены, алгоритмы работают правильно
- Ограничения параллелизации: для задачи поиска минимума в векторе накладные расходы MPI делают параллелизацию неэффективной
- Относительная эффективность: наилучший результат достигнут при 8 процессах с ускорением 0.53
- Область применения MPI: может стать эффективной при значительном увеличении объема данных или усложнении вычислений
- Масштабируемость: алгоритм демонстрирует стабильную работу с различным количеством процессов

Несмотря на отсутствие абсолютного ускорения, работа демонстрирует корректную реализацию 
параллельного алгоритма и понимание принципов распределенных вычислений. Для демонстрации 
реальных преимуществ MPI требуются задачи с большим объемом вычислений или данные большего размера.

## 9. Источники
MPI Standard https://www.mpi-forum.org/docs/
MPICH guides: https://www.mpich.org/documentation/guides/
Microsoft MPI: https://www.learn.microsoft.com/en-us/message-passing-interface/microsoft-mpi
OpenMPI docs: https://www.open-mpi.org/docs/

## Приложение
```cpp
bool ShkrylevaSVecMinValMPI::RunImpl() {
  if (GetInput().empty()) {
    return false;
  }

  int world_rank = 0;
  int world_size = 0;
  MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);
  MPI_Comm_size(MPI_COMM_WORLD, &world_size);

  int total_size = 0;
  const std::vector<int> *input_data_ptr = nullptr;

  if (world_rank == 0) {
    input_data_ptr = &GetInput();
    total_size = static_cast<int>(input_data_ptr->size());
  }

  MPI_Bcast(&total_size, 1, MPI_INT, 0, MPI_COMM_WORLD);

  if (total_size == 0) {
    return false;
  }

  int base_size = total_size / world_size;
  int extra_items = total_size % world_size;

  std::vector<int> sendcounts(world_size);
  std::vector<int> displacements(world_size);

  int offset = 0;
  for (int i = 0; i < world_size; ++i) {
    sendcounts[i] = base_size + (i < extra_items ? 1 : 0);
    displacements[i] = offset;
    offset += sendcounts[i];
  }

  std::vector<int> local_data(std::max(sendcounts[world_rank], 0));

  MPI_Scatterv((world_rank == 0) ? input_data_ptr->data() : nullptr, sendcounts.data(), displacements.data(), MPI_INT,
               local_data.data(), sendcounts[world_rank], MPI_INT, 0, MPI_COMM_WORLD);

  int local_min = INT_MAX;
  if (sendcounts[world_rank] > 0) {
    for (int value : local_data) {
      local_min = (value < local_min) ? value : local_min;
    }
  }

  int total_min = INT_MAX;
  MPI_Allreduce(&local_min, &total_min, 1, MPI_INT, MPI_MIN, MPI_COMM_WORLD);

  GetOutput() = total_min;

  return true;
}
```
