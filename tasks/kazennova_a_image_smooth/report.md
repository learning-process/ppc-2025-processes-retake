 <Task Сглаживание изображения>

- Student: <Казеннова Анастасия Михайловна>, group <3823Б1ФИ2>
- Technology: <SEQ | MPI>
- Variant: <22>

## 1. Introduction
Сглаживание изображения – одна из базовых операций цифровой обработки изображений, применяемая для подавления шумов и уменьшения резких перепадов яркости. Наиболее распространённым методом является свёртка с ядром Гаусса, которое придаёт больший вес центральному пикселю и уменьшает вклад удалённых соседей. При обработке больших изображений последовательная реализация может оказаться слишком медленной. Параллельное выполнение с использованием MPI позволяет эффективно распределить вычислительную нагрузку между несколькими процессами, что существенно сокращает время обработки.

## 2. Problem Statement
Требуется применить фильтр Гаусса размером 3×3 к изображению, заданному структурой Image:
struct Image {
    std::vector<uint8_t> data;   // пиксели в порядке строк
    int width;                    // ширина в пикселях
    int height;                   // высота в пикселях
    int channels;                 // число каналов (1 – оттенки серого, 3 – RGB)
};
Для каждого пикселя вычисляется взвешенная сумма значений пикселей в окрестности 3×3 с использованием ядра
На границах изображения недостающие соседи заменяются ближайшим доступным пикселем (клонирование края). Результат записывается в выходное изображение той же размерности.

Ограничения:
- width и height > 0
- data не пуст;
- channels равен 1 или 3.

## 3. Baseline Algorithm (Sequential)
Последовательный алгоритм обходит все пиксели изображения тройным циклом:

Сложность алгоритма: O( H × W × channels ), где H, W – высота и ширина изображения.

4. Parallelization Scheme
4.1 Распределение данных
Изображение разбивается по горизонтали на полосы строк. Каждый MPI-процесс получает непрерывный набор строк, причём распределение происходит равномерно с учётом остатка: первые remainder процессов получают на одну строку больше. Для корректного применения фильтра на границах полосы каждому процессу выделяется локальный буфер размером (strip_height_ + 2) × (width × channels), где две дополнительные строки служат для хранения данных соседних процессов.

4.2 Обмен граничными строками
Перед применением фильтра процессы обмениваются граничными строками с помощью MPI_Sendrecv:

Каждый процесс (кроме первого) отправляет свою первую строку данных процессу rank-1 и принимает от него строку в свою верхнюю halo-строку.

Каждый процесс (кроме последнего) отправляет свою последнюю строку данных процессу rank+1 и принимает от него строку в свою нижнюю halo-строку.
Крайние процессы (ранг 0 и rank = size-1) копируют свои крайние строки в соответствующие halo-области, так как у них нет соседей.

4.3 Локальная обработка
Каждый процесс применяет свёртку к своим собственным строкам (без halo), используя данные из локального буфера, включая полученные halo-строки. Результат сохраняется в отдельном буфере result_strip_ размером strip_height_ × (width × channels).

4.4 Сбор результатов
Процесс с рангом 0 собирает все полосы в правильном порядке. Сначала с помощью MPI_Gather собираются размеры полос в байтах, затем вычисляются смещения, и MPI_Gatherv собирает сами данные. В результате выходное изображение на процессе 0 содержит полное сглаженное изображение.

4.5 Условия корректности
Для успешной работы необходимо, чтобы количество процессов не превышало высоты изображения (иначе некоторые процессы получат strip_height_ = 0, что приведёт к deadlock при обменах). В данной реализации это условие предполагается выполненным.

5. Implementation Details
Структура проекта:

text
kazennova_a_image_smooth/
├── common/
│   └── common.hpp           // Определения Image, InType, OutType, BaseTask
├── seq/
│   ├── ops_seq.hpp           // Класс KazennovaAImageSmoothSEQ
│   └── ops_seq.cpp           // Реализация последовательной версии
├── mpi/
│   ├── ops_mpi.hpp           // Класс KazennovaAImageSmoothMPI
│   └── ops_mpi.cpp           // MPI-реализация
└── tests/
    ├── functional/           // Функциональные тесты (проверка изменения пикселей)
    └── performance/          // Тесты производительности (замеры времени)
Ключевые классы и методы:

-SEQ: последовательная версия. В RunImpl() реализован тройной цикл с вызовом ApplyKernelToPixel.

-MPI:

DistributeImage() – распределение строк, создание локальных буферов.
ExchangeBoundaries() – обмен граничными строками.
ApplyKernelToStrip() – применение фильтра к своей полосе.
GatherResult() – сбор результатов на процессе 0.

Особые случаи:

Границы изображения обрабатываются через std::clamp.

Для одноканальных и трёхканальных изображений код универсален благодаря параметру channels.

Если количество процессов равно 1, обмен границами заменяется простым копированием.

## 6. Experimental Setup
- Hardware/OS
Процессор: AMD Ryzen 5 3500U, 4 ядра, 8 потоков
Тактовая частота: 2.10 GHz  
Оперативная память: 8 GB RAM
Накопитель: 477 GB SSD
Операционная система: Windows 10 Home 22H2 (сборка 19045.6456)

- Компилятор: g++ 
- Использовался Docker-контейнер.
- Тип сборки: Release

- Environment
Количество процессов: 2, 4

## 7. Results and Discussion

### 7.1 Correctness
Функциональные тесты включали несколько сценариев:

-Изображение 4×4 с известными значениями пикселей

-Сравнение результатов MPI-версии (на процессе 0) с результатами последовательной реализации для идентичных входных данных.

-Проверка, что выходное изображение не совпадает с входным (фильтр действительно изменяет пиксели).

Все тесты пройдены успешно, что подтверждает корректность параллельной реализации.

### 7.2 Performance

| Mode        | Count | Time, s | Speedup | Efficiency |
|-------------|-------|---------|---------|------------|
| seq         | 1     | 0.23326 | 1.00    | N/A        |
| imp         | 2     | 0.15046 | 1.55    | 77.5%      |
| imp         | 4     | 0.10664 | 2.19    | 54.8%      |

## 8. Conclusions
Успешно реализована параллельная MPI-версия алгоритма сглаживания изображения фильтром Гаусса 3×3. Достигнуто ускорение в 1.55 раза на 2 процессах и 2.19 раза на 4 процессах по сравнению с последовательной версией. Эффективность параллелизации составляет 77.5% на 2 процессах и 54.8% на 4 процессах. Снижение эффективности с ростом числа процессов обусловлено увеличением коммуникационных затрат при обмене граничными строками (halo-обмены) и уменьшением объёма вычислений на каждый процесс при фиксированном размере изображения.

## 9. References
1. Лекции и практики курса "Параллельное программирование для кластерных систем"
2. Гонсалес Р., Вудс Р. Цифровая обработка изображений. – М.: Техносфера, 2012.