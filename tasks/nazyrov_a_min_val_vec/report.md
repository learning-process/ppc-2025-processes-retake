# Минимальное значение элементов вектора

- Студент: Назыров Анвар, группа 3823Б1ПР4
- Технология: SEQ + MPI
- Вариант: 4

## 1. Введение

Задача заключается в нахождении минимального значения среди
всех элементов целочисленного вектора. Это классическая задача
редукции данных, которая хорошо распараллеливается за счёт
распределения элементов вектора между процессами.

Цель работы — реализовать последовательную и параллельную
версии алгоритма поиска минимума, провести тестирование
корректности и производительности.

## 2. Постановка задачи

**Входные данные:** Вектор целых чисел (`std::vector<int>`).

**Выходные данные:** Минимальное значение (`int`).

**Ограничения:**

- Вектор непустой

Тип входных данных:

```cpp
using InType = std::vector<int>;
```

Тип выходных данных:

```cpp
using OutType = int;
```

## 3. Базовый алгоритм (последовательный)

Последовательная версия проходит по всему вектору, сравнивая
каждый элемент с текущим минимумом:

```cpp
bool MinValVecSEQ::RunImpl() {
  const auto &input = GetInput();
  int min_elem = input[0];
  for (std::size_t i = 1; i < input.size(); ++i) {
    min_elem = std::min(input[i], min_elem);
  }
  GetOutput() = min_elem;
  return true;
}
```

**Сложность:** O(N) — один проход по вектору из N элементов.

## 4. Схема распараллеливания

### 4.1. Распределение данных

Вектор разбивается на части с помощью `MPI_Scatterv`.
Каждый процесс получает приблизительно `N / size` элементов.

### 4.2. Локальный поиск минимума

Каждый процесс находит минимум в своей части вектора.

### 4.3. Глобальная редукция

Результат собирается через `MPI_Allreduce` с операцией
`MPI_MIN`, что обеспечивает нахождение глобального минимума.

## 5. Используемые функции MPI

- **MPI_Comm_rank** — получение номера текущего процесса
- **MPI_Comm_size** — получение общего количества процессов
- **MPI_Bcast** — рассылка размера вектора всем процессам
- **MPI_Scatterv** — распределение элементов вектора
- **MPI_Allreduce** — глобальная редукция для поиска минимума

## 6. Экспериментальная установка

| Параметр   | Значение         |
|------------|------------------|
| CPU        | —                |
| RAM        | —                |
| ОС         | —                |
| Компилятор | GCC (g++)        |
| MPI        | Open MPI         |
| Тип сборки | Release          |

## 7. Результаты

### 7.1 Корректность

Все функциональные тесты проходят успешно при разных
размерах вектора и разном числе процессов.

### 7.2 Производительность

Вектор из 10 000 000 int. Время указано в секундах.

#### Pipeline

| NP  | SEQ, с     | MPI, с     | Speedup |
|-----|------------|------------|---------|
| 1   | 0.0026     | 0.0402     | 0.06    |
| 2   | 0.0024     | 0.0149     | 0.16    |
| 4   | 0.0022     | 0.0141     | 0.15    |
| 6   | 0.0038     | 0.0113     | 0.34    |

#### Task Run

| NP  | SEQ, с     | MPI, с     | Speedup |
|-----|------------|------------|---------|
| 1   | 0.0022     | 0.0505     | 0.04    |
| 2   | 0.0024     | 0.0092     | 0.26    |
| 4   | 0.0035     | 0.0107     | 0.33    |
| 6   | 0.0057     | 0.0099     | 0.57    |

MPI-версия медленнее SEQ при всех конфигурациях.
Причина — минимальная вычислительная нагрузка (один проход
по массиву) при значительных коммуникационных затратах
(`MPI_Scatterv` + `MPI_Allreduce`). С ростом числа
процессов MPI-время снижается (с 50 мс при NP=1 до 10 мс
при NP=6), но SEQ остаётся быстрее из-за отсутствия
накладных расходов на обмен данными.

## 8. Выводы

Реализованы последовательная и параллельная версии алгоритма
поиска минимального значения в векторе. Все 16 функциональных
тестов проходят при запуске на 4 процессах MPI. Параллельная
версия не даёт ускорения на данной задаче из-за низкой
вычислительной интенсивности, однако корректно распределяет
данные и выполняет редукцию.

## 9. Источники

1. Сысоев А. В. Лекции курса
   «Параллельное программирование для кластерных систем»
2. Документация лабораторных работ —
   <https://learning-process.github.io/parallel_programming_course/ru/>

## Приложение

### Последовательная версия

```cpp
bool MinValVecSEQ::RunImpl() {
  const auto &input = GetInput();
  int min_elem = input[0];
  for (std::size_t i = 1; i < input.size(); ++i) {
    min_elem = std::min(input[i], min_elem);
  }
  GetOutput() = min_elem;
  return true;
}
```

### MPI-версия (ключевой фрагмент)

```cpp
// Рассылка размера вектора
MPI_Bcast(&total_size, 1, MPI_INT, 0, MPI_COMM_WORLD);

// Вычисление sendcounts и displs для Scatterv
int base = total_size / world_size;
int remainder = total_size % world_size;
for (int i = 0; i < world_size; ++i) {
  sendcounts[i] = base + (i < remainder ? 1 : 0);
  displs[i] = offset;
  offset += sendcounts[i];
}

// Распределение данных
MPI_Scatterv(rank == 0 ? GetInput().data() : nullptr,
             sendcounts.data(), displs.data(), MPI_INT,
             local_data.data(), local_size, MPI_INT, 0,
             MPI_COMM_WORLD);

// Локальный поиск минимума
int local_min = INT_MAX;
for (int i = 0; i < local_size; ++i) {
  local_min = std::min(local_data[i], local_min);
}

// Глобальная редукция
MPI_Allreduce(&local_min, &global_min, 1, MPI_INT,
              MPI_MIN, MPI_COMM_WORLD);
```
