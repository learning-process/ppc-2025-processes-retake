# Быстрая сортировка с простым слиянием

- Студент: Виняйкина Екатерина Александровна
- Группа: 3823Б1ПР3
- Технология: SEQ, MPI
- Вариант: 14

## 1. Введение

Быстрая сортировка (quicksort) относится к алгоритмам сортировки
 сравнением и в среднем имеет сложность O(n log n).
 Цель работы - реализовать последовательный и параллельный варианты
 быстрой сортировки с простым слиянием: последовательный вариант
 для одного процесса и параллельный с использованием технологии MPI,
 при котором данные распределяются по процессам, каждый процесс
 сортирует свою часть, после чего результаты собираются и сливаются
 на корневом процессе.

## 2. Постановка задачи

**Входные данные:** вектор целых чисел произвольной длины

**Выходные данные:** тот же набор чисел, отсортированный по неубыванию.

**Ограничения:** реализация параллельного алгоритма должна использовать
 MPI; в методе RunImpl необходимо применять MPI_Send либо MPI_Scatterv
 для распределения данных. Алгоритм должен быть детерминированным
 и корректно обрабатывать граничные случаи (пустой массив, один элемент,
 уже отсортированный массив, дубликаты, отрицательные числа).

## 3. Последовательный алгоритм

В качестве базового алгоритма используется итеративная быстрая
 сортировка с разбиением Хоара и выбором опорного элемента
 по середине отрезка.

**Шаги алгоритма:**

1. Инициализируется стек пар границ (left, right),
 в который помещается (0, n-1).
2. Пока стек не пуст: извлекается пара (lo, hi).
 Если lo >= hi, переход к следующей итерации.
3. Выбирается опорный элемент pivot = arr[lo + (hi - lo) / 2].
4. Выполняется разбиение Хоара: два индекса i и j движутся
 с концов к центру; элементы, меньшие pivot, остаются слева,
 большие - справа; при пересечении индексов выполняется обмен
 и сдвиг. В результате получаются границы (i, j),
 разделяющие массив на две части.
5. Если lo < j, в стек добавляется (lo, j);
 если i < hi, в стек добавляется (i, hi).
6. Шаги 2-5 повторяются до опустошения стека.

Рекурсия не используется (реализация полностью итеративная со стеком),
 что устраняет риск переполнения стека вызовов.
 Сложность по времени в среднем O(n log n),
 по памяти O(log n) для стека границ.

## 4. Схема распараллеливания (MPI)

**Распределение данных:** корневой процесс (rank 0) хранит
 исходный массив. Размер массива n рассылается всем процессам
 через MPI_Bcast. Каждому процессу назначается непрерывный кусок
 массива: размеры кусков вычисляются так, чтобы разница в размерах
 не превышала 1 (base_chunk = n / P, extra = n % P;
 первые extra процессов получают base_chunk + 1 элемент,
 остальные - base_chunk). Распределение выполняется с помощью
 MPI_Scatterv (sendcounts и displs задают размер и смещение
 для каждого ранга).

**Локальная сортировка:** каждый процесс получает свой подмассив
 и сортирует его той же итеративной быстрой сортировкой с разбиением
 Хоара (без обмена данными между процессами на этом этапе).

**Сбор и слияние:** отсортированные куски собираются на ранге 0
 через MPI_Gatherv. На ранге 0 выполняется последовательное слияние
 упорядоченных отрезков: из собранного массива берутся куски
 в порядке рангов и попарно сливаются (merge двух отсортированных
 массивов в один) до получения одного отсортированного массива.

**Рассылка результата:** итоговый отсортированный массив рассылается
 всем процессам через MPI_Bcast, чтобы каждый процесс мог вернуть
 его в выходных данных задачи (требование тестового каркаса).

**Роли процессов:** все процессы участвуют в Bcast, Scatterv,
 локальной сортировке, Gatherv и финальном Bcast; только rank 0
 выполняет слияние и хранит полный результат до Bcast.

## 5. Детали реализации

**Структура кода:** общие типы InType/OutType (`std::vector<int>`)
 и BaseTask в common/include/common.hpp.
 Последовательная реализация: seq/include/ops_seq.hpp,
 seq/src/ops_seq.cpp (класс VinyaikinaEQuicksortSimpleSEQ).
 Параллельная: mpi/include/ops_mpi.hpp, mpi/src/ops_mpi.cpp
 (класс VinyaikinaEQuicksortSimpleMPI). В обоих вариантах используются
 локальные функции Partition (разбиение Хоара) и QuickSort
 (итеративная с явным стеком); в MPI дополнительно - MergeSorted
 для слияния двух отсортированных векторов.

**Граничные случаи:** пустой массив и массив из одного элемента
 не передаются в QuickSort; при n = 0 MPI-версия после Bcast
 выходит из RunImpl без Scatterv/Gatherv. Один процесс (P = 1)
 обрабатывается корректно: Scatterv и Gatherv передают весь массив
 на единственный ранг, слияние одного куска тривиально.

**Память:** на каждом процессе хранится локальный кусок размера
 порядка n/P; на ранге 0 дополнительно - полный массив для сбора
 и результат слияния. Общий объем O(n) по всем процессам.

## 6. Экспериментальная установка

**Оборудование и программное обеспечение:**

- Процессор: Intel Core i7-14700KF
- Количество ядер/потоков: 20 ядер / 28 потоков
- Оперативная память: 16 гб
- Операционная система: Windows 11

**Инструментарий:**

- Компилятор: MSVC 14.44
- Тип сборки: Release
- Версия MPI:  Microsoft MPI 10.1

**Данные:** тесты производительности используют массив фиксированного
 размера, заполненный в обратном порядке для гарантии нетривиальной
 работы сортировки

## 7. Результаты и обсуждение

### 7.1 Проверка корректности

Корректность проверяется функциональными тестами: параметризованные
 тесты для разных размеров ввода (3, 5, 7 элементов) для SEQ и MPI,
 а также отдельные тесты на граничные случаи - пустой массив,
 один элемент, уже отсортированный массив, массив в обратном порядке,
 все элементы одинаковые, отрицательные числа, смешанные знаки,
 дубликаты, два элемента (сортированный и нет), массив из 500
 элементов. Ожидаемый результат - вектор, совпадающий с эталонным
 (отсортированный ввод). Для эталона используется std::ranges::sort
 над копией ввода. Все тесты выполняются в едином пайплайне задачи
 (Validation, PreProcessing, Run, PostProcessing)

### 7.2 Производительность

#### Режим task_run

| Процессов | Время, с | Ускорение | Эффективность |
|-----------|----------|-----------|---------------|
| 1         | 0.223    | 1.00      | -             |
| 2         | 0.181    | 1.23      | 61.5%         |
| 4         | 0.206    | 1.08      | 27.1%         |
| 6         | 0.257    | 0.87      | 14.4%         |
| 8         | 0.332    | 0.67      | 8.4%          |

#### Режим task_pipeline

| Процессов | Время, с | Ускорение | Эффективность |
|-----------|----------|-----------|---------------|
| 1         | 0.226    | 1.00      | -             |
| 2         | 0.185    | 1.22      | 61.1%         |
| 4         | 0.209    | 1.08      | 27.0%         |
| 6         | 0.258    | 0.87      | 14.6%         |
| 8         | 0.325    | 0.70      | 8.7%          |

Ускорение считается относительно времени SEQ на одном процессе.
 Эффективность = (время SEQ / P) / время MPI, в процентах.
 На двух процессах наблюдается ускорение около 1.22-1.23
 и эффективность около 61%. При увеличении числа процессов время MPI
 растет из-за накладных расходов на коммуникации и последовательного
 слияния на ранге 0, поэтому ускорение падает, а при 6 и 8 процессах
 параллельный вариант оказывается медленнее последовательного.

## 8. Выводы

Реализованы последовательная и параллельная (MPI) версии быстрой
 сортировки с простым слиянием. Последовательный алгоритм использует
 итеративную схему с разбиением Хоара и стеком границ. Параллельный
 распределяет данные через MPI_Scatterv, выполняет локальную сортировку
 на каждом процессе и собирает результат с последующим последовательным
 слиянием на корневом процессе и рассылкой через MPI_Bcast.
 Корректность подтверждена функциональными тестами,
 включая граничные случаи.

Ограничения: слияние выполняется только на одном процессе
 и может становиться узким местом при большом числе процессов.

## 9. Источники

1. Кормен Т., Лейзерсон Ч., Ривест Р., Штайн К. Алгоритмы:
 построение и анализ. 3-е изд. М.: Вильямс, 2013.
2. Воеводин В.В., Воеводин Вл.В. Параллельные вычисления.
 СПб.: БХВ-Петербург, 2002.
3. Антонов А.С. Параллельное программирование с использованием
 технологии MPI. М.: Изд-во МГУ, 2004.
4. MPI Forum. MPI: A Message-Passing Interface Standard. Version 4.0.
2021. <https://www.mpi-forum.org/docs/>
5. Сысоев А.В., лекции по курсу "Параллельное программирование" -
 ННГУ, 2025 год.

## Приложение (фрагменты кода)

Итеративное разбиение Хоара:

```cpp
std::pair<int, int> Partition(std::vector<int> &arr, int lo, int hi) {
  int pivot = arr[lo + ((hi - lo) / 2)];
  int i = lo, j = hi;
  while (i <= j) {
    while (arr[i] < pivot) i++;
    while (arr[j] > pivot) j--;
    if (i <= j) { std::swap(arr[i], arr[j]); i++; j--; }
  }
  return {i, j};
}
```

Распределение и сбор в MPI (фрагмент RunImpl):

```cpp
MPI_Scatterv(data_.data(), send_counts.data(), offsets.data(), MPI_INT,
             local_data.data(), send_counts[rank], MPI_INT, 0, MPI_COMM_WORLD);
// ... локальная QuickSort(local_data, ...) ...
MPI_Gatherv(local_data.data(), send_counts[rank], MPI_INT, gathered.data(),
            send_counts.data(), offsets.data(), MPI_INT, 0, MPI_COMM_WORLD);
```
