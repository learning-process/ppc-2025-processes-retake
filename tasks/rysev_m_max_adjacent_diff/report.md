<Task Нахождение максимальной разности между соседними элементами вектора>

    Student: <Рысев Михамл>, group <3823Б1ФИ2>

    Technology: <SEQ | MPI>

    Variant: <...>

1. Introduction

При анализе числовых последовательностей часто требуется выявлять аномальные скачки значений между соседними элементами. Задача поиска максимальной разности между соседними элементами вектора является важной операцией при обработке сигналов, анализе временных рядов и обнаружении выбросов. Параллельная реализация с использованием MPI позволяет существенно ускорить обработку больших объемов данных за счет распределения вычислений между несколькими процессами.

2. Problem Statement

Для заданного вектора целых чисел V = [v₁, v₂, ..., vₙ] необходимо найти пару соседних элементов (vᵢ, vᵢ₊₁), для которых абсолютная разность |vᵢ₊₁ - vᵢ| максимальна. Если таких пар несколько, возвращается первая найденная.

Входные данные: std::vector<int> - последовательность целых чисел
Выходные данные: std::pair<int, int> - пара соседних элементов с максимальной разностью

Ограничения:

    Вектор должен содержать как минимум 2 элемента

    Элементы могут быть любыми целыми числами (положительными, отрицательными или нулевыми)

    В случае нескольких пар с одинаковой максимальной разностью возвращается первая

3. Baseline Algorithm (Sequential)

Последовательный алгоритм выполняет линейный проход по вектору, вычисляя абсолютную разность между каждой парой соседних элементов и сохраняя пару с максимальной разностью.

Сложность алгоритма: O(n), где n - размер входного вектора.

4. Parallelization Scheme
Распределение данных

Входной вектор распределяется между процессами с помощью MPI_Scatterv. Используется блочное распределение с учетом остатка: если размер вектора не делится равномерно на количество процессов, первые remainder процессов получают на один элемент больше.
Паттерн вычислений

Каждый процесс:

    Получает свой сегмент данных
    Вычисляет локальный максимум разности внутри своего сегмента
    Обменивается граничными элементами с соседними процессами
    Участвует в глобальном сборе результатов

Роли процессов

    Процесс 0: распределяет данные, собирает результаты и определяет глобальный максимум
    Все процессы: выполняют локальные вычисления и обмениваются граничными элементами

Обработка границ

Критически важным аспектом является корректная обработка границ между сегментами:

    Каждый процесс (кроме первого) получает последний элемент от предыдущего процесса
    Каждый процесс (кроме последнего) отправляет свой последний элемент следующему процессу
    Разность на границе сегментов вычисляется и учитывается при поиске максимума

5. Implementation Details
Структура проекта
text

rysev_m_max_adjacent_diff/
├── common/
│   └── common.hpp           // Общие типы данных (InType, OutType)
├── seq/
│   ├── ops_seq.hpp          // SEQ заголовок
│   └── ops_seq.cpp          // SEQ реализация
├── mpi/
│   ├── ops_mpi.hpp          // MPI заголовок
│   └── ops_mpi.cpp          // MPI реализация
└── tests/
    ├── functional/          // Функциональные тесты
    └── performance/         // Тесты производительности

Ключевые классы

    RysevMMaxAdjacentDiffSEQ - последовательная реализация
    RysevMMaxAdjacentDiffMPI - параллельная MPI реализация

Обработка особых случаев

    Вектор с одним элементом → возвращается (0, 0)
    Пустой сегмент у процесса → корректная обработка отправки/получения границ
    Все элементы равны → максимальная разность = 0, возвращается первая пара

6. Experimental Setup
Hardware/OS

Процессор: AMD Ryzen 5 3500U, 4 ядра, 8 потоков Тактовая частота: 2.10 GHz (базовая), до 3.7 GHz (Boost) Оперативная память: 8 GB DDR4 Накопитель: 477 GB SSD Операционная система: Windows 10 Home 22H2 (сборка 19045.6456)

Software

    Компилятор: g++ (GCC) 14.2.0
    MPI реализация: OpenMPI (в Docker-контейнере)
    Сборка: Release (оптимизация -O2)
    Тестовые данные: Случайные векторы размером 10^8 элементов

Environment

    Запуск в Docker-контейнере для воспроизводимости результатов
    Количество процессов: 2, 4
    Количество измерений: 5 запусков для каждого режима (усреднение)

7. Results and Discussion
7.1 Correctness

Корректность реализации проверена с помощью набора функциональных тестов, покрывающих различные сценарии:

    Случайные векторы различных размеров
    Крайние случаи (минимальный размер, одинаковые элементы, монотонные последовательности)
    Проверка граничных условий между процессами

7.2 Performance

SEQ версия алгоритма выполняется за 0,0006236076 с., а MPI версия за 0,0006140276 с.
Доля ускорения составляет 1,02
8. Conclusions

В ходе работы успешно реализована параллельная MPI версия алгоритма поиска максимальной разности между соседними элементами вектора.

Основные достижения:

    Разработана масштабируемая схема распараллеливания с корректной обработкой граничных элементов
    Простота задачи не несёт смысла в её распараллеливании 

Наблюдения:

    Основные накладные расходы связаны с коммуникациями, что особенно заметно при увеличении числа процессов
    Для достижения лучшего масштабирования на большем числе процессов требуется оптимизация обмена данными

Перспективы улучшения:

    Использование неблокирующих операций обмена граничными элементами
    Асинхронный сбор результатов
    Динамическая балансировка нагрузки при сильной неравномерности данных

9. References

    Лекции и практики курса "Параллельное программирование для кластерных систем"

