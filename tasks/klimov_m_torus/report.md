# Решетка-тор (Mesh-Torus)

- **Студент**: Климов Михаил Дмитриевич, группа 3823Б1ПР2
- **Технология**: MPI / SEQ
- **Вариант**: 9

## 1. Введение
Цель работы – реализовать виртуальную топологию «решётка-тор» средствами MPI, используя только точечные коммуникации (`MPI_Send`/`MPI_Recv`) без применения готовых функций создания топологий (`MPI_Cart_create`). Задача заключается в организации передачи данных от произвольного процесса-отправителя к произвольному процессу-получателю в рамках двумерной тороидальной решётки, а также в построении маршрута передачи.

## 2. Постановка задачи
На вход подаются три параметра: номер исходного процесса `source`, номер целевого процесса `dest` и вектор целочисленных данных `payload`. Требуется передать данные от `source` к `dest` по кратчайшему пути в топологии «решётка-тор» и зафиксировать пройденный маршрут (последовательность номеров процессов). Размеры решётки определяются автоматически исходя из общего числа MPI-процессов (квадратная или прямоугольная сетка, максимально близкая к квадрату). Выходные данные – полученный вектор `payload` и вектор `path`, содержащий все промежуточные процессы (включая источник и цель).

## 3. Описание последовательного алгоритма
Последовательная версия реализована в классе `TorusSequential`. Поскольку в последовательном случае передача данных не требуется, алгоритм тривиален:
1. Проверка корректности входных данных (`source` и `dest` неотрицательны).
2. Копирование входного вектора `payload` в выходной.
3. Формирование пути: если `source == dest`, путь состоит из одного элемента; иначе – из двух элементов: `source` и `dest`.

Данная реализация служит эталоном для проверки корректности MPI-версии на малом числе процессов.

## 4. Схема распараллеливания (MPI)
Параллельная версия реализована в классе `TorusNetworkMpi`. Основные этапы:

1. **Определение размеров решётки**. На этапе предварительной обработки все процессы вычисляют размеры сетки `gridRows_` и `gridCols_` на основе общего числа процессов `worldSize_`. Используется метод `CalculateGridDimensions`, который подбирает такие `rows` и `cols`, чтобы их произведение равнялось `worldSize_`, а сетка была как можно более квадратной.

2. **Преобразование «ранг ↔ координаты»**. Реализованы функции `RankFromCoordinates` и `CoordinatesFromRank`, позволяющие переходить от двумерных координат `(row, col)` к рангу MPI и обратно. Учёт тороидальности обеспечивается взятием по модулю.

3. **Построение кратчайшего пути**. Метод `BuildRoute` строит маршрут от `source` до `dest` в тороидальной сетке. Сначала происходит движение по столбцам (выбирается кратчайшее направление – вправо или влево с учётом зацикливания), затем – по строкам. Полученный путь – это последовательность рангов, через которые пройдёт сообщение.

4. **Распространение данных**. Процесс с рангом 0 рассылает всем номера `source` и `dest`. Далее процесс-отправитель (`source`) широковещательно передаёт размер своего вектора `payload` (через `MPI_Bcast`), а затем все процессы, участвующие в пути, организуют цепочку передачи:
   - Отправитель сохраняет свои данные в буфере и отправляет их следующему процессу в пути (если он не является получателем).
   - Каждый промежуточный процесс принимает данные от предыдущего, сохраняет их у себя и, если он не является конечным получателем, отправляет дальше.
   - Получатель принимает данные и сохраняет их в выходной структуре, также фиксируя полный путь.

Взаимодействие между соседями по пути осуществляется только с помощью `MPI_Send` и `MPI_Recv`, что строго соответствует условию задачи (без использования коллективных операций для пересылки данных).

## 5. Детали реализации
- Класс `TorusNetworkMpi` содержит вспомогательные методы: `CalculateGridDimensions`, `RankFromCoordinates`, `CoordinatesFromRank`, `BuildRoute`, `BroadcastSourceAndDestination`, `BroadcastDataSize`, `PrepareDataBuffer`, `ForwardData`, `SaveResult`.
- Все обмены данными между процессами выполняются по принципу «каждый передаёт только следующему»; гарантируется, что сообщение не зациклится и дойдёт до адресата.
- Валидация входных данных включает проверку на то, что `source` и `dest` находятся в допустимых пределах и что MPI инициализирован.
- Последовательная версия `TorusSequential` максимально упрощена и служит только для функционального тестирования.
- Тесты параметризованы размером данных (`1, 4, 8, 16, 32` элемента) и проверяют, что:
  - Данные доходят до получателя без искажений.
  - Путь содержит корректные начальную и конечную точки.
  - Промежуточные процессы (кроме получателя) не сохраняют результат.

## 6. Экспериментальная установка
- **Аппаратное обеспечение**: виртуальная среда (контейнер Docker) с выделением 4 ядер и 8 ГБ ОЗУ.
- **ОС**: Ubuntu 24.04 LTS (внутри контейнера).
- **Компилятор**: g++-14.
- **Сборка**: CMake с типом `RelWithDebInfo`, санитайзеры не использовались при финальных тестах.
- **Среда выполнения**: MPI-тесты запускались с `PPC_NUM_PROC=2`, `4`, `8` (для проверки при разных размерах сетки) через скрипт `scripts/run_tests.py`. Количество процессов подбиралось так, чтобы можно было построить прямоугольную сетку (например, 2→1x2, 4→2x2, 8→2x4 или 4x2).

## 7. Результаты и обсуждение

### 7.1 Корректность
Функциональные тесты покрывают следующие сценарии:
- Передача данных от процесса 0 к процессу `world_size-1` (наиболее удалённому).
- Различные размеры полезной нагрузки (от 1 до 32 целых чисел).
- Проверка, что только процесс-получатель сохраняет данные, а остальные (включая промежуточные) возвращают пустой результат.
- Проверка, что путь начинается с `source` и заканчивается `dest`.

Все тесты успешно проходятся как для последовательной, так и для MPI-версии при любом допустимом количестве процессов. Это подтверждает правильность построения пути и передачи данных в тороидальной топологии.

### 7.2 Особенности реализации
- Использованный метод построения пути (сначала по столбцам, затем по строкам) всегда даёт кратчайший маршрут в метрике Манхэттена на торе, так как движения по разным осям независимы.
- Благодаря модульным вычислениям координат (`(x % N + N) % N`) обеспечивается корректная работа на тороидальных границах.
- При `source == dest` данные не передаются по сети, и путь состоит из одного процесса.

## 8. Выводы
Разработана MPI-программа, реализующая передачу данных в топологии «решётка-тор» с использованием только `MPI_Send/Recv`. Программа автоматически определяет размеры сетки, строит кратчайший путь и выполняет доставку сообщения по цепочке процессов. Функциональное тестирование подтверждает корректность работы для различных конфигураций процессов и размеров данных. Реализация полностью соответствует условиям задачи и может служить основой для дальнейших экспериментов с маршрутизацией в тороидальных сетях.

## 9. Источники
1. Microsoft MPI : документация // Microsoft Learn. – URL: https://learn.microsoft.com/ru-ru/message-passing-interface/microsoft-mpi 
2. Сысоев А. В. Курс лекций по параллельному программированию