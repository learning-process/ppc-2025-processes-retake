# Сумма значений по строкам матрицы

- Студент: Виняйкина Екатерина Александровна
- Группа: 3823Б1ПР3
- Технология: SEQ, MPI
- Вариант: 11

## 1. Введение

Вычисление сумм по строкам матрицы - базовая операция в численных методах
 и анализе данных. При больших размерах матриц последовательное вычисление
 становится узким местом. Параллелизация с использованием MPI позволяет
 распределить вычисления между процессами и ускорить обработку.

Цель работы - реализовать последовательный и параллельный алгоритмы
 суммирования элементов по строкам матрицы с использованием технологии MPI,
 провести анализ производительности и сравнить результаты.

## 2. Постановка задачи

Дана матрица A размером M x N. Требуется вычислить вектор сумм строк S,
 где каждый элемент S[i] равен сумме всех элементов i-й строки матрицы A.

Формат входных данных: вектор целых чисел, где первые два элемента -
 количество строк M и столбцов N, далее следуют M*N элементов матрицы,
 записанных построчно.

Формат выходных данных: вектор из M целых чисел, содержащий суммы по строкам.

Ограничения:

- M > 0, N > 0
- Все элементы матрицы - целые числа
- Размер входного вектора должен быть равен M*N + 2

## 3. Последовательный алгоритм

Последовательный алгоритм выполняет следующие шаги:

1. Валидация входных данных: проверка корректности размеров матрицы
 и количества элементов.

2. Извлечение данных: из входного вектора извлекаются размеры матрицы (M, N)
 и сами элементы матрицы.

3. Вычисление сумм: для каждой строки i от 0 до M-1 выполняется суммирование
 всех элементов этой строки.
 Результат сохраняется в результирующий вектор.

Псевдокод последовательного алгоритма:

```text
Вход: вектор input [M, N, a[0][0], a[0][1], ..., a[M-1][N-1]]
Выход: вектор row_sums [sum_0, sum_1, ..., sum_{M-1}]

M = input[0]
N = input[1]
matrix = input[2..M*N+1]

для i от 0 до M-1:
    sum = 0
    для j от 0 до N-1:
        sum = sum + matrix[i*N + j]
    row_sums[i] = sum
```

Временная сложность алгоритма: O(M*N), так как требуется обработать
 каждый элемент матрицы ровно один раз.

Пространственная сложность: O(M*N) для хранения матрицы
 и O(M) для результирующего вектора.

## 4. Схема распараллеливания

Для параллелизации используется горизонтальная схема распределения данных:
 матрица делится между процессами по строкам. Каждый процесс получает
 подмножество строк и вычисляет суммы для своих строк независимо.

Схема распараллеливания:

1. Распределение данных (PreProcessing):
   - Процесс с рангом 0 извлекает размеры матрицы и данные из входного вектора
   - Размеры матрицы передаются всем процессам через MPI_Bcast

2. Распределение строк (RunImpl):
   - Вычисляется количество строк для каждого процесса:
    base_rows = M / P, где P - количество процессов
   - Первые (M % P) процессов получают на одну строку больше
   - Используется MPI_Scatterv для распределения строк матрицы
    между процессами
   - Каждый процесс получает свои строки в виде непрерывного массива

3. Локальные вычисления:
   - Каждый процесс вычисляет суммы для своих строк параллельно
   - Вычисления выполняются независимо, синхронизация не требуется

4. Сбор результатов (RunImpl, продолжение):
   - Используется MPI_Gatherv для сбора локальных сумм
    на процессе с рангом 0
   - Результаты собираются в правильном порядке строк

5. Распространение результатов (PostProcessing):
   - Процесс с рангом 0 передает полный вектор сумм всем процессам
    через MPI_Bcast
   - Это необходимо для обеспечения согласованности данных на всех процессах

Топология коммуникаций: используется коммуникатор MPI_COMM_WORLD
 с топологией "звезда", где процесс с рангом 0 является корневым узлом
 для операций Scatterv и Gatherv.

Временная сложность параллельного алгоритма: O(M*N/P) для вычислений
 плюс O(M*N) для коммуникаций в худшем случае.
 При большом количестве процессов коммуникации могут стать узким местом.

## 5. Детали реализации

Структура кода:

- common/include/common.hpp - определение типов данных (InType, OutType)
- seq/include/ops_seq.hpp, seq/src/ops_seq.cpp - последовательная реализация
- mpi/include/ops_mpi.hpp, mpi/src/ops_mpi.cpp - параллельная реализация с MPI
- tests/functional/main.cpp - функциональные тесты
- tests/performance/main.cpp - тесты производительности

Ключевые классы:

- VinyaikinaEMatrixSumSEQ - класс последовательной реализации
- VinyaikinaEMatrixSumMPI - класс параллельной реализации

Оба класса наследуются от BaseTask и реализуют методы:

- ValidationImpl() - валидация входных данных
- PreProcessingImpl() - предобработка данных
- RunImpl() - основная логика вычислений
- PostProcessingImpl() - постобработка результатов

Важные допущения:

- Матрица хранится в памяти в виде одномерного массива
 с построчным расположением элементов
- Количество процессов не превышает количество строк матрицы
- Все процессы имеют доступ к одинаковым входным данным
 (для функциональных тестов)

Граничные случаи:

- Матрица 1x1 (одна строка, один столбец)
- Матрица с одной строкой или одним столбцом
- Матрица с нулевыми элементами
- Матрица с отрицательными значениями
- Неравномерное распределение строк при делении M на P

Использование памяти:

- Последовательная версия: O(M*N) для матрицы + O(M) для результата
- Параллельная версия: O(M*N/P) для локальной части матрицы

- O(M) для результата на каждом процессе

## 6. Экспериментальная установка

Оборудование и программное обеспечение:

- Процессор: Intel Core i7-14700KF
- Количество ядер/потоков: 20 ядер / 28 потоков
- Оперативная память: 16 гб
- Операционная система: Windows 11

Инструментарий:

- Компилятор: MSVC 14.44
- Тип сборки: Release
- Версия MPI:  Microsoft MPI 10.1

Переменные окружения:

- PPC_NUM_PROC: количество процессов для запуска (2, 4, 6, 8)
- Другие релевантные переменные окружения

Тестовые данные:

- Функциональные тесты: матрицы размеров 3x3, 5x5, 7x7,
 а также граничные случаи
- Тесты производительности: матрица 5000x5000, заполненная единицами
- Данные генерируются программно в тестах

## 7. Результаты и обсуждение

### 7.1 Проверка корректности

Корректность реализации проверялась с помощью набора функциональных тестов:

1. Параметрические тесты для квадратных матриц размеров 3x3, 5x5, 7x7
2. Тесты граничных случаев:
   - Матрица 1x1
   - Матрица с одной строкой (1xN)
   - Матрица с одним столбцом (Mx1)
   - Матрица с нулевыми элементами
   - Матрица с отрицательными значениями
   - Матрица со смешанными положительными и отрицательными значениями
   - Не квадратные матрицы (M != N)
   - Большие матрицы (50x60)

Все тесты выполнялись как для последовательной, так и для параллельной
 версии. Результаты обеих версий сравнивались между собой и с ожидаемыми
 значениями, вычисленными аналитически. Все тесты прошли успешно,
 что подтверждает корректность реализации.

### 7.2 Производительность

Измерения производительности проводились на матрице размером 5000x5000
 элементов. Результаты представлены в таблицах ниже.

#### Таблица 1: Режим task_run

| Процессов | SEQ время, с | MPI время, с | Ускорение | Эффективность |
|-----------|--------------|--------------|-----------|---------------|
| 1         | 0.04417      | 0.05712      | 0.77      | N/A           |
| 2         | 0.04452      | 0.03561      | 1.25      | 62.5%         |
| 4         | 0.04542      | 0.02499      | 1.82      | 45.4%         |
| 6         | 0.04699      | 0.02503      | 1.88      | 31.3%         |
| 8         | 0.04996      | 0.01947      | 2.57      | 32.1%         |

#### Таблица 2: Режим pipeline

| Процессов | SEQ время, с | MPI время, с | Ускорение | Эффективность |
|-----------|--------------|--------------|-----------|---------------|
| 1         | 0.04973      | 0.06296      | 0.79      | N/A           |
| 2         | 0.05301      | 0.04124      | 1.29      | 64.3%         |
| 4         | 0.06025      | 0.03196      | 1.89      | 47.1%         |
| 6         | 0.05796      | 0.02530      | 2.29      | 38.2%         |
| 8         | 0.07169      | 0.02679      | 2.68      | 33.5%         |

Примечание: Ускорение вычисляется как отношение времени последовательной
 версии к времени параллельной версии. Эффективность вычисляется как
 отношение ускорения к количеству процессов, выраженное в процентах.
 Для одного процесса ускорение меньше единицы из-за накладных расходов MPI
 при отсутствии реального параллелизма.

Анализ результатов:

Режим task_run измеряет только время выполнения основной логики (RunImpl),
 исключая время на предобработку и постобработку.
 Этот режим показывает чистую производительность параллельных вычислений.

Режим pipeline измеряет полное время выполнения всего пайплайна, включая
 валидацию, предобработку, вычисления и постобработку. Этот режим отражает
 реальное время выполнения задачи с учетом всех накладных расходов.

Наблюдения из экспериментальных данных:

Режим task_run:

- На одном процессе MPI версия работает медленнее последовательной
 (ускорение 0.77) из-за накладных расходов на инициализацию MPI
 и коммуникации без реального параллелизма
- С увеличением количества процессов до 2 и 4 наблюдается значительное
 улучшение производительности (ускорение 1.25 и 1.82 соответственно)
- При 6 и 8 процессах ускорение продолжает расти (1.88 и 2.57),
 но эффективность снижается до 31-32% из-за роста накладных расходов
 на коммуникации
- Максимальное ускорение достигается на 8 процессах и составляет 2.57

Режим pipeline:

- Аналогично task_run, на одном процессе MPI версия медленнее
 (ускорение 0.79)
- На 2 процессах ускорение составляет 1.29 с эффективностью 64.3%,
 что является наилучшей эффективностью среди всех измерений
- С увеличением количества процессов эффективность постепенно снижается:
 47.1% на 4 процессах, 38.2% на 6 процессах, 33.5% на 8 процессах
- Максимальное ускорение в режиме pipeline составляет 2.68 на 8 процессах
- Интересно отметить, что время выполнения последовательной версии
 в режиме pipeline увеличивается с ростом количества процессов
 (от 0.04973 до 0.07169), что может быть связано с особенностями работы
 тестового фреймворка

Сравнение режимов:

- Режим task_run показывает лучшее ускорение на 8 процессах
 (2.57 против 2.68 в pipeline), но это связано с тем, что pipeline
 включает дополнительные операции передачи данных
- Эффективность в обоих режимах снижается с ростом количества процессов,
 что типично для задач с интенсивными коммуникациями

Узкие места:

- Коммуникации MPI (Scatterv, Gatherv, Bcast) становятся основным
 ограничивающим фактором при большом количестве процессов
- Неравномерное распределение строк при делении 5000 на количество
 процессов может приводить к дисбалансу нагрузки,
 особенно заметному на 6 процессах
- Накладные расходы на передачу данных через MPI преобладают
 над вычислительной работой при большом количестве процессов

## 8. Выводы

В ходе выполнения лабораторной работы были реализованы последовательный
 и параллельный алгоритмы вычисления сумм по строкам матрицы
 с использованием технологии MPI.

Основные результаты:

- Последовательный алгоритм корректно вычисляет суммы строк
 для матриц различных размеров
- Параллельный алгоритм успешно распределяет вычисления между процессами
 и демонстрирует ускорение до 2.68 раз на 8 процессах
- Функциональные тесты подтверждают корректность реализации
 для различных граничных случаев
- Экспериментальные измерения показывают, что оптимальное количество
 процессов для данной задачи находится в диапазоне 2-4,
 где достигается наилучший баланс между ускорением и эффективностью

Ограничения и проблемы:

- Производительность параллельной версии ограничена накладными расходами
 на коммуникации, что приводит к снижению эффективности
 с ростом количества процессов
- На одном процессе MPI версия работает медленнее последовательной
 из-за накладных расходов без реального параллелизма
- Эффективность снижается с 64.3% на 2 процессах до 33.5%
 на 8 процессах в режиме pipeline
- Неравномерное распределение строк при делении 5000 на количество
 процессов может приводить к дисбалансу нагрузки между процессами
- При большом количестве процессов коммуникационные операции
 начинают преобладать над вычислительной работой

Возможные улучшения:

- Использование асинхронных операций MPI для перекрытия
 вычислений и коммуникаций
- Оптимизация распределения данных для более равномерной загрузки процессов

## 9. Источники

1. Гергель В.П. Теория и практика параллельных вычислений:
 Учебное пособие. - М.: Интернет-Университет Информационных Технологий,
2007. - 424 с.

2. Антонов А.С. Параллельное программирование с использованием
 технологии OpenMP: Учебное пособие. - М.: Издательство МГУ,
2009. - 77 с.

3. Message Passing Interface Forum. MPI: A Message-Passing Interface
 Standard, Version 4.0. - 2021.
 URL: <https://www.mpi-forum.org/docs/mpi-4.0/mpi40-report.pdf>

4. Грэхем Р.Л., Кнут Д.Э., Паташник О. Конкретная математика.
 Основание информатики. - М.: Мир, 1998. - 703 с.

5. Сысоев А.В., лекции по курсу по "Параллельное програмирование" - ННГУ, 2025 год.

6. Gropp W., Lusk E., Skjellum A. Using MPI: Portable Parallel
 Programming with the Message-Passing Interface. - 3rd ed. -
 MIT Press, 2014. - 392 p.

## Приложение

Пример ключевого фрагмента кода параллельной реализации:

```cpp
bool VinyaikinaEMatrixSumMPI::RunImpl() {
  int rank = 0;
  int world_size = 0;
  MPI_Comm_rank(MPI_COMM_WORLD, &rank);
  MPI_Comm_size(MPI_COMM_WORLD, &world_size);

  MPI_Bcast(&rows_, 1, MPI_INT, 0, MPI_COMM_WORLD);
  MPI_Bcast(&cols_, 1, MPI_INT, 0, MPI_COMM_WORLD);

  int base_rows = rows_ / world_size;
  int extra = rows_ % world_size;
  
  int local_rows = base_rows + (rank < extra ? 1 : 0);
  std::vector<int> local_data(static_cast<size_t>(local_rows) * cols_);
  
  MPI_Scatterv(matrix_.data(), sendcounts.data(), displs.data(), 
               MPI_INT, local_data.data(), local_rows * cols_,
               MPI_INT, 0, MPI_COMM_WORLD);

  std::vector<int> local_sums(local_rows);
  for (int i = 0; i < local_rows; i++) {
    int sum = 0;
    for (int j = 0; j < cols_; j++) {
      sum += local_data[(i * cols_) + j];
    }
    local_sums[i] = sum;
  }

  MPI_Gatherv(local_sums.data(), local_rows, MPI_INT, 
              row_sums_.data(), recvcounts.data(), rdispls.data(), 
              MPI_INT, 0, MPI_COMM_WORLD);
  
  return true;
}
```
