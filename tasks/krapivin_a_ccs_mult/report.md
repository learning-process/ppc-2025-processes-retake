# Умножение разреженных матриц. Элементы типа double. Формат хранения матрицы – столбцовый (CCS).

- Студент: Крапивин Александр Сергеевич, группа 3823Б1ПР1
- Технология: SEQ, MPI
- Вариант: 5

## 1. Введение
Умножение разреженных матриц - одна из ключевых операций в вычислительной математике, особенно востребованная при решении систем линейных уравнений, обработке графов и машинном обучении. Формат хранения CCS (Compressed Column Storage) позволяет эффективно хранить и обрабатывать разреженные матрицы за счёт сжатия ненулевых элементов по столбцам. Данная работа реализует параллельное умножение разреженных матриц в формате CCS с использованием MPI для распределения вычислений между узлами.

## 2. Постановка задачи
Разработать последовательную (SEQ) и параллельную (MPI) реализации умножения двух разреженных матриц, хранящихся в формате CCS, с элементами типа double.

Входные данные:
- Матрица A в формате CCS (значения, индексы строк, указатели столбцов)
- Матрица B в формате CCS (значения, индексы строк, указатели столбцов)
- Размеры матриц: rows_a, cols_a (совпадает с rows_b), cols_b

Выходные данные:
- Кортеж из (rows_a, cols_b, вектор значений результирующей плотной матрицы C)

## 3. Базовый алгоритм (Последовательный)
```cpp
bool KrapivinACcsMultSEQ::RunImpl() {
  ccs m1 = std::get<0>(GetInput());
  ccs m2 = std::get<1>(GetInput());

  std::vector<double> dense((m1.rows * m2.cols), 0.0);
  int cols_count = m2.cols;

  for(int i = 0; i < cols_count; i++) {
    int j1 = m2.col_index[i];
    int j2 = m2.col_index[i + 1];
    int col = i;

    for(int j = j1; j < j2; j++) {
      int row = m2.row[j];
      int k1 = m1.col_index[row];
      int k2 = m1.col_index[row + 1];
      

      for(int k = k1; k < k2; k++) {
        dense[(m1.row[k] * m2.cols) + col] += m1.val[k] * m2.val[j];
      }
    }
  }
  GetOutput() = std::make_tuple(m1.rows, m2.cols, dense);
  return true;
}
```
### Описание алгоритма
Алгоритм Густавсона.
Алгоритм последовательно перебирает все столбцы матрицы B (внешний цикл по i). Для каждого столбца:
- Определяются границы столбца B: по индексам j1 и j2 из col_index находятся начало и конец текущего столбца в массивах значений и индексов строк.
- Перебираются ненулевые элементы столбца B: для каждого такого элемента (цикл по j) извлекается номер строки row, в которой он находится. Этот номер строки соответствует номеру столбца в матрице A, который нужно использовать для умножения.
- Поиск соответствующих элементов в A: по индексу row (как номеру столбца A) определяются границы k1 и k2 для столбца A.
- Умножение и накопление результата: для каждого ненулевого элемента из столбца A (цикл по k) выполняется умножение 
m1.val * m2.val. Результат добавляется в элемент результирующей плотной матрицы с координатами (m1.row[], col), 
где m1.row[] - строка в A (она же строка в результате), а col - текущий столбец B (он же столбец в результате).

## 4. Схема распараллеливания
### Распределение данных
Разреженные матрицы копируются полностью на каждый процесс, так ка не занимают достаточно большого объема в памяти, затем происходит разбиение вычислений между процессами по столбцам результирующей матрицы C. Матрица B разделяется по столбцам между процессами, при этом каждый процесс получает полную копию матрицы A. Такой подход минимизирует объем передаваемых данных и сохраняет локальность вычислений.

```cpp
int step = m2.cols / mpi_size;
int rem = m2.cols % mpi_size;
std::vector<int> col_per_proc(mpi_size, step);
for(int i = 0; i < rem; i++){
    col_per_proc[i]++;
}

int start_col = 0;
for(int i = 0; i < rank; i++) {
    start_col += col_per_proc[i];
}
```

### Коммуникационная схема
1. Процесс 0 (root):
- Инициализирует структуры данных матриц A и B
- Определяет размеры и количество ненулевых элементов
2. Распространение данных:
- Размеры матриц и количество элементов рассылаются всем процессам через MPI_Bcast
- Матрица A полностью копируется на все процессы через серию MPI_Bcast
- Матрица B полностью копируется на все процессы через серию MPI_Bcast
3. Локальные вычисления:
- Каждый процесс определяет свой диапазон столбцов для обработки
- Выполняет умножение для назначенных столбцов, результат сохраняется в локальном буфере local_result_
4. Глобальная редукция:
- Все процессы участвуют в операции MPI_Reduce с операцией MPI_SUM
- Локальные результаты суммируются в глобальный результат на процессе 0
5. Распространение результата:
- В PostProcessingImpl процесс 0 рассылает финальный результат всем процессам через MPI_Bcast

## 5. Детали реализации
### Структура кода
- `common.hpp` - общие типы данных
- `ops_mpi.cpp` - MPI реализация
- `ops_seq.cpp` - SEQ реализация
- Тесты в папках `tests/functional/` и `tests/performance/`

### Особенности реализации
1. Валидация реализована в ValidationImpl
2. Балансировка нагрузки - равномерное распределение столбцов между процессами с учетом остатка от деления гарантирует, что все процессы получают примерно одинаковый объем работы
3. Управление памятью:
- На каждом процессе создается локальный буфер local_result_ размером rows_a × cols_b
- На процессе 0 выделяется глобальный буфер global_result такого же размера
- После редукции результат сохраняется в выходные данные
4. Особенности формата CCS
- Доступ к элементам осуществляется через указатели столбцов
- Индексация строк ведется от 0
- Для каждого столбца хранятся только ненулевые элементы

## 6. Экспериментальная установка
### Оборудование и ПО
- **Процессор:** AMD Ryzen 7 4800H (2.9 Ггц)
- **ОС:** Ubuntu devcontainer (host Windows)
- **Компилятор:** gcc
- **Тип сборки:** release

### Данные для тестирования
Все данные для тестов были сгенениррованны, ключ генерации был фиксрован.

- Генерируется 2 плоные матрицы согасно заданным размерам.
- Затем эти матрицы умножаются, результат умножения сохраняется для дальнейших проверок алгоритма умножения разреженных матриц.
- После из изначальных матриц формируются разреженные и передаются на исполнение алгоритму. 
- В конце вычислений происходит проверка плотной матрицы, полученной умножением матриц ccs.

Функциональные тесты представляют собой проверку алгоритма на наборе матриц разных корректых размеров, с низким процентом заполненности:
- матрица А `10 x 10` 10% заполенности, матрица B  `10 x 10` 20 % заполенности
- матрица А `100 x 50` 10% заполенности, матрица B  `50 x 100` 20 % заполенности
- матрица А `1000 x 1000` 1% заполенности, матрица B  `1000 x 1000` 1 % заполенности

Производительные тесты - умножение двух квадратных матриц размерами `2000 x 2000` и процентом заполненности равном 1%

## 7. Результаты и обсуждение
### 7.1 Проверка корректности
Все функциональные тесты пройдены:
- Корректность размеров выходных данных
- Соответствие индексов допустимым диапазонам

### 7.2 Производительность

| Процессы | Время, с | Ускорение | Эффективность |
|----------|-----------|-----------|---------------|
| 1 (SEQ)  |    0,04   | 1      | N/A           |
| 2        |    0,02   | 2      | 100%           |
| 4        |    0,15   | 0,27      | 6,75%           |

## 8. Выводы
В ходе работы была успешно решена задача умножения разреженных матриц в формате CCS с использованием последовательного алгоритма и технологии MPI для параллельных вычислений.
- Реализована корректная SEQ версия алгоритма умножения разреженных матриц
- Разработана MPI версия с вертикальным распределением по столбцам
- Обеспечена корректная обработка граничных случаев
- Проведен анализ производительности на различных размерах задач

### Ограничения
- Хранение полной матрицы A на каждом процессе ограничивает масштабируемость по памяти
- При большом количестве процессов накладные расходы MPI превышают выигрыш от распараллеливания
- Плотное хранение результата может быть неэффективно для сильно разреженных результирующих матриц


## 9. Источники
1. Курс лекций по параллельному программированию Сысоева Александра Владимировича. 
2. Документация по курсу: https://learning-process.github.io/parallel_programming_course/ru

## Приложение

```cpp
bool KrapivinACcsMultMPI::RunImpl() {
  int rank = 0;
  int mpi_size = 0;
  MPI_Comm_rank(MPI_COMM_WORLD, &rank);
  MPI_Comm_size(MPI_COMM_WORLD, &mpi_size);

  Ccs m1;
  Ccs m2;

  if (rank == 0) {
    m1 = std::get<0>(GetInput());
    m2 = std::get<1>(GetInput());
  }

  MPI_Bcast(&m1.rows, 1, MPI_INT, 0, MPI_COMM_WORLD);
  MPI_Bcast(&m1.cols, 1, MPI_INT, 0, MPI_COMM_WORLD);
  MPI_Bcast(&m2.rows, 1, MPI_INT, 0, MPI_COMM_WORLD);
  MPI_Bcast(&m2.cols, 1, MPI_INT, 0, MPI_COMM_WORLD);

  const int m1_el_count = (rank == 0) ? static_cast<int>(m1.val.size()) : 0;
  const int m2_el_count = (rank == 0) ? static_cast<int>(m2.val.size()) : 0;
  int m1_count = m1_el_count;
  int m2_count = m2_el_count;
  MPI_Bcast(&m1_count, 1, MPI_INT, 0, MPI_COMM_WORLD);
  MPI_Bcast(&m2_count, 1, MPI_INT, 0, MPI_COMM_WORLD);

  if (rank != 0) {
    m1.val.resize(static_cast<size_t>(m1_count));
    m1.row.resize(static_cast<size_t>(m1_count));
    m1.col_index.resize(static_cast<size_t>(m1.cols) + 1);
    m2.val.resize(static_cast<size_t>(m2_count));
    m2.row.resize(static_cast<size_t>(m2_count));
    m2.col_index.resize(static_cast<size_t>(m2.cols) + 1);
  }

  MPI_Bcast(m1.val.data(), m1_count, MPI_DOUBLE, 0, MPI_COMM_WORLD);
  MPI_Bcast(m1.row.data(), m1_count, MPI_INT, 0, MPI_COMM_WORLD);
  MPI_Bcast(m1.col_index.data(), m1.cols + 1, MPI_INT, 0, MPI_COMM_WORLD);
  MPI_Bcast(m2.val.data(), m2_count, MPI_DOUBLE, 0, MPI_COMM_WORLD);
  MPI_Bcast(m2.row.data(), m2_count, MPI_INT, 0, MPI_COMM_WORLD);
  MPI_Bcast(m2.col_index.data(), m2.cols + 1, MPI_INT, 0, MPI_COMM_WORLD);

  const int ncols = m2.cols;
  const int step = ncols / mpi_size;
  const int rem = ncols % mpi_size;
  const int start_col = (rank * step) + (rank < rem ? rank : rem);
  const int my_col_count = step + (rank < rem ? 1 : 0);

  for (int col = start_col; col < start_col + my_col_count; ++col) {
    const int j1 = m2.col_index[col];
    const int j2 = m2.col_index[col + 1];
    for (int j = j1; j < j2; ++j) {
      const int row_m2 = m2.row[j];
      const int k1 = m1.col_index[row_m2];
      const int k2 = m1.col_index[row_m2 + 1];
      for (int k = k1; k < k2; ++k) {
        const int row_m1 = m1.row[k];
        local_result_[(static_cast<size_t>(row_m1) * static_cast<size_t>(ncols)) + static_cast<size_t>(col)] +=
            m1.val[static_cast<size_t>(k)] * m2.val[static_cast<size_t>(j)];
      }
    }
  }

  const int result_count = m1.rows * m2.cols;

  if (rank == 0) {
    MPI_Reduce(MPI_IN_PLACE, local_result_.data(), result_count, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);
    GetOutput() = std::make_tuple(m1.rows, m2.cols, local_result_);
  } else {
    MPI_Reduce(local_result_.data(), nullptr, result_count, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);
  }
  return true;
}
```