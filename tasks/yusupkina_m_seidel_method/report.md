# <Итеративные методы (Зейделя)>

- Студент: <Юсупкина Маргарита Альбертовна>, группа <3823Б1ПМоп3>
- Технология: <SEQ | MPI >
- Вариант: <19>

## 1. Введение 
- Мотивация: Исследовать эффективность распараллеливания итерационного метода Гаусса-Зейделя для решения СЛАУ с использованием MPI.

- Контекст проблемы: Метод Гаусса-Зейделя является последовательным по своей природе, так как обновление каждой переменной использует уже обновлённые значения предыдущих переменных. Это создаёт сложности для распараллеливания, так как требует синхронизации после каждой итерации.

- Ожидаемый результат: MPI-версия может показать ускорение на больших матрицах, но эффективность будет ограничена необходимостью синхронизации и обмена данными между итерациями.


## 2. Постановка задачи
**Задача:** реализация последовательной (SEQ) и параллельной (MPI) метода Гаусса-Зейделя для решения СЛАУ.

- Вход: Матрица коэффициентов A (квадратная с диагональным преобладанием), вектор правых частей b, размерность системы n.
- Выход: Вектор решения x (размер n), найденный с заданной точностью 

**Ограничения** 
- Матрица должна обладать диагональным преобладанием — гарантия сходимости 
- Система может быть пустой 
- Поддерживаются вещественные числа
- Задано максимальное число итераций и точность


## 3. Описание алгоритма (Последовательного)
- Создаём вектор x, заполненный нулями — это начальное приближение решения.

**Для каждой итерации:**
- Считаем сумму произведений коэффициентов на текущие значения x для всех j ≠ i
- Вычисляем новое значение x[i] по формуле: x[i] = (b[i] - сумма) / A[i][i]
- Сразу обновляем x[i] — это ключевая особенность метода Зейделя
- Запоминаем, насколько изменилось значение (ошибку)

- После прохода по всем уравнениям находим максимальную ошибку
- Если максимальная ошибка меньше точности — останавливаемся
- Возвращаем найденный вектор x

## 4. Схема распараллеливания
**Проверка особого случая** 
- Если система пуста (n=0), все процессы сразу возвращают пустой вектор
- Если процессов больше чем уравнений, некоторые процессы получат 0 строк

**Распределение данных**
- Root-процесс (ранг 0) содержит полную матрицу A и вектор b
- Размерность n передаётся всем процессам через MPI_Bcast
- Базовое количество строк: `base = n / size`
- Остаток: `remainder = n % size`
- Каждый процесс получает `local_rows = base + (rank < remainder ? 1 : 0)` строк

**Коммуникационная схема**
- Каждый процесс узнаёт свой rank и общее количество процессов
- MPI_Bcast для рассылки размера системы n
- MPI_Scatterv для рассылки частей матрицы A и вектора b
На каждой итерации:
- Локальное вычисление — каждый процесс обновляет свои элементы x, используя текущий полный вектор x
- Через MPI_Allgatherv все процессы обмениваются обновлёнными частями и получают полный вектор x
- Через MPI_Allreduce находим максимальную ошибку среди всех процессов
- Если максимальная ошибка стала меньше заданной точности, решение достигнуто — итерации прекращаются


## 5. Детали реализации
**Струкрута кода**
yusupkina_m_seidel_method/
├── common/
│   └── include/
│       └── common.hpp - Общие типы данных (InType, OutType)
├── seq/
│   ├── include/
│   │   └── ops_seq.hpp - Заголовочный файл последовательной версии
│   └── src/
│       └── ops_seq.cpp - Реализация последовательной версии
├── mpi/
│   ├── include/
│   │   └── ops_mpi.hpp - Заголовочный файл MPI версии
│   └── src/
│       └── ops_mpi.cpp - Реализация MPI версии
└── tests/
    ├── functional/
    │   └── main.cpp - Функциональные тесты
    └── performance/
        └── main.cpp - Тесты производительности

**Ключевые классы**
- `YusupkinaMSeidelMethodMPI` — описание параллельного алгоритма
- `YusupkinaMSeidelMethodSEQ` — описание последовательного алгоритма
- Определение пространства имен `yusupkina_m_seidel_method` с обозначением используемых типов данных

**Ключевые функции**
- `YusupkinaMSeidelMethodMPI(const InType &in)` - конструктор
- `ValidationImpl()` - проверка входных данных
- `PreProcessingImpl()` - предварительная обработка данных
- `RunImpl()` - основная логика вычислений
- `PostProcessingImpl()` - постобработка результатов

**Вспомогательные функции**
- `RunOneIteration()` — выполняет одну итерацию на локальных данных

## 6. Окружение
- Hardware/OS: AMD Ryzen 7 8845H, 8 ядер, 16 потоков, 32GB RAM, Win11 Pro
- Toolchain: gcc, version 13.3.0, Release
- Environment: PPC_NUM_PROC: 1, 4, 8, Docker container with AMD-V virtualization
- Data: 
    - Functional tests- тесты различных размеров и типов
    - Performance tests- матрица 8000×8000 с диагональным преобладанием

## 7. Экспериментальные результаты

### 7.1 Корректность
**Корректность проверена с помощью функциональных тестов, охватывающих:**
- Пустые системы, один элемент, малые размеры
- Единичную матрицу, матрицы с диагональным преобладанием
- Дробные элементы матрицы и дробные решения
- Отрицательные элементы в матрице и отрицательные решения
- Нулевое решение и решения со смешанными знаками
- Разреженные матрицы

### 7.2 Производительность 
- Размер матрицы: 8000 × 8000 (трёхдиагональная)
- Максимальное число итераций: 1000
- Точность: 1e-6

| Режим | Процессы | Время (task_run), с | Ускорение | Эффективность |
|:-----:|:--------:|--------------------:|----------:|--------------:|
| SEQ   | 1        | 0.142               | 1.00      | n/a         |
| MPI   | 2        | 0.395               | 0.36      | 18%           |
| MPI   | 4        | 0.269               | 0.53      | 13%           |
| MPI   | 8        | 0.222               | 0.64      | 8%            |

- MPI версия медленнее SEQ из-за накладных расходов на коммуникации
- С ростом числа процессов время выполнения снижается, но ускорение остаётся ниже 1.0


## 8. Выводы
- Обе версии (SEQ и MPI) работают правильно на всех тестовых случаях
- Метод Гаусса-Зейделя плохо подходит для распараллеливания с использованием MPI из-за необходимости синхронизации на каждой итерации. На тестовой матрице 8000×8000 MPI версия не смогла превзойти последовательную.

Для получения реального ускорения метод Гаусса-Зейделя требует значительно больших размеров матриц (где вычисления начинают преобладать над коммуникациями). 
Однако в условиях выполнения в Docker-контейнере дальнейшее увеличение размера матрицы ограничено доступной памятью — при n > 8000 процессоры MPI начинают завершаться с ошибкой из-за нехватки памяти (signal 9), что не позволяет исследовать масштабирование на более крупных задачах.

## 9. References
1. Документация по курсу «Параллельное программирование» - URL: https://learning-process.github.io/parallel_programming_course/ru/
2. Курс лекций по параллельному программированию -URL: https://source.unn.ru

