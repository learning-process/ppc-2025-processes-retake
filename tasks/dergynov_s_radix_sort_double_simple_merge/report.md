# Поразрядная сортировка для вещественных чисел (тип double) с простым слиянием

* Студент: Дергунов Сергей Антонович
* Группа: 3823Б1ПР4
* Вариант: 20
* Технологии: SEQ, MPI

---

## 1. Введение

Сортировка больших массивов данных является одной из фундаментальных задач вычислительной математики и программирования.
Для вещественных чисел типа double часто используются алгоритмы сравнения (например, quicksort),
однако поразрядная сортировка (radix sort) может показать лучшую производительность на больших объёмах данных
благодаря линейной сложности O(n).

В данной работе реализована поразрядная сортировка для чисел типа double с использованием битового представления IEEE 754,
что позволяет корректно сортировать как положительные, так и отрицательные числа.
Параллельная версия использует MPI для распределения данных между процессами,
локальной сортировки и последующего простого слияния результатов на процессе 0.

Цель работы — реализовать последовательную и параллельную версии алгоритма,
провести тестирование корректности и оценить производительность MPI-версии.

---

## 2. Постановка задачи

### Входные данные (InType)

* `std::vector<double>` — неотсортированный массив вещественных чисел.

### Выходные данные (OutType)

* `std::tuple<std::vector<double>, int>` — отсортированный массив и ранг процесса (для MPI).

### Требования

* Корректная сортировка чисел типа double (включая отрицательные)
* Сохранение устойчивости сортировки
* Параллельная версия должна использовать только базовые операции MPI
* Простое слияние отсортированных подмассивов на процессе 0

---

## 3. Алгоритм поразрядной сортировки для double

### 3.1. Преобразование double в сортируемый формат

Числа типа double хранятся в формате IEEE 754:

* 1 бит знака
* 11 бит экспоненты
* 52 бита мантиссы

Для корректной сортировки необходимо преобразовать число так, чтобы:

* Положительные числа были больше отрицательных
* Отрицательные числа сортировались в обратном порядке (от больших по модулю к меньшим)

Алгоритм преобразования:

```cpp
uint64_t DoubleToSortableUint64(double d) {
    uint64_t u;
    std::memcpy(&u, &d, sizeof(double));
    if ((u & 0x8000000000000000ULL) != 0U) {
        u = ~u;  // отрицательные: инвертируем все биты
    } else {
        u |= 0x8000000000000000ULL;  // положительные: устанавливаем старший бит
    }
    return u;
}
```

### 3.2. Поразрядная сортировка по байтам

После преобразования числа сортируются как 64-битные беззнаковые целые
с использованием поразрядной сортировки по 8 битам (256 возможных значений).

Алгоритм для каждого прохода (8 проходов для 64 бит):

1. Подсчёт количества элементов для каждого значения байта
2. Вычисление префиксных сумм
3. Распределение элементов во временный массив
4. Копирование обратно

```cpp
void RadixSortUint64(std::vector<uint64_t> &keys) {
    const int kRadix = 256;
    std::vector<uint64_t> temp(keys.size());

    for (int shift = 0; shift < 64; shift += 8) {
        std::vector<size_t> count(kRadix + 1, 0);

        for (size_t i = 0; i < keys.size(); ++i) {
            uint8_t digit = (keys[i] >> shift) & 0xFF;
            ++count[digit + 1];
        }

        for (int i = 0; i < kRadix; ++i) {
            count[i + 1] += count[i];
        }

        for (size_t i = 0; i < keys.size(); ++i) {
            uint8_t digit = (keys[i] >> shift) & 0xFF;
            size_t pos = count[digit];
            temp[pos] = keys[i];
            count[digit] = pos + 1;
        }

        keys.swap(temp);
    }
}
```

---

## 4. Параллельная реализация (MPI)

### 4.1. Распределение данных

Массив равномерно распределяется между процессами:

```cpp
int base = n / size;
int rem = n % size;
int local_size = base + (rank < rem ? 1 : 0);
int offset = rank * base + std::min(rank, rem);
```

### 4.2. Локальная сортировка

Каждый процесс выполняет поразрядную сортировку своей части данных.

### 4.3. Сбор и слияние

Процесс 0 последовательно собирает отсортированные подмассивы
от всех процессов и выполняет простое слияние:

```cpp
result_ = local_data;
for (int p = 1; p < size; ++p) {
    int recv_count;
    MPI_Recv(&recv_count, 1, MPI_INT, p, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
    std::vector<double> part(recv_count);
    MPI_Recv(part.data(), recv_count, MPI_DOUBLE, p, 1, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
    result_ = MergeSorted(result_, part);
}
```

---

## 5. Программная реализация

### 5.1. Архитектура

* DergynovSRadixSortDoubleSimpleMergeSEQ — последовательная версия
* DergynovSRadixSortDoubleSimpleMergeMPI — параллельная MPI-версия
* Базовый класс ppc::task::Task<InType, OutType>
* Вспомогательные функции преобразования double ↔ uint64

### 5.2. Основные методы

```cpp
bool DergynovSRadixSortDoubleSimpleMergeMPI::RunImpl() {
    int rank, size;
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    MPI_Comm_size(MPI_COMM_WORLD, &size);

    const auto &input = GetInput();
    int n = static_cast<int>(input.size());

    // Рассылаем размер
    MPI_Bcast(&n, 1, MPI_INT, 0, MPI_COMM_WORLD);

    // Распределение данных
    std::vector<int> counts(size), displs(size);
    // ... вычисление смещений ...

    std::vector<double> local_data(counts[rank]);
    MPI_Scatterv(rank == 0 ? input.data() : nullptr,
                 counts.data(), displs.data(), MPI_DOUBLE,
                 local_data.data(), counts[rank], MPI_DOUBLE,
                 0, MPI_COMM_WORLD);

    // Локальная сортировка
    RadixSortDoubles(local_data);

    // Сбор и слияние на процессе 0
    if (rank == 0) {
        result_ = std::move(local_data);
        for (int p = 1; p < size; ++p) {
            // приём и слияние
        }
    } else {
        // отправка на процесс 0
    }

    return true;
}
```

---

## 6. Экспериментальная часть

### Аппаратная конфигурация

* Docker-контейнер (Linux)
* Компилятор: Clang 21.1.8
* Библиотека: OpenMPI
* Режим сборки: Release

### Параметры тестирования

* Размер массива: 1 000 000 элементов
* Диапазон значений: [-1000.0, 1000.0]
* Количество процессов: 1, 2, 4

### Результаты функционального тестирования

Все функциональные тесты успешно пройдены для следующих сценариев:

* Случайный массив
* Уже отсортированный массив
* Массив, отсортированный в обратном порядке
* Массив с одинаковыми элементами
* Пустой массив
* Массив из одного элемента

### Результаты производительности

| Процессы | Время выполнения (с) |    pipeline   |   task_run   |
| -------- | -------------------- | ------------- | ------------ |
| 1 (SEQ)  | 0.0657               | 0.0833        | 0.0657       |
| 1 (MPI)  | 0.0827               | 0.0948        | 0.0827       |
| 2 (MPI)  | 0.0460               | 0.0814        | 0.0460       |
| 4 (MPI)  | 0.0398               | 0.0584        | 0.0398       |

### Анализ производительности

* MPI-версия показывает ускорение при увеличении числа процессов
* На 2 процессах время уменьшилось ~1.8 раза по сравнению с 1 процессом
* На 4 процессах время уменьшилось ~2.1 раза
* Накладные расходы на коммуникацию частично компенсируются параллельной сортировкой

---

## 7. Заключение

В ходе работы были достигнуты следующие результаты:

1. Реализована поразрядная сортировка для чисел типа double с корректной обработкой знака
2. Создана MPI-версия с распределением данных и простым слиянием
3. Проведено функциональное тестирование — все тесты пройдены успешно
4. Измерена производительность — MPI-версия показывает ускорение при увеличении числа процессов

### Возможные улучшения

* Оптимизация локальной сортировки с использованием SIMD-инструкций
* Распараллеливание слияния на процессе 0 (например, бинарное дерево слияния)
* Использование неблокирующих операций MPI для перекрытия вычислений и коммуникаций

---

## 8. Литература

1. Gropp W., Lusk E., Skjellum A. Using MPI
2. IEEE Standard for Floating-Point Arithmetic (IEEE 754)
3. Cormen T. H. et al. Introduction to Algorithms
4. Документация GoogleTest
5. Сысоев А. В. Лекции по параллельному программированию
