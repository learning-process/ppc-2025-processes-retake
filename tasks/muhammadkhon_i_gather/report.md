## Введение

Коллективные операции являются важной частью параллельного
программирования с использованием MPI. Одной из базовых операций
является Gather, предназначенная для сбора данных со всех процессов
в один выбранный процесс (root).

Целью данной работы является разработка собственной реализации
операции Gather с использованием MPI без применения стандартной
функции MPI_Gather, а также сравнение её с последовательной
реализацией по корректности и производительности.

## Постановка задачи

Задача:
Реализовать операцию Gather для массива чисел типа double, используя:

- последовательную версию (SEQ),
- параллельную версию на основе MPI (MPI).

Входные данные:

- send_data — локальный массив данных процесса,
- root — номер процесса, в который должны быть собраны данные.

Выходные данные:

- recv_data — результирующий массив размером
world_size × local_count, содержащий данные всех процессов.

Ограничения:

- root должен находиться в диапазоне [0, world_size),
- все процессы должны передавать одинаковое количество элементов,
- нельзя использовать стандартный MPI_Gather,
- необходимо реализовать древовидную схему обмена.

# Описание алгоритма

Последовательная версия предназначена для случая одного процесса.

Алгоритм:

Проверяется, что root == 0.
Результирующий массив просто копируется:

recv_data = send_data

собственно сам код:

``` C++
bool MuhammadkhonIGatherSEQ::RunImpl() {
  const std::vector<double> &send_data = GetInput().send_data;
  GetOutput().recv_data = send_data;
  return true;
}
```

# Схема распараллеливания (MPI)

Общая идея

В параллельной версии реализована древовидная схема (binary tree
gather).

Каждый процесс:

- помещает свои данные в соответствующий сегмент общего буфера,
- участвует в обмене данными согласно шагам дерева,
- передает накопленные данные родителю.

Этапы алгоритма:

1) Broadcast размера данных

```C++
MPI_Bcast(&local_count, ...)
```

Все процессы получают размер передаваемого блока.

2) Инициализация буфера

Создается:

- gather_buffer — общий буфер размером world_size × local_count,
- received — массив флагов получения данных.

Каждый процесс:

- записывает свои данные в соответствующий сегмент,
- помечает себя как получившего данные.

3) Древовидный сбор (TreeGather)

```C++
while (step < world_size_) {
    if ((world_rank_ % (2 * step)) == 0) {
        ...
    } else if ((world_rank_ % step) == 0) {
        ...
        break;
    }
    step *= 2;
}
```

Если процесс удовлетворяет условию:

```C++
rank % (2*step) == 0
```

он принимает данные от процесса rank + step.

Если:

```C++
rank % step == 0
```

он отправляет накопленные данные родителю rank - step.

Таким образом строится бинарное дерево сбора.

Передаются:

- весь буфер данных,
- массив флагов получения.

4) Передача к корневому процессу

Если root ≠ 0, данные передаются от процесса 0 к root.

5) Рассылка финального результата

После завершения сбора:

```C++
MPI_Bcast(gather_buffer, ...)
```

Все процессы получают полный результат.

# Экперементальные результаты

Окружение

- ОС: Linux(Fedora)
- Компилятор: g++
- Сборка: Release
- MPI: OpenMPI

Результаты производительности

| Версия | Режим    | Время (сек) |
| ------ | -------- | ----------- |
| MPI    | pipeline | 0.26649     |
| MPI    | task_run | 0.26945     |
| SEQ    | pipeline | 0.01233     |
| SEQ    | task_run | 0.01232     |

# Выводы

Что показал эксперимент:

Что получилось:

- Реализована корректная древовидная схема сбора данных.
- Обеспечена поддержка произвольного root.
- Реализация прошла все тесты корректности.
- Показана работа без использования MPI_Gather.
