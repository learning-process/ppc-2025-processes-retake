 <Task Построение выпуклой оболочки – проход Грэхема>

- Student: <Казеннова Анастасия Михайловна>, group <3823Б1ФИ2>
- Technology: <SEQ | MPI>
- Variant: <24>

## 1. Introduction
Построение выпуклой оболочки – одна из фундаментальных задач вычислительной геометрии, имеющая множество приложений: обработка изображений, распознавание образов, компьютерная графика, геоинформационные системы. Алгоритм Грэхема позволяет построить выпуклую оболочку за O(n log n) операций, где n – количество точек. Параллельная реализация на MPI позволяет эффективно распределить вычисления между несколькими процессами, что критически важно при работе с большими наборами данных.

## 2. Problem Statement
Для заданного множества точек на плоскости необходимо построить выпуклую оболочку – минимальный выпуклый многоугольник, содержащий все точки множества.

Входные данные: std::vector<Point> - последовательность точек с координатами (x, y)
Выходные данные: std::vector<Point> - точки, образующие выпуклую оболочку в порядке обхода против часовой стрелки

Ограничения:

-Количество точек должно быть больше 0
-Координаты точек – вещественные числа
-Точки могут повторяться (дубликаты игнорируются при построении)

## 3. Baseline Algorithm (Sequential)
Последовательный алгоритм Грэхема включает следующие шаги:

Поиск pivot: находится самая нижняя-левая точка (минимальная по y, при равенстве – по x)
Сортировка: все остальные точки сортируются по полярному углу относительно pivot
Фильтрация коллинеарных точек: из точек с одинаковым углом оставляется самая дальняя от pivot
Построение оболочки: проход по отсортированным точкам с использованием стека, удаление точек при правом повороте

Сложность алгоритма: O(n log n), где n – количество точек.

## 4. Parallelization Scheme
Распределение данных
Исходное множество точек равномерно распределяется между MPI-процессами. Процесс с рангом 0 выполняет роль координатора:
рассылает всем процессам общее количество точек и распределяет точки поровну (с учётом остатка).
Каждый процесс получает непрерывный блок точек для локальной обработки.
Каждый процесс независимо строит локальную выпуклую оболочку для своего подмножества, затем результаты собираются на процессе 0 для построения финальной оболочки.

Роли процессов:
-Процесс 0: распределяет исходные данные, собирает локальные оболочки, строит финальную оболочку
-Все процессы: строят локальные выпуклые оболочки для своей порции точек

Объединение результатов
После локального построения все процессы отправляют свои локальные оболочки процессу 0. Процесс 0 собирает все точки из локальных оболочек и строит финальную выпуклую оболочку, применяя алгоритм Грэхема к объединённому множеству.

## 5. Implementation Details

kazenova_a_vec_change_sign/
├── common/
│   └── common.hpp            // Структура Point, общие типы и функции
├── seq/
│   ├── ops_seq.hpp         // SEQ заголовок
│   └── ops_seq.cpp         // SEQ реализация
├── mpi/
│   ├── ops_mpi.hpp         // MPI заголовок  
│   └── ops_mpi.cpp         // MPI реализация
└── tests/
    ├── functional/         // Функциональные тесты
    └── performance/        // Тесты производительности

Ключевые классы:

KazennovaAConvexHullSEQ - последовательная реализация
KazennovaAConvexHullMPI - параллельная MPI реализация

Вспомогательные функции:

DistSq - квадрат расстояния между точками
Orientation - определение ориентации тройки точек (векторное произведение)
PolarAngle - сравнение точек по полярному углу относительно pivot
SortQuick - собственная реализация быстрой сортировки (итеративная, без рекурсии)
FindMinIndex - поиск индекса минимального элемента
FilterCollinearPoints - фильтрация коллинеарных точек
BuildHull - построение оболочки из отфильтрованных точек

Особенности реализации:
Корректная обработка граничных случаев (менее 3 точек)

## 6. Experimental Setup
- Hardware/OS
Процессор: AMD Ryzen 5 3500U, 4 ядра, 8 потоков
Тактовая частота: 2.10 GHz  
Оперативная память: 8 GB RAM
Накопитель: 477 GB SSD
Операционная система: Windows 10 Home 22H2 (сборка 19045.6456)

- Компилятор: g++ 
- Использовался Docker-контейнер.
- Тип сборки: Release

- Environment
Количество процессов: 2, 4

## 7. Results and Discussion

### 7.1 Correctness
Корректность реализации проверена с помощью набора функциональных тестов, покрывающих различные сценарии:

Пустое множество точек
Менее 3 точек (оболочка = все точки)
Точки, образующие квадрат
Точки, образующие треугольник
Случайный набор точек (сравнение результатов последовательной и MPI версий)
Точки с коллинеарными участками (проверка фильтрации)

Все тесты пройдены успешно, результаты последовательной и MPI версий идентичны на процессе 0.

### 7.2 Performance

| Mode        | Count | Time, s  | Speedup | Efficiency |
|-------------|-------|----------|---------|------------|
| seq         | 1     | 0.000092 | 1.00    | N/A        |
| imp         | 2     | 0.000101 | 0.91    | 91.0%      |
| imp         | 4     | 0.000085 | 1.08    | 54.2%      |

Анализ производительности:

На малом размере данных (1000 точек) MPI версия на 2 процессах показывает небольшое ускорение (1.08x) по сравнению с последовательной версией
Эффективность параллелизации составляет 54%, что объясняется накладными расходами на коммуникацию между процессами

Для данного размера данных оптимальным является использование 2 процессов; дальнейшее увеличение числа процессов нецелесообразно из-за роста коммуникационных затрат
Ожидаемая производительность на больших данных:
При увеличении количества точек до сотен тысяч и миллионов параллельная версия должна показывать более существенное ускорение за счёт лучшего соотношения вычислений к коммуникациям.

## 8. Conclusions
Успешно реализована параллельная MPI версия алгоритма Грэхема для построения выпуклой оболочки.

Основные результаты:

Достигнуто ускорение 1.08x на 2 процессах при обработке 1000 точек
Эффективность параллелизации составляет 54% на 2 процессах
Функциональные тесты подтверждают корректность реализации на всех количествах процессов (1, 2, 4)

Ограничения:

На малых размерах данных (до 1000 точек) накладные расходы MPI превышают выгоду от распараллеливания
Для достижения существенного ускорения требуются наборы данных от 100 000 точек и более


## 9. References
1. Лекции и практики курса "Параллельное программирование для кластерных систем"