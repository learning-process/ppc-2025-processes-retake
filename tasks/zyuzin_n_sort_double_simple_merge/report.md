# Поразрядная сортировка для вещественных чисел (тип double) с простым слиянием

- Студент: Зюзин Никита Михайлович, группа 3823Б1ПР2
- Технологии: SEQ + MPI
- Вариант: 20

## 1. Введение

Задача сортировки больших объемов вещественных данных представляет актуальную проблему при обработке расчетов.
С ростом размера массива данных традиционные алгоритмы сортировки демонстрируют снижение
эффективности. Поразрядная сортировка (radix sort) позволяет достичь линейной сложности для целых чисел,
и может быть адаптирована для вещественных чисел путем интерпретации их битового представления.

Цель работы - разработать последовательный и параллельный алгоритмы поразрядной сортировки для вещественных чисел
типа `double` с использованием простого слияния и сравнить их производительность в многопроцессорной среде.

## 2. Постановка задачи

Задача: отсортировать массив вещественных чисел в порядке неубывания, используя поразрядную сортировку.

Входные данные: массив вещественных чисел типа `std::vector<double>`

Выходные данные: отсортированный массив вещественных чисел типа `std::vector<double>`

Ограничения:

- массив может содержать положительные, отрицательные числа и нуль;
- элементы представлены в виде чисел типа double (IEEE 754);
- размер массива может быть произвольным (включая пустой массив).

Ограничения программы:

- должна корректно обрабатывать массивы различных размеров;
- для параллельной реализации должна обеспечивать корректное распределение данных между процессами;
- результат должен быть идентичен для последовательной и параллельной версий.

## 3. Базовый (последовательный) алгоритм (Sequential)

Работа последовательного алгоритма проходит через четыре этапа:

1. `ValidationImpl()` - проверка входных данных (пустой массив считается валидным).
2. `PreProcessingImpl()` - подготовительный этап, выходной массив очищается.
3. `RunImpl()` - основной этап обработки массива:
   - преобразование вещественных чисел в беззнаковые 64-битные целые числа с учетом знака;
   - применение поразрядной сортировки по 8 байтам (256 возможных значений на байт);
   - преобразование отсортированных целых чисел обратно в вещественные.
4. `PostProcessingImpl()` - завершающий этап.

### Детали алгоритма

**Преобразование double → uint64_t:**

- Происходит побитовое копирование представления `double` в `uint64_t`;
- Для корректной сортировки применяется трансформация знака:
  - если старший бит (знаковый бит) равен 0 (положительное число), выполняется XOR с маской `0x8000000000000000`;
  - если старший бит равен 1 (отрицательное число), выполняется побитовое инвертирование (~);
- Эта трансформация обеспечивает, что числа с плавающей запятой отсортируются корректно при интерпретации как целые.

**Поразрядная сортировка (Radix Sort):**

- Использует сортировку подсчетом (counting sort) для каждого байта отдельно;
- Выполняется 8 проходов (для 8 байтов в `uint64_t`);
- На каждом проходе:
  - подсчитывается количество элементов для каждого значения байта (0-255);
  - вычисляются позиции для размещения элементов;
  - элементы перемещаются в новый массив в соответствии с рассчитанными позициями.
- Временная сложность: O(n × 8) = O(n), где n - количество элементов.

**Преобразование uint64_t → double:**

- Обратная трансформация:
  - если старший бит равен 0, выполняется побитовое инвертирование (~);
  - если старший бит равен 1, выполняется XOR с маской `0x8000000000000000`;
- Побитовое копирование исправленного значения обратно в `double`.

Полноценная реализация последовательного алгоритма представлена в Приложении (п.1).

## 4. Схема параллелизации

Идея параллелизации заключается в том, что массив можно разбить на несколько независимых блоков и
предоставить локальную сортировку каждому процессу, а затем объединить отсортированные блоки использованием
простого слияния.

Распределение данных:

- **Распределение массива**: исходный массив делится на n примерно равных блоков, где n равняется числу процессов;
- **Распределение нагрузки**: для равномерного распределения используются следующие формулы:
  - вычисляется размер блока: количество элементов делится (оператор `/`) на количество процессов;
  - вычисляется остаток: количество элементов делится (оператор `%`) на количество процессов;
  - первые k процессов (где k - остаток от деления) получают блоки увеличенного размера (размер + 1);
  - остальные процессы получают блоки ранее вычисленного размера;
- **Преобразование битов**: процесс 0 выполняет преобразование double → uint64_t, а затем распределяет данные.

Схема связи/топологии:

- Топология: коммуникатор MPI_COMM_WORLD;
- Коммуникационные операции:
  - `MPI_Bcast` - трансляция преобразованных битов от процесса 0 ко всем остальным;
  - `MPI_Scatterv` - распределение блоков данных между процессами;
  - `MPI_Send`/`MPI_Recv` - обмен данными между процессами при слиянии;
  - `MPI_Bcast` - трансляция результата со степени логарифма слияния (обычно из процесса 0) всем остальным.

Распределение ролей:

Процесс 0 имеет специальную роль (владеет исходными данными), остальные процессы равноправны:

1. **Процесс 0**:
   - преобразует входный массив из `double` в представление `uint64_t`;
   - вычисляет распределение данных между процессами;
   - передает преобразованные данные всем процессам используя `MPI_Bcast`;
   - распределяет блоки данных остальным процессам используя `MPI_Scatterv`;
   - выполняет локальную сортировку своей части;
   - получает результат слияния и преобразует обратно в `double`.

2. **Остальные процессы**:
   - получают преобразованные данные и информацию о распределении от процесса 0;
   - получают свой блок данных;
   - выполняют локальную поразрядную сортировку;
   - участвуют в процессе простого слияния через обмен с соседними процессами;
   - отправляют свою часть результата назад на процесс 0.

**Алгоритм простого слияния (Simple Merge):**

Использует дерево слияния, где каждый процесс с большим номером отправляет свои отсортированные данные
процессу с меньшим номером, и происходит обычное двухполюсное слияние отсортированных последовательностей:

Шаг 1: (процесс 1 → процесс 0), (процесс 3 → процесс 2), ...
Шаг 2: (процесс 2 → процесс 0), (процесс 6 → процесс 4), ...
Шаг 3: ...

На каждом шаге процесс получает два отсортированных блока и объединяет их в один отсортированный блок.

Полноценная реализация распараллеленного алгоритма представлена в Приложении (п.2).

## 5. Детали реализации

Файловая структура:

zyuzin_n_sort_double_simple_merge/
├── common/include
│   └── common.hpp                  # Базовые определения типов
├── mpi/
│   ├── include/ops_mpi.hpp         # MPI версия
│   └── src/ops_mpi.cpp
├── seq/
│   ├── include/ops_seq.hpp         # Последовательная версия
│   └── src/ops_seq.cpp
└── tests/
    ├── functional/main.cpp         # Функциональные тесты
    └── performance/main.cpp        # Производительные тесты

Ключевые классы:

- `ZyuzinNSortDoubleWithSimpleMergeSEQ` - последовательная реализация.
- `ZyuzinNSortDoubleWithSimpleMergeMPI` - параллельная реализация.

Основные методы:

- Общие для обеих реализаций:
  - `ValidationImpl()` - проверка входных данных;
  - `PreProcessingImpl()` - подготовительный этап;
  - `PostProcessingImpl()` - завершающий этап;
- `RunImpl()` - основной алгоритм обработки;
- `ConvertDoublesToBits()` - преобразование вещественных чисел в беззнаковые целые;
- `SortBits()` - поразрядная сортировка;
- `ConvertBitsToDoubles()` - преобразование целых чисел обратно в вещественные.

Вспомогательные методы для MPI версии:

- `MergeSegments()` - простое слияние отсортированных блоков от разных процессов;
- `MPI_Send`/`MPI_Recv` - обмен данными между процессами при слиянии.

Алгоритмические особенности:

- использование битового представления IEEE 754 для сортировки вещественных чисел;
- использование поразрядной сортировки с временной сложностью O(n);
- использование `MPI_Scatterv` для неравномерного распределения данных;
- использование простого слияния (binary merge) для объединения отсортированных блоков;
- корректное вычисление размеров блоков с учетом остатка от деления.

## 6. Экспериментальная среда

Hardware/OS:

- процессор: AMD Ryzen 5 5600x
- ядра/потоки: 6 ядер / 12 потоков
- оперативная память: 16 GB
- операционная система: Ubuntu 25.10
- архитектура: x64

Toolchain:

- компилятор: GCC 15.2.0
- IDE: Visual Studio Code 2026
- тип сборки: Release
- система сборки: CMake
- версия MPI: Open MPI 5.0.8

Environment:

- количество процессов: задается через `mpirun -n N`
- коммуникатор: MPI_COMM_WORLD

Тестовые данные:

1. Функциональные тесты:
   - тестовые данные различных размеров (100, 1000, 10000 элементов);
   - массивы с положительными, отрицательными и нулевыми значениями;
   - массивы с вещественными числами (дробные части);
   - граничные случаи (пустой массив, единственный элемент);
   - отсортированные и полностью перемешанные массивы.

2. Перформанс тесты:
   - массив размером 1,000,000 элементов (1×10^6);
   - элементы распределены в диапазоне [-5000.0, 5000.0];
   - случайные вещественные числа.

## 7. Результаты и обсуждение

### 7.1 Корректность

Корректность реализации была проверена следующими методами:

- разработано множество функциональных тестов с заранее известными ожидаемыми результатами;
- тесты охватывают различные сценарии:
  - массивы различных размеров;
  - массивы с положительными, отрицательными и нулевыми значениями;
  - массивы с вещественными числами (дробными);
  - граничные случаи (пустой массив, один элемент);
  - предсортированные и обратно отсортированные массивы.

Сравнение последовательной и параллельной версий:

- последовательная версия служит эталоном для проверки параллельной реализации;
- обе версии проходят идентичный набор функциональных тестов с одинаковыми входными данными;
- результаты последовательного и MPI алгоритмов полностью совпадают для всех тестовых случаев;
- различия в точности вычислений обусловлены порядком операций, но результаты логически эквивалентны.

### 7.2 Производительность

Входные данные: массив из 1,000,000 случайных вещественных чисел

Методы измерений:

- Каждый тест запускается 5 раз
- Берется среднее время выполнения (ΣTime / 5)
- Speedup = Time_seq / Time_mpi
- Efficiency = Speedup / Count * 100%

#### Время выполнения `task_run` (секунды)

| Mode | Count | Time  | Speedup | Efficiency |
|------|-------|-------|---------|------------|
| SEQ  | 1     | 0.159 | 1.00    | —          |
| MPI  | 2     | 0.118 | 1.35    | 67.50 %    |
| MPI  | 4     | 0.106 | 1.50    | 37.50 %    |
| MPI  | 6     | 0.110 | 1.44    | 24.00 %    |

#### Время выполнения `pipeline` (секунды)

| Mode | Count | Time  | Speedup | Efficiency |
|------|-------|-------|---------|------------|
| SEQ  | 1     | 0.158 | 1.00    | —          |
| MPI  | 2     | 0.120 | 1.32    | 66.00 %    |
| MPI  | 4     | 0.108 | 1.47    | 36.75 %    |
| MPI  | 6     | 0.112 | 1.41    | 23.50 %    |

Анализ результатов:

- Ускорение достигает 1.50× на 4 процессах для режима task_run, что является ожидаемым результатом
  для алгоритма дерева слияния;
- Эффективность составляет 67.5% на 2 процессах, что указывает на приемлемую масштабируемость;
- Значительное снижение эффективности с 67.50% на 2 процессах до 24.00% на 6 процессах объясняется:
  - логарифмической структурой алгоритма простого слияния (глубина log₂(n)), где часть процессов становится
    неактивной на поздних этапах;
  - для 4 процессов требуется 2 уровня слияния, для 6 процессов требуется 3 уровня;
  - увеличением накладных расходов MPI коммуникаций (MPI_Send, MPI_Recv) при растущем количестве обменов;
- Результаты для task_run и pipeline практически идентичны (разница менее 2%), что демонстрирует
  стабильность реализации;
- Алгоритм демонстрирует классическое поведение алгоритма слияния: хорошее ускорение на 2 процессах,
  но ограниченное масштабирование c ростом количества процессов из-за структуры дерева слияния.

## 8. Заключение

В ходе работы была успешно решена задача поразрядной сортировки массива из 1,000,000 вещественных чисел
с использованием последовательного алгоритма и технологии MPI для параллельных вычислений в двух вариантах
реализации: task_run и pipeline.

Основные результаты:

- разработаны корректные алгоритмы – созданы последовательная и параллельные версии, прошедшие полное
  тестирование на корректность результатов;
- реализована поразрядная сортировка с корректной трансформацией – алгоритм корректно
  обрабатывает вещественные числа путем интерпретации их битового представления;
- реализована схема параллельного слияния – данные распределяются между процессами, каждый процесс
  выполняет локальную поразрядную сортировку, затем результаты объединяются через дерево простого слияния;
- достигнуто ускорение до 1.50× на 4 процессах по сравнению с последовательной версией (для режима task_run).
  Эффективность составляет 67.50% на 2 процессах и снижается до 24.00% на 6 процессах, что объясняется
  логарифмической структурой дерева слияния (глубина log₂(n)) и накладными расходами MPI;
- оба варианта (task_run и pipeline) показали практически идентичные результаты (разница менее 2%),
  демонстрируя стабильность и надежность реализации.

Алгоритм демонстрирует поведение, типичное для алгоритмов слияния: хорошее ускорение на 2 процессах (1.35×),
но ограниченное масштабирование с ростом числа процессов из-за логарифмической структуры дерева слияния,
где на каждом уровне часть процессов становится неактивной.

## 9. Источники

1. Документация по курсу «Параллельное программирование» // URL:
   <https://learning-process.github.io/parallel_programming_course/ru/index.html>
2. Репозиторий курса «Параллельное программирование» // URL:
   <https://github.com/learning-process/ppc-2025-processes-engineers>
3. Сысоев А. В., Лекции по курсу «Параллельное программирование для кластерных систем».
4. IEEE 754 Standard for Floating-Point Arithmetic // URL:
   <https://en.wikipedia.org/wiki/IEEE_754>

## Приложение

П.1 – Последовательная реализация (`RunImpl`)

```cpp
bool ZyuzinNSortDoubleWithSimpleMergeSEQ::RunImpl() {
  const auto &input = GetInput();
  std::vector<double> data = input;
  auto bits = ConvertDoublesToBits(data);
  auto sorted_bits = SortBits(bits);
  auto sorted_data = ConvertBitsToDoubles(sorted_bits);
  GetOutput() = sorted_data;
  return true;
}
```

П.2 – Параллельная реализация (`RunImpl`)

```cpp
bool ZyuzinNSortDoubleWithSimpleMergeMPI::RunImpl() {
  const auto &input = GetInput();
  int rank = 0;
  int size = 0;
  MPI_Comm_rank(MPI_COMM_WORLD, &rank);
  MPI_Comm_size(MPI_COMM_WORLD, &size);

  std::vector<std::uint64_t> bits_data;
  ConvertDoublesToBits(rank, input, bits_data);

  int bits_per_proc = static_cast<int>(bits_data.size()) / size;
  int remainder = static_cast<int>(bits_data.size()) % size;

  std::vector<int> send_bits(size, 0);
  std::vector<int> displacements(size, 0);
  int offset = 0;
  for (int i = 0; i < size; ++i) {
    send_bits[i] = bits_per_proc + (i < remainder ? 1 : 0);
    displacements[i] = offset;
    offset += send_bits[i];
  }

  std::vector<std::uint64_t> local_bits(send_bits[rank]);
  MPI_Scatterv(static_cast<const void *>(bits_data.data()), send_bits.data(), displacements.data(), MPI_UINT64_T,
               static_cast<void *>(local_bits.data()), send_bits[rank], MPI_UINT64_T, 0, MPI_COMM_WORLD);

  std::vector<std::uint64_t> sorted_local_bits = SortBits(local_bits);

  std::vector<std::uint64_t> merged_bits = MergeSegments(sorted_local_bits, rank, size);

  int total_size = 0;
  if (rank == 0) {
    total_size = static_cast<int>(merged_bits.size());
  }
  MPI_Bcast(&total_size, 1, MPI_INT, 0, MPI_COMM_WORLD);

  if (rank != 0) {
    merged_bits.resize(total_size);
  }
  MPI_Bcast(static_cast<void *>(merged_bits.data()), total_size, MPI_UINT64_T, 0, MPI_COMM_WORLD);

  std::vector<double> result_sorted_doubles = ConvertBitsToDoubles(merged_bits);
  GetOutput() = result_sorted_doubles;
  return true;
}
```

П.3 – Функция преобразования double в uint64_t

```cpp
std::vector<std::uint64_t> ZyuzinNSortDoubleWithSimpleMergeSEQ::ConvertDoublesToBits(const std::vector<double> &data) {
  std::vector<std::uint64_t> bits(data.size(), 0);
  for (size_t i = 0; i < data.size(); ++i) {
    std::uint64_t x = 0;
    std::memcpy(&x, &data[i], sizeof(double));
    if ((x >> 63) == 0) {
      x ^= 0x8000000000000000;
    } else {
      x = ~x;
    }
    bits[i] = x;
  }
  return bits;
}
```

П.4 – Функция поразрядной сортировки

```cpp
std::vector<std::uint64_t> ZyuzinNSortDoubleWithSimpleMergeSEQ::SortBits(const std::vector<std::uint64_t> &bits) {
  const int radix = 256;
  const std::size_t n = bits.size();
  if (n == 0) {
    return {};
  }

  std::vector<std::uint64_t> source = bits;
  std::vector<std::uint64_t> destination(n);

  for (int byte_idx = 0; byte_idx < 8; ++byte_idx) {
    int shift = byte_idx * 8;

    std::vector<std::size_t> count(radix, 0);
    for (std::uint64_t value : source) {
      std::size_t digit = (value >> shift) & 0xFF;
      ++count[digit];
    }

    for (int i = 1; i < radix; ++i) {
      count[i] += count[i - 1];
    }

    for (std::size_t i = n; i > 0; --i) {
      std::uint64_t value = source[i - 1];
      std::size_t digit = (value >> shift) & 0xFF;
      destination[--count[digit]] = value;
    }

    source.swap(destination);
  }

  return source;
}
```

П.5 – Функция слияния двух отсортированных последовательностей (фрагмент)

```cpp
std::vector<std::uint64_t> ZyuzinNSortDoubleWithSimpleMergeMPI::MergeTwoVectors(const std::vector<std::uint64_t> &a,
                                                                                const std::vector<std::uint64_t> &b) {
  std::vector<std::uint64_t> result;
  result.reserve(a.size() + b.size());
  std::size_t i = 0;
  std::size_t j = 0;
  while (i < a.size() && j < b.size()) {
    if (a[i] <= b[j]) {
      result.push_back(a[i]);
      ++i;
    } else {
      result.push_back(b[j]);
      ++j;
    }
  }
  while (i < a.size()) {
    result.push_back(a[i]);
    ++i;
  }
  while (j < b.size()) {
    result.push_back(b[j]);
    ++j;
  }
  return result;
}
```
