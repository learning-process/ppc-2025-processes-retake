# Ленточная вертикальная схема - умножение матрицы на вектор

- Студент: Каур Андрей Михайлович
- Группа: ___
- Технология: MPI, SEQ
- Вариант: ___

## 1. Введение

Умножение матрицы на вектор является одной из базовых операций линейной алгебры и используется во многих вычислительных задачах. В данной работе реализована вертикальная ленточная схема распределения данных, при которой матрица делится между процессами по столбцам, а вектор также распределяется между процессами.

Цель работы - реализовать последовательный и параллельный алгоритмы умножения матрицы на вектор с использованием технологии MPI, провести тестирование корректности и оценить производительность на различном количестве процессов.

## 2. Постановка задачи

Дана матрица A размера M x N и вектор b размера N. Требуется вычислить вектор c = A * b размера M, где каждый элемент c[i] вычисляется как:

c[i] = sum(j=0 to N-1) A[i][j] * b[j]

Входные данные:
- Матрица A хранится в памяти в column-major порядке (по столбцам)
- Вектор b размера N
- Размеры матрицы: M строк, N столбцов

Выходные данные:
- Вектор c размера M

Ограничения:
- Матрица и вектор содержат элементы типа double
- Размеры матрицы положительные (M > 0, N > 0)
- Время выполнения тестов производительности должно быть в диапазоне от 0.01 до 10 секунд

## 3. Последовательный алгоритм 

Последовательный алгоритм выполняет умножение матрицы на вектор по такому принципу:

1. Инициализация результирующего вектора c нулевыми значениями
2. Для каждого столбца j матрицы:
   - Для каждой строки i:
     - Вычисление c[i] += A[j*M + i] * b[j]

Алгоритм использует хранение матрицы в column-major порядке, это означает, что элементы матрицы располагаются сначала все элементы первого столбца, затем все элементы второго столбца и так далее. Индекс элемента A[i][j] в одномерном массиве вычисляется как j*M + i, где M - количество строк.

Сложность алгоритма: O(M * N)

Реализация:
- ValidationImpl: проверяет корректность размеров матрицы и вектора
- PreProcessingImpl: копирует входные данные и инициализирует результирующий вектор
- RunImpl: выполняет основное вычисление умножения матрицы на вектор
- PostProcessingImpl: записывает результат в выходные данные

## 4. Схема распараллеливания

Для параллельной реализации используется вертикальная ленточная схема распределения данных с технологией MPI.

### 4.1 Распределение данных

Матрица A делится по столбцам между процессами. Если количество процессов равно P, а количество столбцов равно N, то:
- Базовое количество столбцов на процесс: base_cols = N / P
- Дополнительные столбцы (остаток): extra_cols = N % P
- Первые extra_cols процессов получают base_cols + 1 столбцов
- Остальные процессы получают base_cols столбцов

Вектор b распределяется аналогично - каждый процесс получает часть вектора, соответствующую его столбцам матрицы.

### 4.2 Коммуникационная схема

Процесс с рангом 0 выполняет следующие операции:

1. Валидация входных данных 
2. Broadcast размеров матрицы (rows, cols) всем процессам через MPI_Bcast
3. Распределение столбцов матрицы и соответствующих элементов вектора через MPI_Scatterv:
   - Вычисление массивов send_counts, displs для матрицы (кол-во элементов для каждого процесса и смещения)
   - Вычисление массивов vec_counts, vec_displs для вектора
   - Распределение данных с использованием MPI_Scatterv
4. Сбор частичных результатов через MPI_Reduce с операцией MPI_SUM
5. Broadcast финального результата всем процессам через MPI_Bcast

Каждый рабочий процесс:
- Получает размеры матрицы через MPI_Bcast
- Получает свою часть матрицы и вектора через MPI_Scatterv
- Выполняет локальное вычисление для своих столбцов
- Отправляет частичный результат на процесс 0 через MPI_Reduce
- Получает финальный результат через MPI_Bcast

### 4.3 Особенности реализации

Используется MPI_Scatterv вместо MPI_Scatter, так как количество столбцов может неравномерно делиться между процессами. Это позволяет корректно обработать случай, когда N не кратно P.

Результат broadcast'ится всем процессам для обеспечения корректной работы тестов, которые проверяют результат на всех процессах.

## 5. Детали реализации

### 5.1 Структура кода

Файлы реализации:
- common/include/common.hpp - общие типы данных
- seq/include/ops_seq.hpp, seq/src/ops_seq.cpp - последовательная реализация
- mpi/include/ops_mpi.hpp, mpi/src/ops_mpi.cpp - параллельная реализация
- tests/functional/main.cpp - функциональные тесты
- tests/performance/main.cpp - тесты производительности

### 5.2 Ключевые компоненты

Класс TaskData содержит:
- matrix: вектор элементов матрицы в column-major порядке
- vector: вектор элементов вектора
- rows: количество строк матрицы
- cols: количество столбцов матрицы

Классы KaurAVertRibbonSchemeSEQ и KaurAVertRibbonSchemeMPI наследуются от BaseTask и реализуют методы:
- ValidationImpl: проверка корректности входных данных
- PreProcessingImpl: подготовка данных
- RunImpl: основная логика вычислений
- PostProcessingImpl: формирование выходных данных

### 5.3 Особые случаи

Обработка граничных случаев:
- Проверка на положительные размеры матрицы
- Проверка соответствия размеров матрицы и вектора
- Корректная обработка случая, когда количество столбцов не делится нацело на количество процессов

### 5.4 Использование памяти

Для матрицы размера M x N требуется O(M * N) памяти. В параллельной реализации каждый процесс хранит только свою часть матрицы, что уменьшает требования к памяти на один процесс пропорционально количеству процессов.

## 6. Экспериментальная установка

### 6.1 Оборудование и ОС

- Процессор: AMD Ryzen 5 5600X (6 ядер, 12 потоков, 3.70 ГГц)
- ОЗУ: 16 ГБ DDR4 3400 МГц
- ОС: Windows 11 Pro 64-bit

Тестирование проводилось на CI системе с использованием виртуальных машин.

### 6.2 Инструментарий

- Компилятор: MSVC (Microsoft Visual C++) с поддержкой C++17
- Build type: Release
- MPI: Microsoft MPI 10.1.12498.52
- Система сборки: CMake

### 6.3 Переменные окружения

Количество процессов задается через mpiexec -n <количество_процессов>

### 6.4 Тестовые данные

Для тестов производительности используется матрица размера 8000 x 8000. Данные генерируются детерминированным образом для обеспечения воспроизводимости результатов:
- Элементы вектора: b[j] = (j % 10) + 1
- Элементы матрицы: A[i][j] = ((i + j) % 20) - 10

Для функциональных тестов используются матрицы меньших размеров (3x3, 5x5, 7x7) и специальные случаи (единичная матрица, нулевая матрица, прямоугольные матрицы).

## 7. Результаты и обсуждение

### 7.1 Проверка корректности

Корректность реализации проверяется следующими способами:

1. Функциональные тесты с детерминированными данными - вычисляется ожидаемый результат последовательно и сравнивается с результатом реализации
2. Тесты граничных случаев:
   - Матрица 1x1
   - Единичная матрица
   - Нулевая матрица и нулевой вектор
   - Прямоугольные матрицы
   - Отрицательные значения
   - Большие матрицы (100x100)
3. Сравнение результатов последовательной и параллельной реализаций

Все функциональные тесты проходят успешно, что подтверждает корректность реализации.

### 7.2 Производительность

Результаты измерений производительности представлены в таблицах ниже. Измерения проводились на матрице размера 8000 x 8000.

#### Таблица 1: task_run режим

| Режим | Процессов | Время, с | Ускорение | Эффективность |
|-------|-----------|----------|-----------|---------------|
| seq   | 1         | 0.687    | 1.00      | N/A           |
| mpi   | 1         | 0.877    | 0.78      | 78.3%         |
| mpi   | 2         | 0.618    | 1.11      | 55.6%         |
| mpi   | 4         | 0.635    | 1.08      | 27.1%         |
| mpi   | 6         | 1.120    | 0.61      | 10.2%         |
| mpi   | 8         | 0.604    | 1.14      | 14.2%         |

#### Таблица 2: pipeline режим

| Режим | Процессов | Время, с | Ускорение | Эффективность |
|-------|-----------|----------|-----------|---------------|
| seq   | 1         | 0.744    | 1.00      | N/A           |
| mpi   | 1         | 1.036    | 0.72      | 71.8%         |
| mpi   | 2         | 0.823    | 0.90      | 45.2%         |
| mpi   | 4         | 0.646    | 1.15      | 28.8%         |
| mpi   | 6         | 0.669    | 1.11      | 18.5%         |
| mpi   | 8         | 0.667    | 1.12      | 13.9%         |

### 7.3 Анализ результатов

Наблюдения:
1. Однопроцессная MPI реализация работает медленнее последовательной реализации из-за накладных расходов на инициализацию MPI и коммуникационные операции, даже если они минимальны.

2. При использовании 2 процессов наблюдается ускорение по сравнению с последовательной реализацией в режиме task_run, что говорит о том, что выигрыш от параллелизации перекрывает накладные расходы.

3. Эффективность параллелизации снижается с увеличением количества процессов. Это происходит по причинам:
   - Увеличением коммуникационных затрат 
   - Неравномерным распределением нагрузки при неполном делении количества столбцов на количество процессов
   - Накладными расходами на синхронизацию процессов

4. При 6 процессах в режиме task_run наблюдается значительное ухудшение производительности, что может быть связано с особенностями коммуникационной топологии или перегрузкой системы при тестировании.

5. Наибольшее ускорение достигается при использовании 2-4 процессов, что указывает на оптимальный диапазон для данной задачи и размера данных.

## 8. Выводы

В ходе работы были успешно реализованы последовательный и параллельный алгоритмы умножения матрицы на вектор с использованием вертикальной ленточной схемы распределения данных.

Основные результаты:
- Алгоритм корректно работает на различном количестве процессов
- Параллельная реализация показывает ускорение при использовании 2-4 процессов
- Эффективность параллелизации снижается с увеличением количества процессов из-за роста коммуникационных затрат

Ограничения:
- Для данной задачи и размера данных оптимальным является использование 2-4 процессов
- При большом количестве процессов накладные расходы на коммуникации могут превысить выигрыш от параллелизации
- Реализация эффективна для задач с достаточно большими матрицами, где выигрыш от параллелизации перекрывает накладные расходы

## 9. Источники

1. MPI: A Message-Passing Interface Standard (https://www.mpi-forum.org/docs/) — официальная документация MPI
2. Open MPI Documentation (https://www.open-mpi.org/doc/) — документация Open MPI
3. Learning Process (https://learning-process.github.io/) — учебные материалы по параллельному программированию
4. C++ Reference (https://en.cppreference.com) — документация по C++

## 10. Приложение

### 10.1 Структура данных

```cpp
struct TaskData {
  std::vector<double> matrix;
  std::vector<double> vector;
  int rows{0};
  int cols{0};
};
```

### 10.2 Последовательный алгоритм (основная логика)

```cpp
bool KaurAVertRibbonSchemeSEQ::RunImpl() {
  for (int j = 0; j < cols_; j++) {
    for (int i = 0; i < rows_; i++) {
      result_[i] += matrix_[static_cast<std::size_t>(j * rows_) + i] * vector_[j];
    }
  }
  return true;
}
```

### 10.3 Параллельный алгоритм (основная логика)

```cpp
bool KaurAVertRibbonSchemeMPI::RunImpl() {
  int rank = 0;
  int size = 0;
  MPI_Comm_rank(MPI_COMM_WORLD, &rank);
  MPI_Comm_size(MPI_COMM_WORLD, &size);

  MPI_Bcast(&rows_, 1, MPI_INT, 0, MPI_COMM_WORLD);
  MPI_Bcast(&cols_, 1, MPI_INT, 0, MPI_COMM_WORLD);

  int base_cols = cols_ / size;
  int extra_cols = cols_ % size;

  std::vector<int> send_counts(size);
  std::vector<int> displs(size);
  std::vector<int> vec_counts(size);
  std::vector<int> vec_displs(size);

  int offset = 0;
  int vec_offset = 0;
  for (int i = 0; i < size; i++) {
    int local_cols = base_cols + (i < extra_cols ? 1 : 0);
    send_counts[i] = local_cols * rows_;
    displs[i] = offset;
    vec_counts[i] = local_cols;
    vec_displs[i] = vec_offset;
    offset += send_counts[i];
    vec_offset += local_cols;
  }

  int local_cols = base_cols + (rank < extra_cols ? 1 : 0);
  std::vector<double> local_matrix(static_cast<std::size_t>(local_cols) * rows_);
  std::vector<double> local_vector(local_cols);

  MPI_Scatterv(matrix_.data(), send_counts.data(), displs.data(), MPI_DOUBLE, 
               local_matrix.data(), send_counts[rank], MPI_DOUBLE, 0, MPI_COMM_WORLD);

  MPI_Scatterv(vector_.data(), vec_counts.data(), vec_displs.data(), MPI_DOUBLE, 
               local_vector.data(), vec_counts[rank], MPI_DOUBLE, 0, MPI_COMM_WORLD);

  std::vector<double> local_result(rows_, 0.0);
  for (int j = 0; j < local_cols; j++) {
    for (int i = 0; i < rows_; i++) {
      local_result[i] += local_matrix[static_cast<std::size_t>(j * rows_) + i] * local_vector[j];
    }
  }

  result_.resize(rows_);
  MPI_Reduce(local_result.data(), result_.data(), rows_, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);

  MPI_Bcast(result_.data(), rows_, MPI_DOUBLE, 0, MPI_COMM_WORLD);

  return true;
}
```