# Виртуальная кольцевая топология (Ring Topology)

- Студент: Дилшодов Адхам Умидович, группа 3823Б1ПР4
- Технология: SEQ + MPI
- Вариант: Кольцевая топология

## 1. Введение

Задача реализации виртуальной кольцевой топологии является классической задачей
параллельного программирования. Необходимо обеспечить передачу данных от любого
выбранного процесса любому другому процессу, используя возможности MPI по работе
с коммуникаторами, но не используя `MPI_Cart_Create` и `MPI_Graph_Create`.

Цель работы — реализовать последовательную и параллельную версии алгоритма
передачи данных по кольцевой топологии, провести тестирование корректности
и производительности, проанализировать эффективность.

## 2. Постановка задачи

**Входные данные:** Структура `RingMessage`, содержащая номер процесса-источника,
номер процесса-назначения и вектор целых чисел для передачи.

**Выходные данные:** Вектор целых чисел — данные, доставленные получателю.

**Ограничения:**

- `source >= 0`, `dest >= 0`
- Данные непустые
- Не используются `MPI_Cart_Create` и `MPI_Graph_Create`

Тип входных данных:

```cpp
struct RingMessage {
  int source = 0;
  int dest = 0;
  std::vector<int> data;
};
using InType = RingMessage;
```

Тип выходных данных:

```cpp
using OutType = std::vector<int>;
```

## 3. Базовый алгоритм (последовательный)

В последовательной версии передача данных тривиальна — данные просто копируются
из входа на выход, поскольку в рамках одного процесса маршрутизация не требуется:

```cpp
bool RingTopologySEQ::RunImpl() {
  GetOutput() = GetInput().data;
  return true;
}
```

**Сложность:** O(N) — копирование вектора из N элементов.

## 4. Схема распараллеливания

Используется кольцевая маршрутизация: процессы образуют виртуальное кольцо,
данные передаются последовательно от источника к получателю по часовой стрелке
через промежуточные узлы.

### 4.1. Определение соседей в кольце

```cpp
int right = (rank + 1) % size;
int left = (rank - 1 + size) % size;
```

Для каждого процесса вычисляется правый и левый сосед в кольцевой топологии.
Это обеспечивает замкнутость кольца: последний процесс связан с первым.

### 4.2. Обработка случая source == dest

```cpp
if (source == dest) {
  if (rank == source) {
    GetOutput() = input.data;
  }
  auto data_size = static_cast<int>(input.data.size());
  MPI_Bcast(&data_size, 1, MPI_INT, source, MPI_COMM_WORLD);
  if (rank != source) {
    GetOutput().resize(static_cast<std::size_t>(data_size));
  }
  MPI_Bcast(GetOutput().data(), data_size, MPI_INT, source, MPI_COMM_WORLD);
  return true;
}
```

Когда источник и получатель совпадают, данные уже находятся на нужном процессе.
Для синхронизации всех процессов результат рассылается через `MPI_Bcast`.

### 4.3. Вычисление количества шагов и позиции

```cpp
int hops = (dest - source + size) % size;
int my_pos = (rank - source + size) % size;
```

Количество шагов `hops` определяет длину пути по кольцу от источника к получателю.
Позиция `my_pos` показывает расстояние текущего процесса от источника в направлении
передачи. Это позволяет каждому процессу определить свою роль: отправитель,
промежуточный узел или получатель.

### 4.4. Передача данных по кольцу

```cpp
if (my_pos == 0) {
  buffer = input.data;
  auto buf_size = static_cast<int>(buffer.size());
  MPI_Send(&buf_size, 1, MPI_INT, right, 0, MPI_COMM_WORLD);
  if (buf_size > 0) {
    MPI_Send(buffer.data(), buf_size, MPI_INT, right, 1, MPI_COMM_WORLD);
  }
} else if (my_pos > 0 && my_pos < hops) {
  int buf_size = 0;
  MPI_Recv(&buf_size, 1, MPI_INT, left, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
  buffer.resize(static_cast<std::size_t>(buf_size));
  if (buf_size > 0) {
    MPI_Recv(buffer.data(), buf_size, MPI_INT, left, 1, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
  }
  MPI_Send(&buf_size, 1, MPI_INT, right, 0, MPI_COMM_WORLD);
  if (buf_size > 0) {
    MPI_Send(buffer.data(), buf_size, MPI_INT, right, 1, MPI_COMM_WORLD);
  }
} else if (my_pos == hops) {
  int buf_size = 0;
  MPI_Recv(&buf_size, 1, MPI_INT, left, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
  buffer.resize(static_cast<std::size_t>(buf_size));
  if (buf_size > 0) {
    MPI_Recv(buffer.data(), buf_size, MPI_INT, left, 1, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
  }
}
```

Данные передаются поэтапно по кольцу. Сначала отправляется размер буфера,
затем сами данные. Каждый промежуточный процесс принимает данные от левого
соседа и пересылает правому.

### 4.5. Рассылка результата всем процессам

```cpp
auto result_size = static_cast<int>((rank == dest) ? GetOutput().size() : 0);
MPI_Bcast(&result_size, 1, MPI_INT, dest, MPI_COMM_WORLD);
if (rank != dest) {
  GetOutput().resize(static_cast<std::size_t>(result_size));
}
MPI_Bcast(GetOutput().data(), result_size, MPI_INT, dest, MPI_COMM_WORLD);
```

После доставки данных получателю результат рассылается всем процессам
через `MPI_Bcast` для обеспечения согласованности выходных данных.

## 5. Используемые функции MPI

- **MPI_Comm_rank** — получение номера текущего процесса
- **MPI_Comm_size** — получение общего количества процессов
- **MPI_Bcast** — широковещательная рассылка данных от одного процесса всем остальным
- **MPI_Send** — блокирующая отправка данных соседнему процессу в кольце
- **MPI_Recv** — блокирующий приём данных от соседнего процесса в кольце

## 6. Экспериментальная установка

| Параметр    | Значение                                              |
|-------------|-------------------------------------------------------|
| CPU         | AMD Ryzen 5 5600 (6 ядер / 12 потоков) @ 3.693 GHz    |
| RAM         | 32 ГБ                                                 |
| ОС          | Windows 11 + WSL2 (Ubuntu 24.04.2 LTS)                |
| Ядро        | 6.6.87.2-microsoft-standard-WSL2                      |
| Компилятор  | GCC (g++)                                             |
| MPI         | Open MPI                                              |
| Тип сборки  | Release                                               |

### Тестовые данные

- Функциональные тесты: вектор из 1, 5, 50, 100, 500, 1000 элементов
  (случайные значения от -1000 до 1000)
- Тест производительности: вектор из **1 000 000** элементов
  (случайные значения от -10000 до 10000),
  передача от процесса 0 к процессу `size - 1`

## 7. Результаты

### 7.1 Корректность

Все функциональные тесты проходят успешно при разных размерах данных
(5, 100, 1000 элементов) и разном числе процессов (1, 2, 4).

### 7.2 Производительность

Вектор из 1 000 000 int, передача от процесса 0 к процессу `size - 1`.

#### Общие результаты

| Режим | Процессы | Время, мс | Speedup (vs MPI-1)  | Efficiency |
|-------|----------|-----------|---------------------|------------|
| SEQ   | 1        | 0.19      | —                   | —          |
| MPI   | 1        | 0.14      | 1.00                | 100%       |
| MPI   | 2        | 2.61      | 0.05                | 2.7%       |
| MPI   | 4        | 5.68      | 0.02                | 0.6%       |
| MPI   | 6        | 9.65      | 0.01                | 0.2%       |

Время MPI растёт с числом процессов, так как количество хопов
по кольцу увеличивается: 0 хопов при NP=1, 1 при NP=2,
3 при NP=4, 5 при NP=6. Задача коммуникационная, а не
вычислительная — ускорения от добавления процессов не ожидается.

#### Детальное профилирование MPI-версии (MPI_Wtime)

Профилирование выполнено с помощью замеров `MPI_Wtime` на каждом этапе алгоритма.

**Pipeline (5 итераций):**

| Итерация               | Ring Send/Recv, мс | Final Bcast, мс | Total, мс |
|------------------------|--------------------|-----------------|-----------|
| 1 (cold)               | 6.50               | 16.64           | 23.14     |
| 2                      | 1.98               | 5.15            | 7.13      |
| 3                      | 1.91               | 3.44            | 5.34      |
| 4                      | 1.40               | 3.37            | 4.77      |
| 5                      | 1.56               | 4.03            | 5.59      |
| **Среднее (без cold)** | **1.71**           | **4.00**        | **5.71**  |

**Task Run (6 итераций):**

| Итерация               | Ring Send/Recv, мс | Final Bcast, мс | Total, мс |
|------------------------|--------------------|-----------------|-----------|
| 1 (cold)               | 1.69               | 10.13           | 11.81     |
| 2                      | 2.09               | 4.99            | 7.08      |
| 3                      | 2.02               | 2.77            | 4.79      |
| 4                      | 1.35               | 2.78            | 4.13      |
| 5                      | 1.90               | 2.77            | 4.67      |
| 6                      | 1.39               | 3.55            | 4.94      |
| **Среднее (без cold)** | **1.75**           | **3.37**        | **5.12**  |

#### Анализ профилирования

1. **Cold start**: Первая итерация значительно медленнее остальных (до 4× разницы).
   Это связано с инициализацией буферов MPI, заполнением кэшей
   и установкой соединений между процессами.

2. **Ring Send/Recv vs Bcast**: Передача данных по кольцу (Send/Recv)
   занимает ~1.7 мс, а финальный `MPI_Bcast` — ~3.4-4.0 мс.
   Bcast дороже, так как рассылает 1M int всем 4 процессам, тогда как
   при кольцевой передаче данные идут последовательно по одному каналу за раз.

3. **Соотношение этапов**: Ring Send/Recv составляет ~30-35% общего времени,
   а финальный Bcast — ~65-70%. Основное узкое место — именно рассылка
   результата всем процессам.

4. **SEQ vs MPI**: Последовательная версия (~0.4 мс) быстрее MPI (~5-9 мс),
   так как просто копирует вектор в памяти.

## 8. Выводы

В ходе работы реализованы последовательная и параллельная версии алгоритма
передачи данных по виртуальной кольцевой топологии. Обе версии успешно
проходят функциональные тесты.

Реализация не использует встроенные функции создания топологий
(`MPI_Cart_Create`, `MPI_Graph_Create`), а вручную организует маршрутизацию
данных по кольцу через непосредственных соседей каждого процесса.

Алгоритм обеспечивает корректную передачу данных от любого процесса-источника
к любому процессу-получателю, последовательно проходя через промежуточные
узлы кольца. Количество шагов передачи равно `(dest - source + size) % size`.

## 9. Источники

1. Сысоев А. В. Лекции курса «Параллельное программирование для кластерных систем»
2. Документация лабораторных работ — <https://learning-process.github.io/parallel_programming_course/ru/>

## Приложение

### Последовательная версия

```cpp
bool RingTopologySEQ::RunImpl() {
  GetOutput() = GetInput().data;
  return true;
}
```

### MPI версия (ключевой фрагмент)

```cpp
int hops = (dest - source + size) % size;
int my_pos = (rank - source + size) % size;

if (my_pos == 0) {
  // Источник: отправляем данные правому соседу
  buffer = input.data;
  MPI_Send(&buf_size, 1, MPI_INT, right, 0, MPI_COMM_WORLD);
  MPI_Send(buffer.data(), buf_size, MPI_INT, right, 1, MPI_COMM_WORLD);
} else if (my_pos > 0 && my_pos < hops) {
  // Промежуточный: принимаем от левого, передаём правому
  MPI_Recv(&buf_size, 1, MPI_INT, left, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
  MPI_Recv(buffer.data(), buf_size, MPI_INT, left, 1, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
  MPI_Send(&buf_size, 1, MPI_INT, right, 0, MPI_COMM_WORLD);
  MPI_Send(buffer.data(), buf_size, MPI_INT, right, 1, MPI_COMM_WORLD);
} else if (my_pos == hops) {
  // Получатель: принимаем данные
  MPI_Recv(&buf_size, 1, MPI_INT, left, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
  MPI_Recv(buffer.data(), buf_size, MPI_INT, left, 1, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
}

// Рассылка результата всем процессам
MPI_Bcast(&result_size, 1, MPI_INT, dest, MPI_COMM_WORLD);
MPI_Bcast(GetOutput().data(), result_size, MPI_INT, dest, MPI_COMM_WORLD);
```
