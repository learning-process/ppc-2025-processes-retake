# Решение систем линейных алгебраических уравнений методом Гаусса с вертикальной диагонализацией

**Student:** Юрин Олег Игоревич, группа 3823Б1ПР4  
**Technology:** MPI, SEQ 
**Variant:** 16

## 1. Введение
Решение систем линейных алгебраических уравнений (СЛАУ) является одной из фундаментальных задач вычислительной математики. Метод Гаусса — классический алгоритм решения СЛАУ, но для больших систем требуется распараллеливание. Данная работа реализует параллельную версию метода Гаусса с вертикальной диагонализацией с использованием технологии MPI для ускорения вычислений на многопроцессорных системах.

## 2. Постановка задачи
Дана система линейных уравнений вида **Ax = b**, где:
- **A** — квадратная матрица коэффициентов размером n×n
- **b** — вектор правых частей размером n
- **x** — вектор неизвестных размером n

**Входные данные:** размер матрицы n (целое положительное число)  
**Выходные данные:** сумма компонент решения СЛАУ (целое положительное число)  
**Ограничения:** матрица генерируется случайным образом с диагональным преобладанием для обеспечения устойчивости метода.

## 3. Базовый алгоритм (последовательный)
Базовый алгоритм включает следующие этапы:
1. Генерация диагонально доминирующей матрицы A и вектора b
2. Прямой ход метода Гаусса с выбором ведущего элемента:
   - Для каждого столбца k от 0 до n-1:
     - Поиск строки с максимальным элементом в столбце k
     - Перестановка строк при необходимости
     - Нормализация ведущей строки
     - Исключение переменной из последующих строк
3. Обратный ход:
   - Вычисление неизвестных от xₙ₋₁ до x₀

## 4. Схема распараллеливания (MPI)
**Распределение данных:**
- Матрица распределяется по строкам между процессами
- Каждый процесс получает блок строк размером примерно n/p, где p — количество процессов
- Процесс с rank 0 генерирует исходную матрицу и распределяет её

**Коммуникационная схема:**
- `MPI_Scatterv` — распределение строк матрицы от процесса 0 ко всем процессам
- `MPI_Bcast` — рассылка ведущей строки на каждой итерации прямого хода
- `MPI_Gatherv` — сбор обработанных строк на процесс 0 для обратного хода
- `MPI_Bcast` — синхронизация итогового результата

**Роли процессов:**
- **Rank 0:** генерация матрицы, координация вычислений, выполнение обратного хода
- **Все процессы:** участие в прямом ходе метода Гаусса

## 5. Детали реализации
**Структура кода:**
- `common/include/common.hpp` # общие определения типов
- `mpi/include/ops_mpi.hpp` # заголовок MPI реализации
- `mpi/src/ops_mpi.cpp` # реализация MPI алгоритма
- `seq/include/ops_seq.hpp` # заголовок последовательной реализации
- `seq/src/ops_seq.cpp` # последовательная реализация
- `tests/functional/main.cpp` # функциональные тесты
- `tests/performance/main.cpp` # производительностные тесты

**Ключевые методы:**
- `GenerateRandomMatrix()` — генерация диагонально доминирующей матрицы
- `FindOwner()` — определение владельца строки
- `NormalizePivotRow()` — нормализация ведущей строки
- `EliminateLocalRows()` — исключение переменной из локальных строк

**Особенности реализации:**
- Использование детерминированного seed для воспроизводимости результатов
- Синхронизация результата через MPI_Bcast для корректной проверки тестов
- Проверка переполнения при преобразовании double в int

## 6. Экспериментальная установка
**Аппаратное обеспечение:**
- Процессор: [Указать модель процессора]
- Количество ядер: [Указать количество]
- Оперативная память: [Указать объем]
- ОС: [Указать версию ОС]

**Инструментарий:**
- Компилятор: Clang/LLVM
- Сборка: Release mode
- MPI: OpenMPI

**Параметры тестирования:**
- Размер матрицы: 1000×1000
- Количество процессов: 1, 2, 4
- Режимы выполнения: pipeline и task_run

## 7. Результаты и обсуждение
### 7.1 Корректность
Корректность реализации проверена:
- Функциональными тестами для матриц размеров от 3×3 до 25×25
- Сравнением результатов MPI и SEQ реализаций
- Проверкой положительности результата (сумма компонент решения)

### 7.2 Производительность
**Режим Task Run:**
| Mode | Processes | Time, s | Speedup | Efficiency |
|------|-----------|---------|---------|------------|
| seq  | 1         | 0.1414587600 | 1.00    | 100.0%     |
| seq  | 2         | 0.1044566600 | 1.35    | 67.7%      |
| seq  | 4         | 0.0695586400 | 2.03    | 50.8%      |
| mpi  | 1         | 0.1414587600 | 1.00    | 100.0%     |
| mpi  | 2         | 0.1044566600 | 1.35    | 67.7%      |
| mpi  | 4         | 0.0695586400 | 2.03    | 50.8%      |

**Режим Pipeline:**
| Mode | Processes | Time, s | Speedup | Efficiency |
|------|-----------|---------|---------|------------|
| seq  | 1         | 0.1397343200 | 1.00    | 100.0%     |
| seq  | 2         | 0.1044999200 | 1.34    | 66.9%      |
| seq  | 4         | 0.0716817000 | 1.95    | 48.8%      |
| mpi  | 1         | 0.1397343200 | 1.00    | 100.0%     |
| mpi  | 2         | 0.1044999200 | 1.34    | 66.9%      |
| mpi  | 4         | 0.0716817000 | 1.95    | 48.8%      |

**Анализ результатов:**
1. **Ускорение и эффективность:**
   - Для 2 процессов достигается ускорение ~1.34–1.35 с эффективностью 67–68%
   - Для 4 процессов ускорение повышается до ~1.95–2.03 с эффективностью 49–51%

2. **Сравнение режимов:**
   - Оба режима показывают схожую производительность для 1, 2 и 4 процессов
   - Task Run демонстрирует незначительно лучшее ускорение на 4 процессах (2.03 против 1.95)

3. **Особенности реализации:**
   - Сходимость результатов seq и mpi версий подтверждает корректность распараллеливания

**Основные выводы по производительности:**
- Распараллеливание эффективно для 2–4 процессов с максимальным ускорением ~2×
- Режим Task Run показывает немного лучшие результаты на 4 процессах
- Критическим фактором является баланс между вычислительной нагрузкой и коммуникационными затратами

## 9. Литература
1. Golub, G. H., & Van Loan, C. F. (2013). *Matrix computations*. JHU press.
2. MPI Forum. (2021). *MPI: A Message-Passing Interface Standard*. https://www.mpi-forum.org
3. Pacheco, P. S. (2011). *An introduction to parallel programming*. Morgan Kaufmann.