1. Introduction
Сортировка является одной из фундаментальных задач программирования, широко применяемой в обработке данных, базах данных, научных вычислениях и многих других областях. При работе с большими объёмами данных последовательные алгоритмы сортировки становятся узким местом производительности. Параллельная реализация с использованием MPI позволяет значительно ускорить процесс за счёт распределения данных между несколькими процессами и одновременной сортировки частей массива с последующим слиянием. В данной работе реализована сортировка Шелла с последующим простым слиянием отсортированных блоков.

2. Problem Statement
Дан одномерный массив целых чисел произвольного размера nn. Необходимо отсортировать его по возрастанию, используя алгоритм Шелла. Результатом должен быть отсортированный массив того же размера.

Входные данные: std::vector<int> – исходный неотсортированный массив.
Выходные данные: std::vector<int> – отсортированный массив.

Ограничения:

    Размер массива n≥0 (пустой массив допустим).
    Элементы массива – целые числа.
    В параллельной версии массив распределяется между процессами, каждый процесс сортирует свою часть, затем результаты сливаются на корневом процессе.

3. Baseline Algorithm (Sequential)
В качестве базового алгоритма используется сортировка Шелла – улучшенная версия сортировки вставками. Основная идея заключается в сортировке элементов, находящихся на определённом расстоянии друг от друга (шаг или gap), с постепенным уменьшением этого расстояния до единицы.


Сложность алгоритма: В худшем случае O(n2)O(n2), на практике для выбранной последовательности шагов (деление на два) сложность близка к O(nlog⁡2n)O(nlog2n).

4. Parallelization Scheme
В параллельной версии используется подход «разделяй и властвуй» с распределением исходного массива между процессами, локальной сортировкой каждого блока и последующим слиянием отсортированных блоков на корневом процессе.
Распределение данных

    Исходный массив равномерно (с учётом остатка) распределяется между всеми процессами с помощью MPI_Scatterv.
    Каждый процесс получает непрерывный блок элементов (подмассив).
    Размер блока для процесса i определяется как base = n / p плюс один элемент для первых n % p процессов.

Вычисления

    Каждый процесс выполняет сортировку Шелла над своим локальным блоком.
    После завершения локальной сортировки блоки собираются на корневом процессе с помощью MPI_Gatherv.

Слияние

    Корневой процесс (ранг 0) выполняет простое многопутевое слияние всех отсортированных блоков, выбирая на каждом шаге минимальный элемент из голов каждого блока.
    Полученный полностью отсортированный массив сохраняется как результат.

Синхронизация и коммуникации

    Размер массива рассылается всем процессам через MPI_Bcast.
    Матрицы распределения (send_counts, displs) вычисляются на корне и рассылаются всем процессам.
    После слияния результат дополнительно рассылается всем процессам (для единообразия, чтобы на всех процессах GetOutput() содержал корректный результат).

Обработка пустого массива

    Если размер массива равен нулю, все процессы сразу завершаются с пустым результатом.

5. Implementation Details
Структура проекта
text

rysev_m_shell_sort_simple_merge/
├── common/
│   └── common.hpp          // Общие типы данных
├── seq/
│   ├── ops_seq.hpp         // Заголовок последовательной версии
│   └── ops_seq.cpp         // Реализация последовательной сортировки
├── mpi/
│   ├── ops_mpi.hpp         // Заголовок MPI-версии
│   └── ops_mpi.cpp         // Реализация параллельной сортировки
└── tests/
    ├── functional/         // Функциональные тесты
    └── performance/        // Тесты производительности

Ключевые классы и методы

    RysevShellSortSEQ – последовательная реализация:
        ValidationImpl() – проверка (всегда true, пустой массив допустим).
        RunImpl() – копирует входной массив, сортирует его методом Шелла.
    RysevMShellSortMPI – параллельная MPI-реализация:
        ValidationImpl() – синхронизированная проверка на всех процессах (пустой массив считается валидным).
        RunImpl() – распределение данных, локальная сортировка, сбор результатов, слияние на корне.
        ShellSort(std::vector<int>&) – вспомогательный метод, реализующий сортировку Шелла для локального блока.

Особые случаи

    Неравномерное распределение: если размер массива не кратен числу процессов, первые процессы получают на один элемент больше.
    Пустой массив: обрабатывается без вызова коммуникационных операций, возвращается пустой результат.
    Процессы с нулевым блоком: корректно обрабатываются благодаря использованию MPI_Scatterv/Gatherv с нулевыми размерами.

6. Experimental Setup
Hardware/OS

    Процессор: AMD Ryzen 5 3500U, 4 ядра, 8 потоков
    Тактовая частота: 2.10 GHz (базовая), до 3.7 GHz (Boost)
    Оперативная память: 8 GB DDR4
    Накопитель: 477 GB SSD
    Операционная система: Windows 10 Home 22H2 (сборка 19045.6456)

Software

    Компилятор: g++ (GCC) 14.2.0
    MPI реализация: OpenMPI (в Docker-контейнере)
    Сборка: Release (оптимизация -O2)
    Тестовые данные: Для оценки производительности использовался массив фиксированного размера (количество элементов уточняется в результатах).

Environment

    Запуск в Docker-контейнере для воспроизводимости результатов.
    Количество процессов: 2.
    Количество измерений: однократный запуск (согласно предоставленным данным).

7. Results and Discussion
7.1 Correctness

Корректность реализации подтверждена набором функциональных тестов:
    Случайные массивы различных размеров (10, 50, 100, 500, 1000).
    Крайние случаи: пустой массив, массив из одного элемента, уже отсортированный массив, массив, отсортированный в обратном порядке, массив с дубликатами.
    Для всех тестов результат параллельной версии совпадает с результатом последовательной сортировки и эталонной сортировкой std::sort.

7.2 Performance

Измерения производительности проводились для двух процессов (cpu_num = 2)
Время последовательной версии	0.0001749516 с
Время параллельной версии (2)	0.0002338096 с
Ускорение S=Tseq/TmpiS=Tseq​/Tmpi​	0.7483
Эффективность E=S/2E=S/2	0.3741

Наблюдения:

    Параллельная версия на двух процессах показала время больше, чем последовательная. Ускорение меньше единицы означает, что накладные расходы на коммуникации (рассылка данных, сбор результатов, слияние) превысили выигрыш от параллельных вычислений.

    Такая ситуация характерна для относительно небольших массивов, где объём вычислений недостаточен, чтобы окупить затраты на передачу данных и синхронизацию.

    Эффективность 37% подтверждает, что для данного размера входных данных использование параллельной версии нецелесообразно; оптимальная область применения MPI-сортировки начинается с существенно больших объёмов данных (например, миллионы элементов).

8. Conclusions

В ходе работы успешно реализована параллельная MPI-версия алгоритма сортировки Шелла с использованием схемы «разделяй, сортируй локально, сливай».

Основные достижения:

    Разработана масштабируемая схема распределения данных с учётом неравномерного деления.

    Обеспечена корректная обработка пустого массива и крайних случаев.

    Проведён анализ производительности, выявивший, что для небольших массивов параллельная версия уступает последовательной из-за накладных расходов.

Наблюдения:

    Для эффективного использования MPI требуется достаточно большой размер задачи, чтобы вычисления доминировали над коммуникациями.

    Слияние на одном процессе может стать узким местом при большом количестве процессов.

Перспективы улучшения:

    Использование параллельного слияния (например, на основе сети сортирующих сетей) для снижения нагрузки на корневой процесс.

    Применение неблокирующих операций для перекрытия вычислений и передачи данных.

    Оптимизация выбора шагов в сортировке Шелла для улучшения локальной производительности.

9. References

    Лекции и практики курса «Параллельное программирование».