# Метод Гаусса — ленточная вертикальная схема (MPI)

- Студент: Черемхин Андрей Александрович, группа 3823Б1ПР3
- Технология: SEQ | MPI
- Вариант: 16

## Введение

Цель работы — реализовать метод Гаусса для решения системы линейных уравнений Ax=b в двух вариантах:

- последовательный (SEQ)
- параллельный (MPI) с ленточной вертикальной схемой (распределение столбцов между процессами)

## Постановка задачи

Дано:

- квадратная матрица A размера n×n
- вектор b размера n

Требуется найти вектор решения x такой, что:

A * x = b

Тип входных данных:

```cpp
struct Input {
  int n;
  std::vector<double> a;  
  std::vector<double> b;  
};
using InType = Input;
```

Тип выходных данных:

```cpp
using OutType = std::vector<double>; 
```

## Базовый алгоритм (Sequential)

Используется прямой ход метода Гаусса с частичным выбором главного элемента (по модулю) по столбцу и обратный ход:

1. Для каждого шага k выбирается опорная строка `pivot_row` (максимум |A[r,k]| для r >= k).
2. Опорная строка меняется местами со строкой k.
3. Выполняется зануление элементов под диагональю в столбце k.
4. После прямого хода выполняется обратная подстановка.

Сложность:

- O(n^3) по времени
- O(n^2) по памяти

## Схема распараллеливания (MPI): ленточная вертикальная

Матрица A распределяется по столбцам: каждый процесс хранит непрерывный диапазон столбцов [start, end) для всех строк.

На шаге k:

1. Определяется процесс-владелец столбца k.
2. Владелец выбирает опорную строку `pivot_row` по значениям в столбце k и рассылает `pivot_row` всем (`MPI_Bcast`).
3. Все процессы переставляют строки k и `pivot_row` в своих локальных лентах; вектор b хранится везде и переставляется также.
4. Владелец столбца k вычисляет множители m_i = A[i,k] / A[k,k] для строк i > k и рассылает их всем (`MPI_Bcast`).
5. Каждый процесс обновляет только свои столбцы (с индексом столбца > k) по формуле:
   - A[i,j] = A[i,j] - m_i * A[k,j]
   - b[i] = b[i] - m_i * b[k]
6. Обратный ход:
   - частичные суммы sum(j>i) A[i,j] * x[j] считаются локально и суммируются через `MPI_Allreduce`
   - диагональный элемент A[i,i] берётся у владельца столбца i и рассылается через `MPI_Bcast`
   - x[i] вычисляется на всех процессах (вектор `x` реплицирован)

## Детали реализации

Структура задачи:

```
tasks/cheremkhin_a_gaus_vert/
├── common/include/common.hpp
├── seq/include/ops_seq.hpp
├── seq/src/ops_seq.cpp
├── mpi/include/ops_mpi.hpp
├── mpi/src/ops_mpi.cpp
├── tests/functional/main.cpp
├── tests/performance/main.cpp
├── settings.json
├── info.json
└── report.md
```

Классы:

- `CheremkhinAGausVertSEQ`
- `CheremkhinAGausVertMPI`

## Проверка корректности

Functional-тесты решают несколько систем с известным решением и сравнивают полученный `x` с эталоном с точностью `1e-8`:

- 1×1
- 2×2
- 3×3

Для MPI-версии тесты должны запускаться под `mpirun`.

## Производительность

Performance-тест формирует диагонально доминирующую систему размера n=1200 и проверяет корректность по невязке:

max_i |(A*x)[i] - b[i]| < 1e-5

Ниже приведены результаты одного прогона на локальной машине. Базовая точка для расчёта ускорения — SEQ (1 процесс).

pipeline:

| Mode | Count | Time, s | Speedup | Efficiency |
|------|------:|--------:|--------:|-----------:|
| seq  | 1 | 1.5233808180 | 1.00 | N/A |
| mpi  | 2 | 1.0194023110 | 1.49 | 74.7% |
| mpi  | 4 | 0.6980515950 | 2.18 | 54.6% |

task_run:

| Mode | Count | Time, s | Speedup | Efficiency |
|------|------:|--------:|--------:|-----------:|
| seq  | 1 | 1.5344248210 | 1.00 | N/A |
| mpi  | 2 | 1.0393405410 | 1.48 | 73.8% |
| mpi  | 4 | 0.7955681110 | 1.93 | 48.2% |

Запуск:

```bash
./build/bin/ppc_perf_tests --gtest_color=0 --gtest_filter='*cheremkhin_a_gaus_vert*'
ASAN_OPTIONS=detect_leaks=0 mpirun --allow-run-as-root -x ASAN_OPTIONS -np 2 ./build/bin/ppc_perf_tests --gtest_color=0 --gtest_filter='*_mpi_*'
```

## Источники

1. [Материалы курса: отчёт (требования и структура)](https://learning-process.github.io/parallel_programming_course/ru/common_information/report.html#overview-and-placement)

