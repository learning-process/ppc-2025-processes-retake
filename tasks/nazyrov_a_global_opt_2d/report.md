# Многошаговая глобальная оптимизация 2D

- Студент: Назыров Анвар, группа 3823Б1ПР4
- Технология: SEQ + MPI
- Вариант: 12

## 1. Введение

Задача глобальной оптимизации в двумерном пространстве
заключается в нахождении глобального минимума функции
двух переменных $f(x, y)$ на заданной прямоугольной области
$[x_{\min}, x_{\max}] \times [y_{\min}, y_{\max}]$.

Используется многошаговая схема на основе характеристик
интервалов с редукцией размерности через кривую Пеано,
которая отображает отрезок $[0,1]$ на единичный квадрат
$[0,1]^2$. Это позволяет свести двумерную задачу к
одномерной и применить алгоритм глобального поиска
Стронгина. Распараллеливание выполняется путём
распределения интервалов между MPI-процессами для
параллельного вычисления характеристик.

## 2. Постановка задачи

**Входные данные:** Структура `OptInput`:

- `func` — целевая функция $f(x, y)$
- `x_min`, `x_max`, `y_min`, `y_max` — границы области
- `epsilon` — точность (критерий останова по длине интервала)
- `r_param` — параметр надёжности ($r > 1$)
- `max_iterations` — максимальное число итераций

**Выходные данные:** Структура `OptResult`:

- `x_opt`, `y_opt` — координаты найденного оптимума
- `func_min` — значение функции в оптимуме
- `iterations` — число выполненных итераций
- `converged` — флаг сходимости

**Ограничения:**

- Функция не `nullptr`
- `x_min < x_max`, `y_min < y_max`
- `epsilon > 0`, `r_param > 1`, `max_iterations > 0`

## 3. Базовый алгоритм (последовательный)

Алгоритм основан на многошаговой схеме глобального поиска
Стронгина с использованием характеристик интервалов:

1. **Редукция размерности.** Кривая Пеано отображает
   параметр $t \in [0,1]$ в точку $(u_x, u_y) \in [0,1]^2$,
   которая масштабируется в область поиска.
2. **Инициализация.** Вычисляются значения функции в
   граничных точках $t=0$ и $t=1$.
3. **Итеративный цикл.** На каждой итерации:
   - Сортировка пробных точек по параметру $t$
   - Оценка константы Липшица $L$ по соседним точкам
   - Вычисление характеристик $R_i$ для каждого интервала:
     $$R_i = m \cdot \Delta t_i + \frac{(\Delta z_i)^2}{m \cdot \Delta t_i} - 2(z_i + z_{i-1})$$
     где $m = r \cdot L$
   - Выбор интервала с максимальной характеристикой
   - Вычисление новой пробной точки внутри интервала
4. **Критерий останова** — длина лучшего интервала < `epsilon`

## 4. Схема распараллеливания

### 4.1. Инициализация (`InitTrials`)

Rank 0 вычисляет начальные пробные точки ($t=0$, $t=1$)
и рассылает их всем процессам через `MPI_Bcast`.

### 4.2. Распределение и вычисление (`DistributeAndCompute`)

Интервалы между пробными точками упаковываются в массив
$[\, t_{\text{left}},\; t_{\text{right}},\; z_{\text{left}},\; z_{\text{right}} \,]$
и распределяются через `MPI_Scatterv`. Каждый процесс
вычисляет характеристики для своих интервалов. Результаты
собираются на rank 0 через `MPI_Gatherv`.

### 4.3. Выбор лучшего интервала (`SelectBestAndAddTrial`)

Rank 0 находит интервал с максимальной характеристикой,
проверяет критерий сходимости, вычисляет новую пробную
точку и добавляет её в список.

### 4.4. Синхронизация

Флаг сходимости и новая точка рассылаются через
`MPI_Bcast`. Остальные процессы вычисляют значение
функции в новой точке локально.

### 4.5. Финализация (`FindBestResult`)

Rank 0 находит точку с минимальным значением $f$ среди
всех пробных точек и рассылает результат через `MPI_Bcast`.

## 5. Используемые функции MPI

- **MPI_Comm_rank** — получение номера текущего процесса
- **MPI_Comm_size** — получение общего количества процессов
- **MPI_Bcast** — рассылка данных (начальные точки,
  флаг сходимости, новая пробная точка, результат)
- **MPI_Scatterv** — распределение упакованных интервалов
  между процессами
- **MPI_Gatherv** — сбор вычисленных характеристик
  на rank 0

## 6. Экспериментальная установка

| Параметр   | Значение         |
|------------|------------------|
| CPU        | —                |
| RAM        | —                |
| ОС         | —                |
| Компилятор | GCC (g++)        |
| MPI        | Open MPI         |
| Тип сборки | Release          |

## 7. Результаты

### 7.1 Корректность

Все 12 функциональных тестов (6 MPI + 6 SEQ) проходят
успешно при запуске на 4 процессах MPI. Тестируются
различные целевые функции: сфера ($x^2 + y^2$),
эллиптическая ($2x^2 + 3y^2$), квадратичная с
перекрёстным членом, $\sin + \cos$, Растригина 2D,
а также стандартное значение по умолчанию.

### 7.2 Производительность

Функция $f(x,y) = \sin(x^2) + \cos(y^2) + 0.1x^2 + 0.1y^2$
на $[-5, 5]^2$, 2000 итераций, $\varepsilon = 0.001$.
Время указано в секундах.

#### Pipeline

| NP  | SEQ, с     | MPI, с     | Speedup |
|-----|------------|------------|---------|
| 1   | 0.002899   | 0.002933   | 0.99    |
| 2   | 0.002958   | 0.005200   | 0.57    |
| 4   | 0.003197   | 0.006404   | 0.50    |
| 6   | 0.004407   | 0.019784   | 0.22    |

#### Task Run

| NP  | SEQ, с     | MPI, с     | Speedup |
|-----|------------|------------|---------|
| 1   | 0.000504   | 0.002991   | 0.17    |
| 2   | 0.000495   | 0.005512   | 0.09    |
| 4   | 0.000568   | 0.006236   | 0.09    |
| 6   | 0.000946   | 0.018251   | 0.05    |

Параллельная версия медленнее SEQ. Это объясняется
итеративной природой алгоритма: на каждой итерации
выполняется цепочка `MPI_Scatterv` → `MPI_Gatherv` →
`MPI_Bcast`, а объём вычислений в каждой итерации
минимален (оценка характеристики нескольких интервалов).
Накладные расходы на коммуникацию MPI доминируют над
полезными вычислениями. Ускорение возможно при большем
числе пробных точек или более дорогой целевой функции.

## 8. Выводы

Реализованы последовательная и параллельная версии
алгоритма многошаговой глобальной оптимизации 2D.
Редукция размерности выполняется через кривую Пеано,
а поиск — методом характеристик интервалов Стронгина.
MPI-версия разделена на вспомогательные методы:
`InitTrials`, `DistributeAndCompute`,
`SelectBestAndAddTrial`, `FindBestResult`.
Все 12 функциональных тестов проходят при запуске на
4 процессах. Параллельная версия корректно распределяет
интервалы между процессами, однако не даёт ускорения
из-за высоких коммуникационных затрат на каждой итерации
при малом объёме вычислений.

## 9. Источники

1. Сысоев А. В. Лекции курса
   «Параллельное программирование для кластерных систем»
2. Документация лабораторных работ —
   <https://learning-process.github.io/parallel_programming_course/ru/>

## Приложение

### Кривая Пеано (отображение [0,1] → [0,1]²)

```cpp
void GlobalOpt2dSEQ::PeanoMap(double t_val, int level,
                               double &ux, double &uy) {
  ux = 0.0;
  uy = 0.0;
  double scale = 0.5;
  for (int i = 0; i < level; ++i) {
    int quad = static_cast<int>(t_val * 4.0);
    t_val = (t_val * 4.0) - quad;
    double dx = 0.0;
    double dy = 0.0;
    switch (quad) {
      case 0: break;
      case 1: dy = 1.0; break;
      case 2: dx = 1.0; dy = 1.0; break;
      case 3: dx = 1.0; break;
      default: break;
    }
    ux += dx * scale;
    uy += dy * scale;
    scale *= 0.5;
  }
}
```

### Основной цикл оптимизации (SEQ)

```cpp
for (int iter = 0; iter < input.max_iterations; ++iter) {
  SortTrials();
  lip_est_ = ComputeLipschitz();
  if (lip_est_ < 1e-10) lip_est_ = 1.0;

  int best_idx = FindBestInterval();
  double t_left = t_points_[best_idx];
  double t_right = t_points_[best_idx + 1];

  if (t_right - t_left < input.epsilon) {
    output.converged = true;
    break;
  }

  double m_val = input.r_param * lip_est_;
  double t_new = 0.5 * (t_left + t_right)
               - (z_right - z_left) / (2.0 * m_val);
  double xn = ToX(t_new);
  double yn = ToY(t_new);
  t_points_.push_back(t_new);
  trials_.push_back({xn, yn, input.func(xn, yn)});
}
```

### Главный цикл MPI-версии (RunImpl)

```cpp
InitTrials();
for (int iter = 0; iter < input.max_iterations; ++iter) {
  SortTrials();
  lip_est_ = ComputeLipschitz();
  double m_val = input.r_param * lip_est_;

  std::vector<double> all_chars;
  DistributeAndCompute(m_val, all_chars);

  std::array<double, 3> new_trial = {0.0, 0.0, 0.0};
  int converge_flag = 0;
  if (world_rank_ == 0)
    new_trial = SelectBestAndAddTrial(all_chars, m_val,
                                      converge_flag);

  MPI_Bcast(&converge_flag, 1, MPI_INT, 0, MPI_COMM_WORLD);
  if (converge_flag != 0) { output.converged = true; break; }

  MPI_Bcast(new_trial.data(), 3, MPI_DOUBLE, 0,
            MPI_COMM_WORLD);
  if (world_rank_ != 0) {
    t_points_.push_back(new_trial[0]);
    trials_.push_back({new_trial[1], new_trial[2],
                       input.func(new_trial[1], new_trial[2])});
  }
}
FindBestResult();
```

### MPI: распределение интервалов (DistributeAndCompute)

```cpp
void GlobalOpt2dMPI::DistributeAndCompute(
    double m_val, std::vector<double> &all_chars) {
  int n = static_cast<int>(t_points_.size()) - 1;
  // Упаковка: [t_left, t_right, z_left, z_right]
  std::vector<double> packed(n * 4);
  for (int i = 0; i < n; ++i) { /* ... */ }

  std::vector<int> counts, displs;
  ComputeDistribution(n, world_size_, counts, displs);

  MPI_Scatterv(packed.data(), send_counts.data(), ...);

  // Локальное вычисление характеристик
  for (int i = 0; i < local_count; ++i) {
    double dt = ...; double dz = ...;
    local_chars[i] = m * dt + dz² / (m * dt)
                   - 2 * (z_right + z_left);
  }

  MPI_Gatherv(local_chars.data(), ...);
}
```
