# Линейная фильтрация изображений (вертикальное разбиение). Ядро Гаусса 3×3

- Студент: Красавин Артем Павлович, группа 3823Б1ПР5
- Технология: MPI + SEQ
- Задача: 3 (вариант 27)

## 1. Введение

Фильтрация изображений – одна из базовых операций при обработке визуальной информации. Сглаживание с помощью гауссова ядра широко применяется для подавления шумов и удаления высокочастотных составляющих. В данной работе рассматривается реализация фильтрации изображения с ядром Гаусса 3×3.

Основная цель – создание последовательной (SEQ) и параллельной (MPI) версий алгоритма, использующих **вертикальную схему декомпозиции** (разбиение по столбцам), и анализ эффективности параллельной реализации.

## 2. Постановка задачи

**Входные данные:** изображение размера \(W \times H\) с числом каналов \(C\), хранящееся в **одномерном массиве** типа `uint8_t` с порядком row-major:

`data[(y * width + x) * channels + c]`

**Выходные данные:** результирующее изображение тех же размеров и с тем же числом каналов после применения свёртки с гауссовым ядром 3×3.

**Ядро фильтра:**

\[
K = \frac{1}{16}
\begin{pmatrix}
1 & 2 & 1\\
2 & 4 & 2\\
1 & 2 & 1
\end{pmatrix}
\]

Выходное значение пикселя \((x, y)\) для канала \(c\) вычисляется по формуле:

\[
out(x, y, c) = \sum_{dy=-1}^{1}\sum_{dx=-1}^{1} K(dx, dy)\cdot in(x+dx, y+dy, c)
\]

## 3. Базовый алгоритм (последовательная версия)

Последовательный алгоритм выполняет свёртку в окрестности 3×3 для каждого пикселя изображения по каждому каналу.

**Обработка границ:** используется стратегия **clamp-to-edge** – координаты, выходящие за пределы изображения, заменяются координатами ближайшего краевого пикселя.

**Вычисления и округление:** промежуточная сумма накапливается в целочисленной переменной (`int`), после чего применяется округление \((sum + 8) / 16\) и ограничение результата диапазоном \([0, 255]\).

**Вычислительная сложность:**

- По времени: \(O(W \cdot H \cdot C)\)
- По памяти: \(O(W \cdot H \cdot C)\) для хранения выходного изображения

## 4. Схема параллелизации

### 4.1 Распределение данных (вертикальные полосы)

Изображение разбивается на вертикальные полосы (по столбцам) между MPI-процессами:

- `base_cols = width / size`
- `rem_cols = width % size`
- процессы с рангом `rank < rem_cols` получают на один столбец больше остальных

Каждый процесс хранит свою локальную полосу шириной `local_w`.

### 4.2 Локальные вычисления

Каждый процесс производит свёртку только для пикселей, принадлежащих его полосе. Для корректного применения ядра 3×3 по горизонтали необходимы соседние столбцы слева и справа (halo-область).

### 4.3 Коммуникации

**Используемые операции MPI:**

1. Рассылка исходных данных от процесса 0 всем остальным. Поскольку вертикальная полоса не является непрерывной в исходном row-major массиве, на процессе 0 выполняется **упаковка** данных полосы в непрерывный буфер (строка за строкой внутри полосы) перед отправкой.
2. Обмен halo-столбцами между соседними процессами с помощью `MPI_Sendrecv`.
3. Сбор локальных результатов на процессе 0 через `MPI_Gatherv`.
4. Рассылка итогового изображения всем процессам командой `MPI_Bcast` (для единообразия выходных данных на всех рангах).

### 4.4 Роли процессов

- **Процесс 0:** отвечает за распределение исходных полос, сборку финального изображения и его широковещательную рассылку.
- **Процессы 1…n-1:** получают свою полосу, обмениваются граничными столбцами, выполняют локальную фильтрацию и участвуют в сборе результатов.

## 5. Детали реализации

### 5.1 Структура кода

Проект организован следующим образом:

- `common/include/common.hpp` – общие типы (`Image`, `InType`, `OutType`)
- `seq/src/ops_seq.cpp` – последовательная реализация (`KrasavinAParallelGaussian3x3VerticalSEQ`)
- `mpi/src/ops_mpi.cpp` – MPI-реализация (`KrasavinAParallelGaussian3x3VerticalMPI`)
- `tests/functional/main.cpp` – функциональные тесты
- `tests/performance/main.cpp` – тесты производительности

### 5.2 Классы задач

Обе версии наследуют базовый класс `ppc::task::Task<InType, OutType>` и переопределяют виртуальные методы:

- `ValidationImpl()`
- `PreProcessingImpl()`
- `RunImpl()`
- `PostProcessingImpl()`

### 5.3 Распределение данных (вертикальные полосы)

Изображение логически делится на вертикальные полосы. На процессе 0 для каждой полосы формируется непрерывный буфер, который затем пересылается соответствующему процессу при помощи `MPI_Send`/`MPI_Recv`. Полученный буфер интерпретируется как локальное изображение (row-major порядок внутри полосы).

### 5.4 Вычисления и агрегация

- Каждый процесс выполняет свёртку только над пикселями своей полосы.
- Перед вычислением крайних столбцов полосы производится обмен halo-данными с соседями (один левый и один правый столбец).
- После завершения локальной фильтрации процесс 0 собирает все части с помощью `MPI_Gatherv` и формирует полное выходное изображение.
- Финальное изображение рассылается всем процессам командой `MPI_Bcast`.

### 5.5 Использование памяти

- **SEQ:** хранит входное и выходное изображения целиком – \(O(W \cdot H \cdot C)\)
- **MPI:**  
  - процесс 0: хранит полные входные и выходные данные (для распределения и сборки)  
  - остальные процессы: хранят только свою полосу плюс два дополнительных столбца (halo)

## 6. Настройка эксперимента

### 6.1 Аппаратное окружение

- **CPU:** Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz
- **Количество ядер:** 14
- **ОЗУ:** 32 GiB
- **ОС:** Linux (WSL2)

### 6.2 Программное окружение

- **Компилятор:** g++ 13.3.0
- **MPI:** Open MPI 4.1.6
- **Система сборки:** CMake
- **Тестовый фреймворк:** Google Test

### 6.3 Параметры окружения

- `PPC_NUM_PROC` – количество MPI-процессов
- `PPC_NUM_THREADS` – количество потоков (не используется, но требуется окружением)
- `OMPI_ALLOW_RUN_AS_ROOT=1`, `OMPI_ALLOW_RUN_AS_ROOT_CONFIRM=1` – для запуска OpenMPI в контейнере (root)

### 6.4 Тестовые данные

- **Функциональные тесты:** небольшие синтетические изображения (grayscale и RGB)
- **Тесты производительности:** изображение 4096×4096, 1 канал (grayscale), заполнение по формуле  
  `img[row, col] = (row + col) % 256`

### 6.5 Команды запуска

**Пример для performance-тестов:**

```bash
export OMPI_ALLOW_RUN_AS_ROOT=1
export OMPI_ALLOW_RUN_AS_ROOT_CONFIRM=1
export PPC_NUM_THREADS=1
mpirun --allow-run-as-root --oversubscribe -np 4 ./build/bin/ppc_perf_tests --gtest_filter='*krasavin_a_parallel_gaussian_3x3_vertical_mpi_enabled*'
./build/bin/ppc_perf_tests --gtest_filter='*krasavin_a_parallel_gaussian_3x3_vertical_seq_enabled*'
```

## 7\. Результаты и обсуждение

### 7.1 Корректность

Правильность работы проверялась сравнением результатов последовательной и параллельной реализаций с эталонной свёрткой на наборе синтетических изображений (grayscale и RGB). Все функциональные тесты успешно пройдены.

### 7.2 Производительность

За базовое время взято время выполнения последовательной версии:  
T<sub>seq</sub> = 0.1625783920 с.  
Измерялось полное время выполнения конвейера (pipeline).

| Режим | Процессы | Время, с | Ускорение | Эффективность |
| --- | --- | --- | --- | --- |
| seq | 1 | 0.1625783920 | 1.00 | N/A |
| mpi | 1 | 0.2850076972 | 0.57 | 57.0% |
| mpi | 2 | 0.2254145984 | 0.72 | 36.1% |
| mpi | 4 | 0.1965714584 | 0.83 | 20.7% |
| mpi | 8 | 0.2093870330 | 0.78 | 9.7% |

Метрики:

-   Ускорение (Speedup): S(p)\=Tseq/Tmpi(p)S(p)\=Tseq​/Tmpi​(p)
    
-   Эффективность (Efficiency): E(p)\=S(p)/p×100%E(p)\=S(p)/p×100%
    

### 7.3 Анализ результатов

Для изображения 4096×4096 вычислительная нагрузка достаточно велика, однако с ростом числа процессов эффективность параллельной версии снижается из-за накладных расходов на коммуникации (упаковка/распаковка полос, обмен halo-столбцами, сборка результатов). Наилучшее время достигнуто при использовании 4 процессов; при увеличении до 8 процессов затраты на обмен данными начинают преобладать над выигрышем от распараллеливания, что приводит к падению производительности.

## 8\. Выводы

В ходе работы были реализованы последовательная и параллельная (MPI) версии алгоритма линейной фильтрации изображения гауссовым ядром 3×3. В параллельной версии применено вертикальное разбиение изображения с обменом граничных столбцов (halo). Экспериментальные измерения показали, что максимальное ускорение на используемом стенде достигается при 4 процессах; дальнейшее увеличение числа процессов неэффективно из-за роста коммуникационных издержек.