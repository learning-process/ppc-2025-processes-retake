# Реализация виртуальной топологии «Гиперкуб»

- Student: Царьков Клим Александрович, группа 3823Б1ПР4
- Technology: MPI + SEQ
- Variant: 10

## 1. Introduction

Топологии сетей передачи данных играют ключевую роль в построении
параллельных алгоритмов и распределённых вычислений.

Одной из фундаментальных топологий является гиперкуб (n-dimensional hypercube).
Гиперкуб обладает логарифмическим диаметром сети и хорошими свойствами
масштабируемости.

Цель работы — реализовать виртуальную топологию гиперкуба средствами MPI
(без использования MPI_Cart_Create и MPI_Graph_Create) и обеспечить
передачу данных от произвольного процесса-источника к произвольному
процессу-получателю.

## 2. Problem Statement

Дано:

- множество процессов MPI (число процессов — степень двойки),
- номер процесса-источника `source`,
- номер процесса-получателя `destination`,
- размер передаваемых данных `data_size`.

Необходимо:

- реализовать виртуальную топологию гиперкуба,
- передать данные от `source` к `destination`,
- вернуть размер успешно переданных данных.

### Входные данные

Вектор из трёх целых чисел:

```cpp
[source_rank, destination_rank, data_size]
```

### Выходные данные

Целое число — размер успешно доставленных данных.

### Ограничения

- Число процессов должно быть степенью двойки.
- Использование MPI_Cart_Create и MPI_Graph_Create запрещено.
- Коммуникации реализуются вручную через MPI_Comm_split и MPI_Sendrecv.

## 3. Baseline Algorithm (Sequential)

Последовательная версия не использует реальную передачу данных.

Алгоритм:

1. Проверяется корректность входных данных.
2. Возвращается `data_size`.

Последовательная версия служит эталоном корректности параллельной реализации.

## 4. Parallelization Scheme

### 4.1 Представление гиперкуба

Если число процессов равно 2^d, то размерность гиперкуба равна:

```cpp
d = log2(world_size)
```

Каждый процесс идентифицируется своим рангом,
который интерпретируется как бинарный вектор длины d.

### 4.2 Схема маршрутизации

Передача осуществляется по измерениям гиперкуба.

Для каждого измерения i:

```cpp
bit_mask = 1 << i;
partner  = rank ^ bit_mask;
```

Если соответствующий бит ранга отличается от бита процесса `destination`,
данные пересылаются соседу по текущему измерению.

Таким образом, за d шагов данные достигают целевого процесса.

### 4.3 Формирование коммуникаторов

Для каждого измерения создаётся подкоммуникатор:

```cpp
color = rank & ~bit_mask;
MPI_Comm_split(MPI_COMM_WORLD, color, rank, &dim_comm);
```

В каждом таком коммуникаторе находятся ровно два процесса —
соседи по соответствующему измерению.

### 4.4 Передача данных

Передача выполняется в два этапа:

1. Обмен размером буфера через MPI_Sendrecv.
2. Обмен самими данными через MPI_Sendrecv.

После прохождения всех измерений:

- процесс `destination` получает данные,
- выполняется MPI_Bcast размера доставленных данных,
  чтобы все процессы имели одинаковый результат.

Используемая топология — виртуальный гиперкуб,
реализованный поверх MPI_COMM_WORLD.

## 5. Implementation Details

### Структура проекта

```text
tasks/tsarkov_k_hypercube/
  common/include/common.hpp
  seq/include/ops_seq.hpp
  seq/src/ops_seq.cpp
  mpi/include/ops_mpi.hpp
  mpi/src/ops_mpi.cpp
  tests/
  report.md
```

### Классы

- TsarkovKHypercubeSEQ
- TsarkovKHypercubeMPI

### Особенности реализации

- Проверяется, что число процессов — степень двойки.
- Размерность гиперкуба вычисляется динамически.
- Для имитации измерений используется MPI_Comm_split.
- Для передачи данных используется MPI_Sendrecv.
- Тип данных — std::int32_t.
- Используется симметричный обмен без блокирующих цепочек.

Память:

- Буфер создаётся только у процесса-источника.
- В процессе маршрутизации буфер передаётся и освобождается.

## 6. Experimental Setup

### Аппаратное обеспечение

- CPU: Intel Core i5-1135G7 (4 ядра / 8 потоков)
- RAM: 8 GB
- ОС: Windows 10

### Инструменты

- Компилятор: MSVC (Visual Studio 2022)
- MPI: MS-MPI
- Тип сборки: Release

### Параметры запуска

- 1 процесс (SEQ)
- 2 процесса (MPI)
- 4 процесса (MPI)

Размер данных для тестирования:

```cpp
data_size = 400
```

## 7. Results and Discussion

### 7.1 Correctness

Проведены функциональные тесты:

- передача между удалёнными процессами,
- передача к самому себе,
- передача в обратном направлении,
- граничные случаи.

Результаты:

- SEQ (1 процесс) — успешно
- MPI (2 процесса) — успешно
- MPI (4 процесса) — успешно

Все тесты пройдены, результаты SEQ и MPI совпадают.

### 7.2 Performance

#### Pipeline mode

| Mode | Count | Time, s        | Speedup  | Efficiency |
|------|-------|----------------|----------|------------|
| seq  | 1     | 0.0000001000   | 1.00000  | 100.00%    |
| mpi  | 1     | 0.0000067201   | 0.01488  | 1.49%      |
| mpi  | 2     | 0.0003673000   | 0.00027  | 0.0136%    |
| mpi  | 4     | 0.0014866000   | 0.00007  | 0.00168%   |

#### Task_run mode

| Mode | Count | Time, s        | Speedup | Efficiency |
|------|-------|----------------|---------|------------|
| seq  | 1     | 0.0000000400   | 1.00000 | 100.00%    |
| mpi  | 1     | 0.0000006599   | 0.06061 | 6.06%      |
| mpi  | 2     | 0.0000510000   | 0.00078 | 0.0392%    |
| mpi  | 4     | 0.0000565600   | 0.00071 | 0.0177%    |

### Analysis

Полученные результаты демонстрируют отсутствие ускорения при увеличении числа процессов.

Основные причины:

- крайне малый объём данных (`data_size = 400`);
- доминирование коммуникационных накладных расходов;
- создание подкоммуникаторов (`MPI_Comm_split`) на каждом измерении;
- двукратный обмен (размер + данные) на каждом шаге маршрутизации.

Алгоритм имеет теоретическую сложность `O(log P)`,
однако при малых объёмах данных коммуникационные издержки существенно превышают вычислительную нагрузку.

Таким образом, задача не является вычислительно интенсивной,
что приводит к падению эффективности при масштабировании.

## 8. Conclusions

В работе реализована виртуальная топология гиперкуба
без использования встроенных средств создания топологий MPI.

Алгоритм корректно маршрутизирует данные
от любого процесса к любому другому.

Реализация соответствует требованиям задания,
использует только базовые средства MPI
и успешно проходит тесты PPC.

Коммуникационные накладные расходы доминируют
при малых объёмах данных,
что ограничивает масштабируемость решения.

## 9. References

1. MPI Standard Documentation — <https://www.mpi-forum.org/docs/>
2. Документация Microsoft MPI — <https://learn.microsoft.com/>
3. Учебные материалы курса PPC
