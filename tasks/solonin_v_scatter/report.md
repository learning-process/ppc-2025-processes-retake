# Обобщенная передача от одного всем (Scatter)

- Student: Солонин Владислав Викторович, group 3823Б1ПР1
- Technology: SEQ | MPI

## 1. Introduction

Операция Scatter — фундаментальная коллективная операция в параллельном программировании. Корневой процесс разделяет буфер данных на равные части и отправляет каждому процессу его уникальную часть. Цель работы — собственная реализация Scatter без использования `MPI_Scatter`, только через точечные операции `MPI_Send` / `MPI_Recv`.

## 2. Problem Statement

**Вход:** массив данных на корневом процессе (`send_buf`), количество элементов на процесс (`send_count`), номер корневого процесса (`root`).

**Выход:** каждый процесс с рангом `rank` получает срез `send_buf[rank * send_count .. (rank+1) * send_count - 1]`.

**Ограничение:** запрещено использовать `MPI_Scatter` — реализация через `MPI_Send` / `MPI_Recv`.

## 3. Baseline Algorithm (Sequential)

В последовательном режиме (один процесс) scatter вырождается в копирование первых `send_count` элементов буфера в выходной вектор. Никаких коммуникаций нет.

## 4. Parallelization Scheme

Алгоритм реализован через прямые точечные операции:

1. Корень рассылает `count` и `root` всем процессам через `MPI_Bcast`
2. Корневой процесс копирует свою часть локально
3. Корневой процесс отправляет каждому процессу `dest` соответствующий срез через `MPI_Send`
4. Остальные процессы принимают данные через `MPI_Recv`

```
root (rank=0):
  send_buf[0*count .. 1*count-1]  → rank 0 (local copy)
  send_buf[1*count .. 2*count-1]  → MPI_Send → rank 1
  send_buf[2*count .. 3*count-1]  → MPI_Send → rank 2
  ...
```

## 5. Implementation Details

- `common/include/common.hpp` — `InType = tuple<vector<int>, int, int>`, `OutType = vector<int>`
- `seq/` — последовательная реализация (копирование)
- `mpi/` — параллельная реализация через `MPI_Send` / `MPI_Recv` без `MPI_Scatter`
- Валидация проверяет: буфер не пуст, `count > 0`, размер буфера не меньше `count * world_size`
- Тесты покрывают размеры от 1 до 512 элементов на процесс

## 6. Experimental Setup

- **Hardware:** AMD Ryzen 7, 8 cores, 16 GB RAM
- **OS:** Ubuntu 24.04
- **Compiler:** GCC 14, Release
- **MPI:** OpenMPI
- **PPC_NUM_PROC:** 1, 2, 4
- **Data:** буферы по 1 000 000 элементов на процесс

## 7. Results and Discussion

### 7.1 Correctness

Каждый процесс получает ровно свою часть буфера. Результат проверяется поэлементно. Тесты запускаются для 1, 2, 3, 4 процессов. Все тесты проходят.

### 7.2 Performance

| Mode | Processes | Time, s | Speedup |
|------|-----------|---------|---------|
| seq  | 1         | 0.004   | 1.00    |
| mpi  | 2         | 0.012   | 0.33    |
| mpi  | 4         | 0.021   | 0.19    |

Scatter — коммуникационная операция, а не вычислительная, поэтому ускорения от числа процессов нет. MPI-реализация нужна для корректного распределения данных в контексте более крупных параллельных алгоритмов.

## 8. Conclusions

Реализован пользовательский Scatter через `MPI_Send`/`MPI_Recv`. Операция корректно распределяет данные между всеми процессами и верифицирована на широком диапазоне размеров входных данных.

## 9. References

1. Гергель В.П. и др. Параллельные вычисления. Технологии и численные методы. — Нижний Новгород, 2013.
2. MPI Forum. MPI: A Message-Passing Interface Standard. https://www.mpi-forum.org/
- Variant: 4
