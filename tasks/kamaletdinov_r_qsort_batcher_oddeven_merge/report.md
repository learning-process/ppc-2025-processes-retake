#Быстрая сортировка с четно - нечетным слиянием Бэтчера

## Информация об авторе
- **Студент:** Камалетдинов Рамзан Рамилевич, группа 3823Б1ПР4  
- **Технологии:** SEQ | MPI  
- **Вариант:** 15  

## 1. Введение
Задача заключается в сортировке входного массива целых чисел по возрастанию с использованием гибридного алгоритма, который объединяет быструю сортировку и чётно-нечётное слияние Бэтчера. Реализованы как последовательная, так и параллельная версии алгоритма, и проведено сравнение их производительности.

## 2. Постановка задачи
На вход подаётся массив целых чисел размером N. Требуется отсортировать массив по возрастанию с использованием алгоритма, сочетающего быструю сортировку и чётно-нечётное слияние Бэтчера. В результате должен быть получен отсортированный по возрастанию вектор длиной N.

## 3. Базовый алгоритм (последовательная версия)
Последовательная версия алгоритма реализована на основе нерекурсивной быстрой сортировки с использованием стека для хранения границ подмассивов. Основные шаги алгоритма:

1. Инициализация стека с диапазоном всего массива `[0, N-1]`
2. Пока стек не пуст:
   - Извлекается текущий диапазон `[left, right]`
   - Если `left >= right`, переход к следующей итерации
   - Выбирается опорный элемент — центральный элемент подмассива
   - Выполняется разделение подмассива на две части: элементы меньше опорного сдвигаются влево, больше — вправо
   - Границы полученных подмассивов помещаются в стек для дальнейшей обработки
3. После обработки всех диапазонов массив полностью отсортирован

**Примечание:** В последовательной версии чётно-нечётное слияние Бэтчера не используется, так как оно эффективно только в параллельных вычислениях.

## 4. Схема распараллеливания
Параллельная версия алгоритма сочетает локальную быструю сортировку с чётно-нечётным слиянием Бэтчера для объединения данных между процессами.

### Распределение данных:
1. Процесс с рангом 0 рассылает размер глобального массива `global_size` всем процессам с помощью `MPI_Bcast`
2. Вычисляются размеры блоков для каждого процесса:
   - Базовый размер: `block_size = global_size / size`
   - Остаток: `remainder = global_size % size`
   - Первые `remainder` процессов получают `block_size + 1` элементов
3. Данные распределяются с помощью `MPI_Scatterv`

### Локальная сортировка:
Каждый процесс сортирует свой локальный массив с помощью итеративной быстрой сортировки (функция `IterativeQuickSort`)

### Чётно-нечётные фазы Бэтчера:
После локальной сортировки выполняются фазы обмена между процессами:
1. Вычисляется количество фаз: `phase_count = (global_size + min_block - 1) / min_block`
2. Для каждой фазы:
   - Определяется направление обмена:
     - В чётных фазах: чётные процессы обмениваются с правым соседом, нечётные — с левым
     - В нечётных фазах: нечётные процессы обмениваются с правым соседом, чётные — с левым
   - Процессы обмениваются данными с соседями с помощью `NeighborExchange`
   - После обмена выполняется слияние полученных данных с локальными, сохраняя либо меньшую, либо большую часть в зависимости от позиции процесса

### Сбор результатов:
1. Процессы собирают данные на процессе 0 с помощью `MPI_Gatherv`
2. Процесс 0 рассылает итоговый отсортированный массив всем процессам через `MPI_Bcast`

## 5. Детали реализации

### Основные файлы:
- `ops_seq.cpp`, `ops_seq.hpp` — последовательная реализация
- `ops_mpi.cpp`, `ops_mpi.hpp` — параллельная реализация с использованием MPI
- `common.hpp` — определения общих типов данных
- `main.cpp` — функциональные и производительные тесты

### Основные классы:
- `KamaletdinovQuicksortWithBatcherEvenOddMergeSEQ` — последовательная версия
- `KamaletdinovQuicksortWithBatcherEvenOddMergeMPI` — параллельная версия

### Ключевые методы:
- `PartitionRange` / `PartitionBlock` — разделение массива относительно опорного элемента
- `IterativeQuickSort` — нерекурсивная быстрая сортировка
- `NeighborExchange` — обмен данными между соседними процессами
- `BatcherPhases` — управление фазами чётно-нечётного слияния Бэтчера
- `BroadcastOutputToAllRanks` — рассылка результатов всем процессам

## 6. Экспериментальная установка
- **Аппаратное обеспечение/ОС:** CPU - Intel Core i5-11400F, 6 ядер/12 потоков;
RAM - 16 Gb;
ОС - Windows 10 - **Инструментарий : **MinGW - w64(g++ 7.3.0, x86_64 - posix - seh),
    тип сборки : Release - **Окружение : **PPC_NUM_PROC -
                 **Данные
    : **тестовые наборы включают массивы различных размеров и характеристик

      ##7. Результаты и обсуждение

      ## #7.1 Корректность Функциональные тесты проверяют корректность алгоритма в различных сценариях
    : -Массивы разного размера(от 0 до 20 элементов) -
                 Упорядоченные и неупорядоченные последовательности - Повторяющиеся элементы - Отрицательные числа -
                 Крайние случаи(пустой массив, один элемент)

                     Все тесты проходят успешно,
    что подтверждает корректность реализации.

        ## #7.2 Производительность Тестирование проводилось на массивах разного размера при использовании 2 и
        4 процессов :

    ####Тесты на 2 процессах :

        | Количество элементов | Время SEQ(с) | Время MPI(с) | Ускорение | Эффективность | |
        -- -- -- -- -- -- -- -- -- -- --| -- -- -- -- -- -- -- -| -- -- -- -- -- -- -- -| -- -- -- -- -- -|
        -- -- -- -- -- -- -- -| | 100 000 | 0.0000 | 0.0034 | 0.00 | N / A | | 500 000 | 0.0111 | 0.0135 | 0.82 |
        41.0 % | | 5 000 000 | 0.2807 | 0.1423 | 1.97 | 98.5 % | | 20 000 000 | 1.1978 | 0.5986 | 2.00 | 100.0 % | |
        70 000 000 | 4.0944 | 2.1309 | 1.92 | 96.0 % |

        ####Тесты на 4 процессах :

    |
    Количество элементов | Время SEQ(с) | Время MPI(с) | Ускорение | Эффективность | | -- -- -- -- -- -- -- -- -- -- --|
    -- -- -- -- -- -- -- -| -- -- -- -- -- -- -- -| -- -- -- -- -- -| -- -- -- -- -- -- -- -| | 100 000 | 0.0000 |
    0.0031 | 0.00 | N / A | | 500 000 | 0.0111 | 0.0120 | 0.93 | 23.3 % | | 5 000 000 | 0.2807 | 0.1138 | 2.47 |
    61.8 % | | 20 000 000 | 1.1978 | 0.4806 | 2.49 | 62.3 % | | 70 000 000 | 4.0944 | 1.6557 | 2.47 | 61.8 % |

    ####Анализ результатов : 1. * *Малые размеры данных(≤500 000)
    : **Параллельная версия уступает последовательной из
        - за накладных расходов на инициализацию MPI и межпроцессное взаимодействие 2. *
              *Средние и большие размеры данных(≥5 000 000)
    : **Параллельная версия демонстрирует значительное ускорение по сравнению с последовательной
      3. * *Эффективность : **На 2 процессах достигается высокая эффективность(близкая к 100 %),
    на 4 процессах эффективность снижается до ~60 - 65 % из -
        за увеличения накладных расходов на коммуникацию

        ##8. Выводы 1. Реализованы последовательная и параллельная версии гибридного алгоритма сортировки,
    сочетающего быструю сортировку и чётно - нечётное слияние Бэтчера
        2. Параллельная версия эффективна для больших объемов данных,
    где выигрыш от распараллеливания превышает накладные расходы на коммуникацию 3. Чётно -
        нечётное слияние Бэтчера успешно применяется для объединения локально отсортированных блоков между процессами
        4. На малых объемах данных последовательная версия предпочтительнее из
        -
        за отсутствия накладных расходов на межпроцессное взаимодействие

        ##9. Ссылки 1. Лекции по параллельному программированию
        2. Практические занятия по параллельному программированию 3. Batcher,
    K.E.(1968)."Sorting networks and their applications".Proceedings of the April 30 - May 2,
    1968, spring joint computer conference
