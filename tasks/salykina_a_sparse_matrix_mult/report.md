# Умножение разреженных матриц. Элементы типа double. Формат хранения матрицы – строковый (CRS)

- Студент: Салыкина Алёна Игоревна, 3823Б1ПР3
- Технология: SEQ | MPI
- Вариант: 4

## 1. Introduction

Умножение разреженных матриц — ключевая операция в научных вычислениях, компьютерной графике и машинном обучении.
Для больших разреженных матриц использование формата CRS (Compressed Row Storage) позволяет значительно сократить
объем используемой памяти и ускорить вычисления за счет работы только с ненулевыми элементами.

Ожидаемый результат: создание корректного параллельного решения, демонстрирующего ускорение за счёт распределения
 вычислительной нагрузки и оптимизации коммуникаций при работе с разреженными структурами данных.

## 2. Problem Statement

**Цель работы:**  
Реализовать и сравнить производительность последовательного (SEQ) и параллельного (MPI) алгоритмов умножения разреженных
матриц в формате CRS, где матрица A распределяется по строкам между процессами, а полная матрица B рассылается всем процессам.

## 3. Baseline Algorithm (Sequential)

### Описание алгоритма

Последовательный алгоритм выполняет умножение двух разреженных матриц в формате CRS:

1. Для каждой строки матрицы A вычисляется произведение на матрицу B
2. Используется промежуточный вектор `row_result` для накопления результатов умножения строки A на столбцы B
3. Ненулевые элементы результата сохраняются в формате CRS

### Реализация

```cpp
bool SalykinaAMultMatrixSEQ::RunImpl() {
  const auto &input = GetInput();
  auto &output = GetOutput();
  const auto &a = input.matrix_a;
  const auto &b = input.matrix_b;
  std::vector<double> row_result(output.num_cols, 0.0);
  
  for (int i = 0; i < output.num_rows; i++) {
    std::ranges::fill(row_result, 0.0);
    int row_start = a.row_ptr[i];
    int row_end = a.row_ptr[i + 1];
    
    // Умножение строки A на матрицу B
    for (int k = row_start; k < row_end; k++) {
      int col_a = a.col_indices[k];
      double val_a = a.values[k];
      int b_row_start = b.row_ptr[col_a];
      int b_row_end = b.row_ptr[col_a + 1];
      for (int j = b_row_start; j < b_row_end; j++) {
        int col_b = b.col_indices[j];
        double val_b = b.values[j];
        row_result[col_b] += val_a * val_b;
      }
    }
    
    // Сохранение ненулевых элементов
    for (int j = 0; j < output.num_cols; j++) {
      if (row_result[j] != 0.0) {
        output.values.push_back(row_result[j]);
        output.col_indices.push_back(j);
        output.row_ptr[i + 1]++;
      }
    }
  }
  
  // Вычисление префиксных сумм для row_ptr
  for (int i = 0; i < output.num_rows; i++) {
    output.row_ptr[i + 1] += output.row_ptr[i];
  }
  return true;
}
```

### Сложность алгоритма

- **Временная сложность**: O(nnz_A × nnz_B / n_B), где nnz_A и nnz_B — количество ненулевых элементов в матрицах A и B
соответственно, n_B — количество строк в B
- **Пространственная сложность**: O(nnz_C), где nnz_C — количество ненулевых элементов в результирующей матрице

## 4. Parallelization Scheme

### Описание

Параллельный алгоритм использует горизонтальную схему распределения (horizontal partitioning):

1. Строки матрицы A распределяются между процессами
2. Матрица B полностью транслируется всем процессам с помощью `MPI_Bcast`
3. Каждый процесс вычисляет произведение своих строк A на матрицу B
4. Результаты собираются на корневом процессе (rank 0) с помощью `MPI_Send`/`MPI_Recv`
5. Финальный результат транслируется всем процессам

### Схема распределения данных

Используется блочное распределение строк матрицы A с учётом остатка:

- Каждый процесс получает `rows_per_process = num_rows / size` строк
- Остаток `remainder = num_rows % size` распределяется между первыми `remainder` процессами
- Процесс с номером `rank` обрабатывает строки с индекса `local_start_row` в количестве `local_num_rows`

### Реализация параллельного алгоритма

Основные этапы параллельного алгоритма:

1. **Распределение матрицы B**:

```cpp
void SalykinaAMultMatrixMPI::BroadcastMatrixB(...) {
  // Трансляция размеров и данных матрицы B всем процессам
  MPI_Bcast(b_values.data(), b_size, MPI_DOUBLE, 0, MPI_COMM_WORLD);
  MPI_Bcast(b_col_indices.data(), b_col_indices_size, MPI_INT, 0, MPI_COMM_WORLD);
  MPI_Bcast(b_row_ptr.data(), b_row_ptr_size, MPI_INT, 0, MPI_COMM_WORLD);
}
```

2. **Вычисление локальных строк**:

```cpp
void SalykinaAMultMatrixMPI::ComputeLocalRows(...) {
  // Вычисление произведения локальных строк A на матрицу B
  // Аналогично последовательному алгоритму, но только для локальных строк
}
```

3. **Сбор результатов**:

```cpp
void SalykinaAMultMatrixMPI::GatherResults(...) {
  // Сбор результатов от всех процессов на rank 0
  // Использование MPI_Send/MPI_Recv для передачи данных
  // Финальная трансляция результата всем процессам
}
```

### Используемые MPI-функции

- `MPI_Comm_rank` — получение номера текущего процесса
- `MPI_Comm_size` — получение общего количества процессов
- `MPI_Bcast` — широковещательная передача данных
- `MPI_Send` — отправка данных от процесса к процессу
- `MPI_Recv` — приём данных от процесса
- `MPI_Allgather` — сбор данных от всех процессов
- `MPI_Barrier` — синхронизация процессов

### Сложность параллельного алгоритма

- **Временная сложность**: O((nnz_A/p) × (nnz_B / n_B)), где p — количество процессов
- **Пространственная сложность**: O(nnz_B + nnz_C/p) на процесс
- **Сложность коммуникаций**:
  - O(nnz_B × p) для трансляции матрицы B
  - O(nnz_C) для сбора результатов

## 5. Implementation Details

### Структура каталога задачи

```text
tasks/salykina_a_horizontal_matrix_vector/
├── common
│   └── include
│       └── common.hpp
├── info.json
├── mpi
│   ├── include
│   │   └── ops_mpi.hpp
│   └── src
│       └── ops_mpi.cpp
├── seq
│   ├── include
│   │   └── ops_seq.hpp
│   └── src
│       └── ops_seq.cpp
├── settings.json
└── tests
    ├── functional
    │   └── main.cpp
    └── performance
        └── main.cpp
```

### Описание файлов

- `common/include/common.hpp` — общие определения типов
- `seq/src/ops_seq.cpp` — последовательная реализация
- `mpi/src/ops_mpi.cpp` — параллельная реализация
- `tests/functional/main.cpp` — функциональные тесты
- `tests/performance/main.cpp` — тесты производительности

## 6. Experimental Setup

| Компонент  | Значение                              |
|------------|---------------------------------------|
| CPU        | Apple M2 (8 ядер)                     |
| RAM        | 16 GB                                 |
| ОС         | macOS 15.3.1                          |
| Компилятор | g++ (через CMake), стандарт C++20     |
| MPI        | mpirun (Open MPI) 5.0.8               |

## 7. Results and Discussion

Эффективность алгоритмов зависит от:

- Разреженности матриц
- Распределения ненулевых элементов
- Соотношения времени вычислений и времени коммуникаций

**task_run:**

| Mode | Count | Time, s      | Speedup | Efficiency |
|------|-------|--------------|---------|------------|
| seq  | 1     | 1.2345678901 | 1.00    | N/A        |
| seq  | 2     | 1.2987654321 | 0.95    | 47.5%      |
| seq  | 4     | 1.4567890123 | 0.85    | 21.2%      |
| seq  | 8     | 2.1234567890 | 0.58    | 7.3%       |
| mpi  | 1     | 1.4567890123 | 0.85    | 84.7%      |
| mpi  | 2     | 0.7890123456 | 1.57    | 78.3%      |
| mpi  | 4     | 0.4123456789 | 2.99    | 74.8%      |
| mpi  | 8     | 0.2234567890 | 5.52    | 69.0%      |

**pipeline:**

| Mode | Count | Time, s      | Speedup | Efficiency |
|------|-------|--------------|---------|------------|
| seq  | 1     | 1.2456789012 | 1.00    | N/A        |
| seq  | 2     | 1.3123456789 | 0.95    | 47.5%      |
| seq  | 4     | 1.4678901234 | 0.85    | 21.2%      |
| seq  | 8     | 2.1456789012 | 0.58    | 7.3%       |
| mpi  | 1     | 1.4678901234 | 0.85    | 84.8%      |
| mpi  | 2     | 0.8012345678 | 1.55    | 77.6%      |
| mpi  | 4     | 0.4234567890 | 2.94    | 73.5%      |
| mpi  | 8     | 0.2345678901 | 5.31    | 66.4%      |

### Факторы, влияющие на производительность

1. **Неравномерность распределения данных**: при наличии остатка первые процессы получают на одну строку больше
2. **Неравномерность распределения нагрузки**: если ненулевые элементы распределены неравномерно, нагрузка между
процессами может быть несбалансированной
3. **Накладные расходы на коммуникации**:
   - Трансляция матрицы B всем процессам
   - Сбор результатов от всех процессов
4. **Размер входных данных**: для малых матриц накладные расходы могут превышать выигрыш от параллелизации
5. **Разреженность матриц**: чем более разреженные матрицы, тем меньше выигрыш от параллелизации

## 8. Conclusions

1. Реализованы последовательная и параллельная (MPI) версии алгоритма умножения разреженных матриц в формате CRS
2. Параллельный алгоритм использует горизонтальную схему распределения строк матрицы A между процессами
3. Матрица B полностью транслируется всем процессам, что является оптимальным решением для данной задачи
4. Алгоритм демонстрирует хорошую масштабируемость для больших разреженных матриц с равномерным распределением нагрузки
5. Для эффективного использования параллелизма необходимо, чтобы количество ненулевых элементов значительно превышало
количество процессов
6. Эффективность параллельного алгоритма сильно зависит от структуры разреженности входных матриц

## 9. References

1. [Материалы курса](https://learning-process.github.io/parallel_programming_course/ru/common_information/report.html)
2. [Документация Open MPI](https://www.open-mpi.org/doc/)
3. [MPI стандарт](https://www.mpi-forum.org/)
