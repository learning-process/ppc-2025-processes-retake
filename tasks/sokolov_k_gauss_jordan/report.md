# Метод Гаусса-Жордана (решение СЛАУ)

- Студент: Соколов Кирилл Денисович, группа 3823Б1ПР4
- Технология: SEQ, MPI
- Вариант: 16

## 1. Введение

Решение систем линейных алгебраических уравнений (СЛАУ) относится к базовым задачам вычислительной математики. Метод Гаусса-Жордана приводит расширенную матрицу системы к приведенному ступенчатому виду, после чего решение считывается из последнего столбца. Цель работы: реализовать последовательный и параллельный (MPI) варианты метода, провести замеры производительности и оценить ускорение и эффективность при разном числе процессов.

## 2. Постановка задачи

Дана система линейных уравнений Ax = b с квадратной матрицей A размера n x n и вектором правых частей b. Требуется найти вектор x.

- Вход: размер системы n (целое, n > 0). Матрица и вектор b формируются внутри программы детерминированно по n для воспроизводимости тестов.
- Выход: признак успешного решения; при успехе вектор x проверяется на совпадение с эталонным решением (x[i] = i + 1).
- Ограничения: матрица строится диагонально доминантной, чтобы метод был устойчив без выбора главного элемента по столбцу в параллельной версии.

## 3. Базовый (последовательный) алгоритм

Используется метод Гаусса-Жордана с частичным выбором главного элемента по столбцу.

1. Расширенная матрица [A | b] хранится по строкам в одномерном массиве размера n * (n+1).
2. Для каждого столбца k = 0, 1, ..., n-1:
   - Выбор главного элемента: в столбце k среди строк с номерами k, k+1, ..., n-1 ищется строка с максимальным по модулю элементом; эта строка меняется местами со строкой k.
   - Нормировка строки k: деление строки k на диагональный элемент a_kk.
   - Исключение столбца k во всех остальных строках: для каждой строки i != k выполняется вычитание из строки i строки k, умноженной на a_ik, так что в столбце k получаются нули.
3. После n шагов в первых n столбцах получается единичная матрица, решение находится в (n+1)-м столбце.

Выбор главного элемента повышает устойчивость к ошибкам округления. Сложность последовательного алгоритма O(n^3).

## 4. Схема распараллеливания (MPI)

Используется горизонтальная схема разбиения: матрица делится между процессами по строкам. Каждый процесс хранит свой блок строк расширенной матрицы.

- Распределение данных: процесс с рангом 0 генерирует полную матрицу [A | b], затем с помощью MPI_Scatterv распределяет строки по процессам (каждому процессу передается свое количество строк и смещение). Число строк на процесс вычисляется так, чтобы разница между максимальным и минимальным количеством не превышала 1.
- Роли процессов: для каждого шага k определяется процесс-владелец строки k (тот, в чей блок попадает строка k). Этот процесс нормирует свою строку k и рассылает ее всем процессам через MPI_Bcast. Все процессы выполняют исключение столбца k в своих локальных строках.
- Сбор результата: после всех шагов процесс 0 собирает обновленные блоки строк с остальных процессов с помощью MPI_Send (неранговые процессы отправляют свои блоки рангу 0). Ранг 0 формирует полную матрицу и извлекает вектор решения из последнего столбца.
- Топология: линейная (COMM_WORLD); коллективные операции Bcast и Scatterv, точечные Send/Recv при сборе.

## 5. Детали реализации

- Структура кода: последовательная версия в seq/src/ops_seq.cpp (класс SokolovKGaussJordanSEQ), параллельная в mpi/src/ops_mpi.cpp (SokolovKGaussJordanMPI). Общие типы и базовый класс в common/include/common.hpp. Генерация системы вынесена в анонимные функции GenerateSystem в обоих файлах.
- Ключевые функции SEQ: GenerateSystem (построение диагонально доминантной матрицы и правой части), GaussJordanStep (один шаг метода с выбором главного элемента и исключением).
- Ключевые функции MPI: ComputeDistribution (подсчет строк и смещений по процессам), FindRowOwner (определение владельца строки k), EliminationStep (нормировка строки владельцем, Bcast, исключение в локальных строках), CollectResults (сбор блоков на ранге 0 через MPI_Send/Recv).
- Угловые случаи: n <= 0 обрабатывается в ValidationImpl и PreProcessingImpl; при большом числе процессов часть процессов может получить 0 строк, циклы и обмены с нулевым размером учтены.
- Память: на ранге 0 хранится полная матрица до и после сбора; на остальных рангах только локальный блок строк.

## 6. Экспериментальная установка

- **CPU: Intel core i5-10400f** 
- **Ядра/Потоки: 6/12** 
- **ОС: Windows 10** 
- **Компилятор:** MSVC 14.44
- **Тип сборки:** Release
- **MPI реализация:** MS-MPI 10.0
- **CMake:** 4.2.0-rc1
- **Tестирование**: Google Test
- **Данные**: тестовая система генерируется по размеру n детерминированным генератором (линейный конгруэнтный генератор с фиксированным зерном от n); эталонное решение x[i] = i + 1

## 7. Результаты и обсуждение

### 7.1 Проверка корректности

Корректность проверяется функциональными тестами: для заданного n строится система с известным решением x[i] = i + 1, после решения вычисленное решение сравнивается с эталоном с допуском 1e-6. Тесты прогоняются для последовательной и MPI-реализаций на разных n (включая малые и большие размеры). Успешное прохождение CI (включая функциональные и статический анализ) подтверждает корректность реализации.

### 7.2 Производительность

Режим task_run: замеряется время многократного выполнения только этапа Run (без повторной генерации данных и постобработки). Режим task_pipeline: замеряется время полного прохода Validation, PreProcessing, Run, PostProcessing.

Таблица task_run:

| Процессов | Время, с | Ускорение | Эффективность |
|-----------|----------|-----------|---------------|
| 1 (seq)   | 1.1558   | 1.00      | -             |
| 2         | 1.1481   | 1.01      | 50.3%         |
| 4         | 0.6078   | 1.90      | 47.6%         |
| 6         | 0.4532   | 2.55      | 42.5%         |
| 8         | 0.4673   | 2.48      | 30.9%         |

Таблица task_pipeline:

| Процессов | Время, с | Ускорение | Эффективность |
|-----------|----------|-----------|---------------|
| 1 (seq)   | 1.1623   | 1.00      | -             |
| 2         | 1.1570   | 1.00      | 50.2%         |
| 4         | 0.6145   | 1.89      | 47.3%         |
| 6         | 0.4965   | 2.34      | 39.0%         |
| 8         | 0.4747   | 2.45      | 30.6%         |

Ускорение вычислено относительно последовательного варианта (1 процесс); эффективность - как (Ускорение / число процессов) * 100%.

Основные факторы, ограничивающие масштабируемость: накладные расходы MPI (Bcast на каждом из n шагов, Scatterv в начале и сбор в конце), дисбаланс нагрузки при малом n и большом числе процессов.

## 8. Выводы

Реализованы последовательный и параллельный (MPI) варианты метода Гаусса-Жордана с горизонтальным разбиением матрицы по строкам. Последовательный вариант использует частичный выбор главного элемента; параллельный распределяет строки через MPI_Scatterv и собирает результат через MPI_Send. Замеры на 2, 4, 6 и 8 процессах позволяют оценить ускорение и эффективность для режимов task_run и task_pipeline. Ожидаемо, task_run дает лучшее ускорение за счет отсутствия повторной генерации данных. Рост накладных расходов при увеличении числа процессов ограничивает эффективность при фиксированном размере задачи.

## 9. Источники

1. Бахвалов Н.С., Жидков Н.П., Кобельков Г.М. Численные методы. М.: БИНОМ. Лаборатория знаний, 2015.
2. Антонов А.С. Параллельное программирование с использованием технологии MPI. М.: Изд-во МГУ, 2004.
3. Message Passing Interface Forum. MPI: A Message-Passing Interface Standard. Version 4.0. 2021. https://www.mpi-forum.org/docs/
4. Gropp W., Lusk E., Skjellum A. Using MPI: Portable Parallel Programming with the Message Passing Interface. 3rd ed. MIT Press, 2014.
5. Калиткин Н.Н. Численные методы. М.: Наука, 1978.

## Приложение (фрагменты кода)

Последовательный шаг метода (нормировка строки k и исключение столбца k):

```cpp
void GaussJordanStep(std::vector<double> &matrix, int n, int cols, int k) {
  int max_row = k;
  double max_val = std::abs(matrix[(k * cols) + k]);
  for (int i = k + 1; i < n; i++) {
    double val = std::abs(matrix[(i * cols) + k]);
    if (val > max_val) { max_val = val; max_row = i; }
  }
  if (max_row != k) { /* обмен строк k и max_row */ }
  double pivot = matrix[(k * cols) + k];
  for (int j = k; j < cols; j++) matrix[(k * cols) + j] /= pivot;
  for (int i = 0; i < n; i++)
    if (i != k) {
      double factor = matrix[(i * cols) + k];
      for (int j = k; j < cols; j++)
        matrix[(i * cols) + j] -= factor * matrix[(k * cols) + j];
    }
}
```

Параллельный шаг: владелец строки k нормирует ее и рассылает, все процессы исключают столбец k в своих строках:

```cpp
MPI_Bcast(pivot_row.data(), cols, MPI_DOUBLE, owner, MPI_COMM_WORLD);
for (int i = 0; i < local_rows; i++) {
  int gi = local_start + i;
  if (gi != k) {
    double factor = local_data[(i * cols) + k];
    for (int j = 0; j < cols; j++)
      local_data[(i * cols) + j] -= factor * pivot_row[j];
  }
}
```
