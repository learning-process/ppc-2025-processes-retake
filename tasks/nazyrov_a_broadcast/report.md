# Передача от одного всем (Broadcast)

- Студент: Назыров Анвар, группа 3823Б1ПР4
- Технология: SEQ + MPI
- Вариант: 1

## 1. Введение

Задача реализации широковещательной рассылки (broadcast) является
классической задачей параллельного программирования. Необходимо
реализовать broadcast с использованием только `MPI_Send` и
`MPI_Recv`, применяя древовидную схему передачи для эффективности.

Цель работы — реализовать последовательную и параллельную версии
алгоритма broadcast, провести тестирование корректности и
производительности.

## 2. Постановка задачи

**Входные данные:** Структура `BcastInput` с номером
корневого процесса и вектором данных для рассылки.

**Выходные данные:** Вектор данных, полученный на каждом процессе.

**Ограничения:**

- `root >= 0` и `root < size`
- Данные непустые
- Не используется `MPI_Bcast`
- Передача через «дерево» процессов

Тип входных данных:

```cpp
struct BcastInput {
  int root = 0;
  std::vector<int> data;
};
using InType = BcastInput;
```

Тип выходных данных:

```cpp
using OutType = std::vector<int>;
```

## 3. Базовый алгоритм (последовательный)

В последовательной версии данные просто копируются из входа
на выход, так как рассылка тривиальна для одного процесса:

```cpp
bool BroadcastSEQ::RunImpl() {
  GetOutput() = GetInput().data;
  return true;
}
```

**Сложность:** O(N) — копирование вектора из N элементов.

## 4. Схема распараллеливания

### 4.1. Древовидная рассылка (Tree Broadcast)

Используется бинарное дерево. На каждом шаге процессы,
уже имеющие данные, отправляют их дочерним процессам.

Процесс с относительным номером `relative = (rank - root + size) % size`
на шаге с маской `mask` отправляет данные процессу `relative + mask`,
если тот существует.

### 4.2. Обработка произвольного root

Относительные номера процессов вычисляются с учётом root:
`relative = (rank - root + size) % size`, абсолютный номер
восстанавливается как `(relative + root) % size`.

## 5. Используемые функции MPI

- **MPI_Comm_rank** — получение номера текущего процесса
- **MPI_Comm_size** — получение общего количества процессов
- **MPI_Send** — блокирующая отправка данных дочернему процессу
- **MPI_Recv** — блокирующий приём данных от родительского процесса

## 6. Экспериментальная установка

| Параметр   | Значение         |
|------------|------------------|
| CPU        | —                |
| RAM        | —                |
| ОС         | —                |
| Компилятор | GCC (g++)        |
| MPI        | Open MPI         |
| Тип сборки | Release          |

## 7. Результаты

### 7.1 Корректность

Все функциональные тесты проходят успешно при разных
размерах данных, разных root и разном числе процессов.

### 7.2 Производительность

Вектор из 1 000 000 int, root = 0. Время указано в секундах.

#### Pipeline

| NP  | SEQ, с     | MPI, с     |
|-----|------------|------------|
| 1   | 0.000279   | 0.001501   |
| 2   | 0.000246   | 0.002945   |
| 4   | 0.000353   | 0.006416   |
| 6   | 0.000926   | 0.006453   |

#### Task Run

| NP  | SEQ, с     | MPI, с     |
|-----|------------|------------|
| 1   | 0.000215   | 0.001133   |
| 2   | 0.000180   | 0.002011   |
| 4   | 0.000767   | 0.008008   |
| 6   | 0.000997   | 0.005430   |

MPI-версия медленнее SEQ, что ожидаемо: broadcast —
коммуникационная задача, а не вычислительная. Цель
параллельной реализации — корректность древовидной
рассылки через `MPI_Send`/`MPI_Recv`, а не ускорение
относительно копирования в памяти.

Глубина дерева — O(log₂ NP), что лучше линейной рассылки
(O(NP)). При NP=6 время MPI практически не растёт
по сравнению с NP=4.

## 8. Выводы

Реализованы последовательная и параллельная версии алгоритма
broadcast с использованием только `MPI_Send`/`MPI_Recv` и
древовидной схемы передачи. Все 12 функциональных тестов
проходят при запуске на 4 процессах MPI (включая тесты
с произвольным root). Древовидная схема обеспечивает
логарифмическую глубину пересылки O(log₂ NP).

## 9. Источники

1. Сысоев А. В. Лекции курса
   «Параллельное программирование для кластерных систем»
2. Документация лабораторных работ —
   <https://learning-process.github.io/parallel_programming_course/ru/>

## Приложение

### Последовательная версия

```cpp
bool BroadcastSEQ::RunImpl() {
  GetOutput() = GetInput().data;
  return true;
}
```

### MPI-версия: TreeBcast (ключевой фрагмент)

```cpp
void TreeBcast(void *buffer, int count, MPI_Datatype datatype,
               int root, MPI_Comm comm) {
  int rank, size;
  MPI_Comm_rank(comm, &rank);
  MPI_Comm_size(comm, &size);
  if (size <= 1) return;

  int relative = (rank - root + size) % size;

  int mask = 1;
  while (mask < size) {
    if ((relative & mask) != 0) {
      int parent = relative - mask;
      int src = (parent + root) % size;
      MPI_Recv(buffer, count, datatype, src, 0, comm,
               MPI_STATUS_IGNORE);
      break;
    }
    mask <<= 1;
  }

  mask >>= 1;
  while (mask > 0) {
    int child = relative + mask;
    if (child < size) {
      int dest = (child + root) % size;
      MPI_Send(buffer, count, datatype, dest, 0, comm);
    }
    mask >>= 1;
  }
}
```
