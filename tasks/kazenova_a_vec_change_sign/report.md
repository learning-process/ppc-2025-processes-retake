# <Task Нахождение числа чередований знаков значений соседних элементов вектора>

- Student: <Казеннова Анастасия Михайловна>, group <3823Б1ФИ2>
- Technology: <SEQ | MPI>
- Variant: <5>

## 1. Introduction
При обработке больших числовых последовательностей в вычислениях и анализе данных возникает необходимость в эффективном подсчете статистических характеристик. Задача подсчета изменений знака между соседними элементами вектора является типичной операцией предварительной обработки сигналов. Параллельная реализация с использованием MPI позволяет значительно ускорить обработку больших объемов данных.

## 2. Problem Statement
Для заданного вектора целых чисел V = [v₁, v₂, ..., vₙ] необходимо найти количество индексов i (где 1 ≤ i ≤ n-1), для которых знак элемента vᵢ отличается от знака элемента vᵢ₊₁.

Входные данные: std::vector<int> - последовательность целых чисел
Выходные данные: int - количество изменений знака между соседними элементами

Ограничения:
- Вектор должен содержать как минимум 2 элемента для возможного изменения знака
- Элементы могут быть положительными, отрицательными или нулевыми
- Нулевые значения считаются положительными числами

## 3. Baseline Algorithm (Sequential)
Базовый последовательный алгоритм проходит по вектору и сравнивает знаки соседних элементов, если знаки отличаются прибается 1 к счетчеку смены знаков.

Сложность алгоритма: O(n), где n - размер входного вектора.

## 4. Parallelization Scheme
- Распределение данных
Каждый процесс работает непосредственно с исходным вектором, но обрабатывает только свой сегмент, вычисленный по индексам. Если размер вектора не делится равномерно, первые процессы получают по одному дополнительному элементу.

- Паттерн: Каждый процесс независимо вычисляет локальное количество изменений знаков внутри своего сегмента и проверяет границу с соседним сегментом, затем результаты собираются на процессе 0.

- Роли процессов
Все процессы: Вычисляют индексы своего сегмента. Подсчитывают изменения знаков внутри сегмента. Проверяют границу с правым соседним сегментом (кроме последнего процесса). Отправляют локальный счетчик и результат проверки границы
- Обработка границ
Каждый процесс (кроме последнего) дополнительно проверяет изменение знака между последним элементом своего сегмента и первым элементом следующего сегмента. Эта проверка выполняется отдельно от основного подсчета внутри сегмента.

## 5. Implementation Details

kazenova_a_vec_change_sign/
├── common/
│   └── common.hpp           // Общие типы данных
├── seq/
│   ├── ops_seq.hpp         // SEQ заголовок
│   └── ops_seq.cpp         // SEQ реализация
├── mpi/
│   ├── ops_mpi.hpp         // MPI заголовок  
│   └── ops_mpi.cpp         // MPI реализация
└── tests/
    ├── functional/         // Функциональные тесты
    └── performance/        // Тесты производительности

- Ключевые классы:
KazenovaAVecChangeSignSEQ - последовательная реализация
KazenovaAVecChangeSignMPI - параллельная MPI реализация

- Особые случаи обработки:
Корректная обработка границ между сегментами процессов
Единичный элемент в векторе (результат = 0)
Все элементы одного знака (результат = 0)

## 6. Experimental Setup
- Hardware/OS
Процессор: AMD Ryzen 5 3500U, 4 ядра, 8 потоков
Тактовая частота: 2.10 GHz  
Оперативная память: 8 GB RAM
Накопитель: 477 GB SSD
Операционная система: Windows 10 Home 22H2 (сборка 19045.6456)

- Компилятор: g++ 
- Использовался Docker-контейнер.
- Тип сборки: Release

- Environment
Количество процессов: 2, 4

## 7. Results and Discussion

### 7.1 Correctness
Корректность реализации проверена с помощью набора функциональных тестов, покрывающих различные сценарии с разным количетвом чередований знаков в векторе.

### 7.2 Performance

| Mode        | Count | Time, s | Speedup | Efficiency |
|-------------|-------|---------|---------|------------|
| seq         | 1     | 0.21896 | 1.00    | N/A        |
| imp         | 2     | 0.16307 | 1.34    | 67.0%      |
| imp         | 4     | 0.15995 | 1.37    | 34.2%      |

## 8. Conclusions
Успешно реализована параллельная MPI версия алгоритма подсчета изменений знака. На больших данных (100 миллионов элементов) достигнуто ускорение в 1.34 раза на 2 процессах и 1.37 раза на 4 процессах по сравнению с последовательной версией. Эффективность параллелизации составляет 67.0% на 2 процессах и 34.2% на 4 процессах. Снижение эффективности с ростом числа процессов обусловлено коммуникационными затратами и накладными расходами MPI, особенно заметными при увеличении числа процессов.

## 9. References
1. Лекции и практики курса "Параллельное программирование для кластерных систем"