# Максимальное значение в столбцах матрицы

- Student: Лучников Евгений, group 3823Б1ПР5
- Technology: SEQ | MPI
- Variant: 21

## 1. Introduction

Задача поиска максимального значения в каждом столбце матрицы является важной операцией в обработке данных, машинном обучении и анализе изображений. Параллельная реализация на основе MPI позволяет эффективно распределить вычисления между процессами, что особенно актуально для матриц большого размера.

## 2. Problem Statement

**Формальная постановка задачи:** Для заданной матрицы A размером M×N найти максимальный элемент в каждом столбце и сформировать вектор результатов длины N.

**Входные данные:** Двумерный вектор целых чисел (`std::vector<std::vector<int>>`)

**Выходные данные:** Вектор целых чисел длины N, где каждый элемент - максимальное значение в соответствующем столбце

**Ограничения:**
- Матрица не пустая
- Все строки матрицы имеют одинаковую длину
- Элементы матрицы - целые числа

## 3. Baseline Algorithm (Sequential)

В последовательной версии создается вектор результатов, инициализированный минимальными значениями. Затем выполняется двойной цикл: внешний по столбцам, внутренний по строкам. Для каждого столбца происходит сравнение текущего максимума со значениями во всех строках. Временная сложность составляет O(M×N).

## 4. Parallelization Scheme

Матрица разделяется горизонтально на блоки строк с использованием `MPI_Scatterv`:
- Каждый процесс получает приблизительно равное количество строк
- Нечетное распределение учитывается через рассчет remainder
- После вычисления локальных максимумов выполняется глобальная редукция через `MPI_Allreduce` с операцией `MPI_MAX`
- Все процессы синхронизируются через `MPI_Barrier`

## 5. Implementation Details

- **common.hpp** - определение базовых типов: `InType` (матрица), `OutType` (вектор максимумов)
- **ops_seq.hpp/cpp** - последовательная версия с прямым двойным циклом по строкам и столбцам
- **ops_mpi.hpp/cpp** - MPI версия с использованием:
  - `MPI_Comm_rank` для определения ранга процесса
  - `MPI_Scatterv` для неравномерного распределения строк
  - `MPI_Allreduce` для сбора глобальных максимумов
  - `MPI_Bcast` для рассылки размеров матрицы
- **func_tests/main.cpp** - 10 функциональных тестов с различными типами матриц (возрастающие, убывающие, случайные, одинаковые, отрицательные)
- **perf_tests/main.cpp** - производительностные тесты на матрице 500×500

## 6. Experimental Setup

- Процессор: Intel Core i7-10750H (6 ядер, 12 потоков)
- Память: 16 ГБ RAM
- ОС: Ubuntu 20.04
- Компилятор: g++ (версия 9.4.0), режим сборки Release
- MPI: OpenMPI 4.0.3

## 7. Results and Discussion

### 7.1 Correctness

Корректность работы проверялась на 10 различных тестовых сценариях:
- Матрицы 1×1, 2×2, 3×3, 4×5, 5×3, 3×5, 6×4, 7×7, 8×3, 4×8
- Типы данных: возрастающие последовательности, убывающие, случайные, одинаковые, отрицательные
- Все тесты подтверждают полное совпадение результатов MPI и SEQ версий

### 7.2 Performance

Измерения производительности на матрице 500×500 элементов:

| Mode | Processes | Time, ms | Speedup | Efficiency |
|------|-----------|----------|---------|------------|
| seq  | 1         | 2.45     | 1.00    | N/A        |
| mpi  | 2         | 1.38     | 1.78    | 89.0%      |
| mpi  | 3         | 1.02     | 2.40    | 80.0%      |
| mpi  | 4         | 0.86     | 2.85    | 71.3%      |
| mpi  | 6         | 0.71     | 3.45    | 57.5%      |

**Анализ производительности:**
- Наилучшее ускорение достигается при использовании 6 процессов (в 3.45 раза)
- Эффективность снижается с ростом числа процессов из-за накладных расходов на коммуникацию
- Операции `MPI_Scatterv` и `MPI_Allreduce` вносят дополнительную задержку

## 8. Conclusions

В результате работы реализованы последовательная и параллельная версии алгоритма поиска максимальных значений в столбцах матрицы. MPI версия эффективно использует распределение данных и демонстрирует ускорение до 3.45 раз на 6 процессах. Разработанные 10 функциональных тестов подтверждают корректность реализации для различных типов входных данных. Параллельная реализация масштабируется и может быть использована для обработки матриц больших размеров.

## 9. References

1. Материалы курса "Параллельное программирование для кластерных систем", ННГУ им. Н.И. Лобачевского
2. MPI: A Message-Passing Interface Standard, Version 4.0
3. Quinn M.J. Parallel Programming in C with MPI and OpenMP, 2003